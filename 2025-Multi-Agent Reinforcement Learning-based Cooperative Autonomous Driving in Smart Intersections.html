<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 16pt; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s2 { color: #7F7F7F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 20pt; }
 .s3 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 .s4 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s5 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s6 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s7 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .h2, h2 { color: black; font-family:"Trebuchet MS", sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s8 { color: black; font-family:Tahoma, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s9 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s10 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s11 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s12 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s13 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 9pt; }
 .s14 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s15 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s16 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s17 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s18 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -6pt; }
 .s19 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 7pt; }
 .s20 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s21 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s22 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 5pt; }
 .s23 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s24 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s25 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -2pt; }
 .s26 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s27 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s28 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 3pt; }
 .s29 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s30 { color: black; font-family:Tahoma, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 3pt; }
 .s31 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .h4, h4 { color: black; font-family:Tahoma, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; }
 .s32 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 3pt; }
 .s33 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s34 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s35 { color: black; font-family:Tahoma, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -7pt; }
 .s36 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -10pt; }
 .s37 { color: black; font-family:Tahoma, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s38 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 1pt; }
 .s39 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 3pt; }
 .s40 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s41 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s42 { color: black; font-family:Tahoma, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s43 { color: black; font-family:Tahoma, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s44 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s45 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s46 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s47 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 10pt; }
 .s48 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 6pt; }
 .s49 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s50 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -4pt; }
 .s51 { color: black; font-family:Tahoma, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -1pt; }
 .s52 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s53 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -2pt; }
 .s54 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 11pt; }
 .s55 { color: black; font-family:Tahoma, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -4pt; }
 .s56 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -5pt; }
 .s57 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -5pt; }
 .s58 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 8pt; }
 .s59 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s60 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -7pt; }
 .s61 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; }
 .s62 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .a { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 0; }
 #l1> li:before {counter-increment: c1; content: counter(c1, upper-roman)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l2 {padding-left: 0pt; }
 #l2> li:before {content: "• "; color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 #l3 {padding-left: 0pt;counter-reset: e1 2; }
 #l3> li:before {counter-increment: e1; content: counter(e1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l4 {padding-left: 0pt; }
 #l4> li:before {content: "— "; color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l5 {padding-left: 0pt;counter-reset: g1 0; }
 #l5> li:before {counter-increment: g1; content: counter(g1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l6 {padding-left: 0pt;counter-reset: g1 2; }
 #l6> li:before {counter-increment: g1; content: counter(g1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><h1 style="padding-top: 1pt;padding-left: 14pt;text-indent: 0pt;line-height: 108%;text-align: center;">Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="text-indent: 0pt;text-align: center;">Taoyuan Yu, Kui Wang, Zongdian Li, Tao Yu, and Kei Sakaguchi</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 1pt;text-indent: 0pt;line-height: 21pt;text-align: left;">arXiv:2505.04231v1  [cs.RO]  7 May 2025</p><p style="text-indent: 0pt;text-align: left;"/><p class="s3" style="padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: justify;">Abstract<span class="h3">— Unsignalized intersections pose significant safety and efficiency challenges due to complex traffic flows. This paper proposes a novel roadside unit (RSU)-centric cooperative driving system leveraging global perception and vehicle-to- infrastructure (V2I) communication. The core of the system is an RSU-based decision-making module using a two-stage hybrid reinforcement learning (RL) framework. At first, policies are pre-trained offline using conservative Q-learning (CQL) combined with behavior cloning (BC) on collected dataset. Subsequently, these policies are fine-tuned in the simulation using multi-agent proximal policy optimization (MAPPO), aligned with a self-attention mechanism to effectively solve inter-agent dependencies. RSUs perform real-time inference based on the trained models to realize vehicle control via V2I communications. Extensive experiments in CARLA environ- ment demonstrate high effectiveness of the proposed system, by: </span>(i) <span class="h3">achieving failure rates below 0.03% in  coordinating three connected and autonomous vehicles (CAVs) through complex intersection scenarios, significantly outperforming the traditional Autoware control method, and </span>(ii) <span class="h3">exhibiting strong robustness across varying numbers of controlled agents and shows promising generalization capabilities on other maps.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l1"><li style="padding-left: 15pt;text-indent: 72pt;text-align: left;"><p style="display: inline;"><a name="bookmark0">I</a><span class="s4">NTRODUCTION</span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Intersection management is regarded as a bottleneck in the development of intelligent transportation systems (ITS), considering the complex and uncertain nature of urban intersections [1]. According to statistics from the Federal Highway Administration (FHWA) and the National High- way Traffic Safety Administration (NHTSA), intersection- related fatalities account for a substantial proportion of total traffic accident deaths, especially at unsignalized intersec- tions, which reportedly accounted for 68% of such fatalities in 2024 [2], [3]. Unsignalized intersections have become accident hotspots due to blind spots and the lack of clear interaction rules between motor vehicles, non-motorized vehicles, and pedestrians. In such environments, connected and autonomous vehicles (CAVs) should possess highly effective perception, prediction, and coordinated decision- making capabilities to minimize conflicts and ensure safe and smooth driving [4].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">With the popularization of autonomous vehicles (AVs), mixed traffic scenarios involving AVs and human-driven vehicles (HDVs) are becoming common, thereby intro- ducing novel challenges to traffic participants. Vehicle-to- everything (V2X) communication technologies have emerged as a promising solution to  improve  roadway  efficiency and safety [5], typically including vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), vehicle-to-pedestrian (V2P), and vehicle-to-network (V2N) links [6]. Among these, V2I communication facilitates real-time data exchange between</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: right;">CAVs and smart roadside infrastructure (e.g., RSUs), laying a foundation for constructing cooperative driving systems [7]. Building upon V2I capabilities, the design of RSU-based cooperative systems has appeared as an attractive research topic for both academia and industry in recent years [8], [9]. Research on optimizing traffic flow at intersections has explored various methods. Traditional methods often rely on model-based optimization or game-theoretic frameworks to allocate right-of-way and manage traffic, achieving progress in reducing delays under certain conditions [10]–[12]. How- ever, these methods typically lack the adaptability required to effectively handle the high complexity and uncertainty inher- ent in dynamic real-world traffic scenarios. To address these limitations, multi-agent reinforcement learning (MARL) has emerged as a promising alternative, with research investigat- ing hierarchical structures or integrating perception modules to enhance coordination [13]–[15]. Nevertheless, many ex- isting MARL applications in this domain tend to treat all vehicles uniformly, often overlooking the critical need for distinct policies tailored to specific driving roles (e.g., turning left, going straight, turning right) and failing to fully capture</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">the complex interaction dynamics.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Further advancements aim to enhance MARL’s capabilities in complex environments. Self-attention mechanisms have been incorporated to dynamically model inter-agent depen- dencies, potentially improving generalization and decision efficiency [16], [17]. However, the effectiveness validation of adopting self-attention in MARL is still unexplored, particularly its adaptability to dynamically varying  num- bers of interacting vehicles within realistic transportation contexts like unsignalized intersections. Similarly, hybrid offline-online RL frameworks offer potential benefits by leveraging real-world data for safer and more efficient policy learning [18]–[21]. However, the practical implementation and demonstrated effectiveness of these hybrid approaches in coordinating multiple cooperative vehicles through complex and real-world intersection scenarios still require deeper investigation. Therefore, there exists a research gap in devel- oping and validating an integrated MARL framework capable of robustly and efficiently coordinating vehicles with diverse intentions within the complex dynamics of intersections.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">To address such challenges, we propose an innovative RSU-centric intelligent management system for unsignalized intersections. Utilizing bird-eye-view (BEV) perception from RSU-mounted LiDAR, the system employs a centralized MARL decision module featuring role-specific policy net- works integrated with a self-attention  mechanism,  allow- ing for dynamic modeling of interactions between distinct</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 55pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="543" height="218" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_001.png"/></span></p><p style="padding-top: 3pt;padding-left: 111pt;text-indent: 0pt;text-align: left;"><a name="bookmark1">Fig. 1: High-level system design of the RSU-CAVs cooperative system</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">driving roles and  flexible  adaptation  to  varying  numbers of vehicle participants. The policy networks are developed using a two-stage hybrid learning approach, involving offline pre-training on the collected dataset, followed by online fine-tuning in the simulation environment. The proposed system demonstrates significant advantages in adaptability, generalization, safety, and efficiency,  while also  reducing model deployment complexity and computational demands. The key contributions of this research are as follows:</p><ul id="l2"><li style="padding-top: 2pt;padding-left: 25pt;text-indent: -10pt;text-align: justify;"><p style="display: inline;">A novel hybrid RL framework combining offline pre- training and online fine-tuning techniques to enable co- operative driving for CAVs at unsignalized intersections.</p></li><li style="padding-left: 25pt;text-indent: -10pt;text-align: justify;"><p style="display: inline;">Development of personalized policy networks tailored to distinct driving roles (e.g., left-turn, straight, right- turn) at intersections.</p></li><li style="padding-left: 25pt;text-indent: -10pt;text-align: justify;"><p style="display: inline;">Integration of a self-attention mechanism into role- based MARL to enhance policy adaptability to varying vehicle numbers and model dynamic interactions.</p></li><li style="padding-left: 25pt;text-indent: -10pt;text-align: justify;"><p style="display: inline;">Demonstration of the model’s generalization capability and rapid adaptability across diverse unsignalized inter- section scenarios.</p></li></ul><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">The remainder of the paper is structured as follows: Section II presents the overall architecture of the RSU- CAVs cooperative system. Section III describes the proposed algorithm in detail. Section IV demonstrates experiment results. Finally, Section V summarizes the paper and outlines future research directions.</p></li><li style="padding-top: 7pt;padding-left: 59pt;text-indent: -15pt;text-align: left;"><p style="display: inline;"><a name="bookmark2">RSU-CAV</a><span class="s4">S </span>C<span class="s4">OOPERATIVE </span>S<span class="s4">YSTEM</span></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark1" class="s5">The overall system architecture of the proposed RSU-CAV cooperative framework is illustrated in Fig. </a>1. This system employs an RSU, equipped with sensors like LiDAR, for comprehensive intersection monitoring [22] and centralized decision-making for multiple CAVs at an unsignalized inter- section. Leveraging the RSU’s BEV perception, this central- ized method overcomes the inherent limitations of individual vehicle perception, providing a global understanding of the traffic situation essential. This contrasts with individualistic methods, where each vehicle optimizes only its own goals, which can lead to competitive standoffs, inefficient gridlocks,</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">or unsafe maneuvers in unsignalized interactions. Instead, our framework enhances collective safety and overall traffic throughput by resolving conflicts harmoniously.</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">To effectively manage the intersection’s complexities and uncertainties, the RSU utilizes adaptive decision-making policies developed through an RL-based method. This method begins by employing offline RL to instill founda- tional driving knowledge and essential interaction behaviors into the policy from collected datasets, establishing a robust and competent initial strategy. Subsequently, these founda- tional policies undergo targeted refinement using online RL within a simulation environment. This allows the policies to adapt to the intersection’s unique dynamic characteristics and optimize performance for the required multi-vehicle co- operative tasks. Compared to learning entirely from scratch, this hybrid RL approach offers the advantages of accelerating learning convergence during the online refinement phase and ultimately yielding coordination policies that are more robust and effective in handling real-world complexities. Further- more, deploying the computationally intensive analysis and multi-vehicle coordination logic onto the RSU also reduces the processing burden on individual CAVs. This allocation of tasks improves the system’s real-time responsiveness and simplifies requirements for the CAVs [23].</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">As CAVs approach the intersection, they maintain contin- uous information exchange with the RSU. Based on real- time traffic data and the CAVs’ approach trajectories, the RSU determines each vehicle’s driving role (e.g., left-turn, straight, right-turn). Subsequently, leveraging its pre-loaded and role-based strategy networks within the centralized de- cision module, the RSU computes vehicle control signals, including throttle input, braking force, and steering angles. These control commands are transmitted in real-time to the corresponding CAVs through V2I communication, enabling direct command execution. Concurrently, the RSU continu- ously monitors the comprehensive real-time traffic conditions at the intersection, including the states and predicted move- ments of CAVs, observed HDVs, and pedestrians, alongside traffic flow smoothness, collision risks, and any abnormal situations. This continuous monitoring provides the neces-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="659" height="174" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_002.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 131pt;text-indent: 0pt;text-align: left;"><a name="bookmark3">Fig. 2: Offline-online hybrid RL algorithm framework design</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">sary real-time inputs for the RSU’s decision networks and facilitates ongoing performance evaluation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 6pt;padding-left: 27pt;text-indent: -19pt;text-align: left;"><p style="display: inline;"><a name="bookmark4">H</a><span class="s4">YBRID </span>R<span class="s4">EINFORCEMENT </span>L<span class="s4">EARNING </span>F<span class="s4">RAMEWORK</span></p><p class="s6" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark6">B. Action Space</a></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;line-height: 89%;text-align: justify;">Throughout the learning framework, we define a unified two-dimensional continuous action space <span class="s7">A </span>for the vehicle. It is structured as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 14pt;text-indent: 0pt;text-align: left;"><a href="#bookmark3" class="s5">As  shown  in  Fig.  </a>2,  we  propose  a  two-stage  learning</p><h2 style="padding-top: 4pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">a<span class="s8">(</span><span class="s6">t</span><span class="s8">) = [</span>a<span class="s9">acc</span><span class="s6">, </span>a<span class="s9">steer</span><span class="s8">] </span><span class="s7">∈ </span><span class="s10">R</span><span class="s11">2</span></h2><p style="padding-top: 6pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">(2)</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">framework to develop effective cooperative driving strategies for complex unsignalized intersections. This approach first employs offline pre-training on the collected dataset, using offline RL to safely learn foundational driving skills and traffic priors. Subsequently, online fine-tuning within the CARLA simulator [24] allows agents to adapt to specific environment dynamics. This synergistic framework combines the safety and data efficiency of offline learning with the adaptability and performance optimization capabilities of online interaction. The specific methodologies for offline pre- training and online fine-tuning are discussed in this section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark5">A. Observation Space</a></p><p class="s8" style="padding-top: 8pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><span class="p">At each time step </span><span class="s6">t</span><span class="p">, the state  space  can  be  defined </span><span class="h2">s</span>(<span class="s6">t</span>)<span class="p">, which encompasses all traffic participants monitored by RSU within the intersection. the RSU utilizes the global information to generate specific perspective information for each CAV, constructing an individual observation vector </span><span class="h2">o</span>(<span class="s6">t</span>)<span class="p">.  This  construction  process,  denoted  conceptually  as</span></p><p class="s8" style="padding-left: 5pt;text-indent: 0pt;line-height: 90%;text-align: justify;"><span class="h2">o</span>(<span class="s6">t</span>) = <span class="s7">O</span>(<span class="h2">s</span>(<span class="s6">t</span>))<span class="p">, reflects simulated perception and processing limitations, meaning each </span><span class="h2">o</span>(<span class="s6">t</span>) <span class="p">is a partially observable and potentially noisy representation of the true state </span><span class="h2">s</span>(<span class="s6">t</span>)<span class="p">. The</span></p><p class="s8" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;"><span class="p">observation </span><span class="h2">o</span>(<span class="s6">t</span>) <span class="p">is decomposed as:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 5pt;text-indent: 52pt;text-align: left;">o<span class="s8">(</span><span class="s6">t</span><span class="s8">) = [</span>o<span class="s9">core</span><span class="s6">, </span>o<span class="s9">veh</span><span class="s6">, </span>o<span class="s9">ped</span><span class="s6">, </span>o<span class="s9">role</span><span class="s6">, </span>o<span class="s9">ctx</span><span class="s8">]</span><span class="s6">,                </span><span class="p">(1)</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where <span class="h2">o</span><span class="s9">core</span><span class="s12"> </span>denotes the ego vehicle features including speed magnitude, global position, heading angle, distance to inter- section center, and junction occupancy status; <span class="h2">o</span><span class="s9">veh</span><span class="s12"> </span>denotes relative positions and velocities of nearby vehicles within a predefined ranged; <span class="h2">o</span><span class="s9">ped</span><span class="s12"> </span>denotes information on nearby pedestrians, including detection flags, distance, and relative angle; <span class="h2">o</span><span class="s9">role</span><span class="s12"> </span>denotes the one-hot encoding  of  the  agent’s role (e.g., left-turn, straight, or right-turn); and <span class="h2">o</span><span class="s9">ctx</span><span class="s12"> </span>denotes scenario-level identifiers used to curriculum learning.</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">where <span class="h2">a</span><span class="s9">acc</span><span class="s12"> </span>denotes the longitudinal acceleration and <span class="h2">a</span><span class="s9">steer</span><span class="s12"> </span>denotes the steering angular velocity. It is  important  to note that during the offline pre-training phase, as ground- truth control signals are unavailable in the source data, the action <span class="h2">a</span><span class="s8">(</span><i>t</i><span class="s8">) </span>in the dataset is estimated by  analyzing  the state transitions between consecutive changes in velocity and heading. In the online fine-tuning phase, the policy network directly outputs two-dimensional action.</p><ol id="l3"><li style="padding-top: 8pt;padding-left: 19pt;text-indent: -14pt;text-align: justify;"><p class="s6" style="display: inline;"><a name="bookmark7">Reward Function</a></p><p class="s8" style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><span class="p">To effectively guide the agent in learning desired cooperative driving behaviors during complex online interactions, we design a structured reward function </span><span class="s7">R</span><span class="s9">online</span>(<span class="h2">s</span>(<span class="s6">t</span>)<span class="s6">, </span><span class="h2">a</span>(<span class="s6">t</span>)<span class="s6">, </span><span class="h2">s</span>(<span class="s6">t </span>+ 1)) <span class="p">to translate high-level objectives</span></p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: justify;"><span class="p">into real-time feedback. The overall reward </span>r<span class="s8">(</span>t<span class="s8">) </span><span class="p">is structured</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: justify;">as:</p><p class="s6" style="text-indent: 0pt;line-height: 18pt;text-align: right;">r<span class="s8">(</span>t<span class="s8">) = </span><span class="s13">\</span><span class="s10"> </span>w<span class="s14">k</span>r<span class="s14">k</span><span class="s15"> </span><span class="s8">(</span><span class="h2">s</span><span class="s8">(</span>t<span class="s8">)</span>, <span class="h2">a</span><span class="s8">(</span>t<span class="s8">)</span>, <span class="h2">s</span><span class="s8">(</span>t <span class="s8">+ 1))</span>,         <span class="p">(3)</span></p><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: right;">where <i>r</i><span class="s14">i</span><span class="s15"> </span>denotes individual reward components and <i>w</i><span class="s14">i</span><span class="s15"> </span>de- notes the corresponding weights. The reward terms include:</p><p class="s16" style="padding-top: 4pt;text-indent: 0pt;line-height: 18pt;text-align: right;">r<span class="s15">i </span><span class="s17">∈</span><span class="s7"> {</span><span class="s6">r</span><span class="s12">safety</span>,<span class="s6"> r</span><span class="s12">eff</span>,<span class="s6"> r</span><span class="s12">comfort</span>,                          <span class="s6"> </span><span class="s18">(4)</span></p><p class="s16" style="padding-left: 85pt;text-indent: 0pt;line-height: 13pt;text-align: left;">r<span class="s12">task</span>,<span class="s6"> r</span><span class="s12">yield</span>,  r<span class="s12">coop</span>,<span class="s6"> r</span><span class="s12">penalty</span><span class="s17">}</span></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where <i>r</i><span class="s9">safety</span><span class="s12"> </span>denotes the penalty for hazardous behavior based on metrics like minimum time-to-collision (TTC) and distance to nearby vehicles or pedestrians; <i>r</i><span class="s9">eff</span><span class="s12"> </span>denotes the efficiency that encourages maintaining a reasonable speed that is compatible with traffic flow; <i>r</i><span class="s9">comfort</span><span class="s12"> </span>denotes the penalty for large acceleration changes; <i>r</i><span class="s9">task</span><span class="s12"> </span>denotes the reward for all agents to cooperatively reach the navigation target; <i>r</i><span class="s9">yield</span><span class="s12"> </span>and <i>r</i><span class="s9">coop</span><span class="s12"> </span>denotes the rewards for promoting compliance with traffic rules and cooperation; and <i>r</i><span class="s9">penalty</span><span class="s12"> </span>denotes a severe penalty imposed on events like collisions or timeouts. Each term is scaled by its corresponding weight <i>w</i><span class="s14">k</span><span class="s15"> </span>, where <i>w</i><span class="s9">safety</span><span class="s12"> </span>and <i>w</i><span class="s9">penalty</span><span class="s12"> </span>are typically assigned larger values due to their critical safety implications.</p></li><li style="padding-top: 2pt;padding-left: 20pt;text-indent: -14pt;text-align: left;"><p class="s6" style="display: inline;"><a name="bookmark8">Offline Pre-training: Networks and Algorithm</a></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 9pt;line-height: 12pt;text-align: justify;">The primary goal  of  the  offline  pre-training  phase  is to provide a high-quality initialization for the subsequent online  fine-tuning  stage.  In  this  stage,  we  train  models</p><p style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">to information from different representation subspaces at different positions. The core component is the scaled dot- product attention:</p><p class="s19" style="padding-top: 2pt;padding-left: 150pt;text-indent: 0pt;line-height: 10pt;text-align: left;"> <span class="s10"> </span><span class="s6">QK</span><span class="s20">⊤</span><span class="s21"> </span> </p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;">independently for each driving role (left-turn, straight, right-</p><p style="text-indent: 0pt;text-align: left;"><span><img width="32" height="3" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_003.png"/></span></p><p class="s7" style="text-indent: 0pt;line-height: 9pt;text-align: left;">√</p><p style="text-indent: 0pt;text-align: left;"/><p class="s8" style="padding-left: 6pt;text-indent: 0pt;line-height: 6pt;text-align: left;"><span class="p">A</span>(<span class="s6">Q, K, V </span>) = <span class="p">softmax</span></p><p class="s6" style="text-indent: 0pt;line-height: 8pt;text-align: right;">d<span class="s14">k</span></p><p class="s6" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">V               <span class="p">(7)</span></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">turn) to incorporate role-specific prior knowledge. We first</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 89%;text-align: justify;">partition the InD dataset [25] based on vehicle intentions to create subsets <span class="s7">D</span><span class="s9">role</span>.</p><p style="padding-left: 6pt;text-indent: 9pt;line-height: 10pt;text-align: left;">For each subset, we apply an offline reinforcement learn-</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">ing algorithm that combines CQL with BC [26] [27]. The algorithm is implemented using an actor-critic framework to learn effectively from fixed datasets while mitigating distributional shift and imitating expert behavior.</p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where <i>Q</i>, <i>K</i>, and <i>V </i>denote the query, key, and value matrices, and <i>d</i><span class="s14">k</span><span class="s15"> </span>denotes the dimension of the keys. MHSA computes <i>h </i>attention ’heads’ in parallel. For each head <i>i</i>, the input embedding <i>E </i>is linearly projected using learned weights <i>W</i><span class="s22">Q</span>, <i>W</i><span class="s23">K</span><span class="s15"> </span>, <i>W</i><span class="s23">V</span><span class="s15">   </span>to obtain the head’s specific query,</p><p class="s15" style="padding-left: 49pt;text-indent: 0pt;line-height: 1pt;text-align: left;">i     i      i</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">key, and value:</p><p class="s6" style="padding-top: 3pt;padding-left: 51pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="p">h</span><span class="s14">i</span><span class="s15"> </span><span class="s8">= </span><span class="p">Attention</span><span class="s8">(</span>EW<span class="s22">Q</span>, EW<span class="s24">K</span>, EW<span class="s24">V</span><span class="s15"> </span><span class="s8">)         </span><span class="p">(8)</span></p><p class="s15" style="text-indent: 0pt;line-height: 2pt;text-align: right;">i       i        i</p><p style="padding-left: 6pt;text-indent: 9pt;line-height: 10pt;text-align: left;">To stabilize learning and reduce Q-value overestimation,</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">the critic uses twin Q-networks <i>Q</i><span class="s14">θ</span><span class="s25">i,</span><span class="s26">1 </span>, <i>Q</i><span class="s14">θ</span><span class="s25">i,</span><span class="s26">2 </span>with target networks. Each Q-network is trained with the following objective:</p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">The outputs of the parallel heads are then concatenated and linearly projected using weights <i>W</i><span class="s23">O</span><span class="s15"> </span>to produce the final MHSA output:</p><p style="padding-top: 4pt;padding-left: 48pt;text-indent: 0pt;line-height: 7pt;text-align: left;">M<span class="s8">(</span><i>E</i><span class="s8">) = </span>Concat<span class="s8">(</span>head<span class="s27">1</span><i>, . . . , </i>head<span class="s14">h</span><span class="s8">)</span><i>W</i><span class="s24">O             </span><span class="s15"> </span>(9)</p><p class="s15" style="padding-top: 5pt;padding-left: 18pt;text-indent: 0pt;text-align: left;"><span class="s28">L</span><span class="s29">Q</span><span class="s30">(</span><span class="s6">θ</span><span class="s29">i,j</span> <span class="s30">) </span><span class="s8">=</span><span class="s10">E</span><span class="s31">(</span><span class="h4">o</span>,<span class="h4">a</span>,r,<span class="h4">o</span><span class="s32">′</span><span class="s33"> </span><span class="s31">)</span><span class="s21">∼D</span><span class="s34">role</span><span class="s26">=</span><span class="s33">i</span></p><p class="s10" style="text-indent: 0pt;line-height: 9pt;text-align: center;">  <span class="s35">1                     </span><span class="s8"> </span><span class="s36">2</span> </p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="1" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_004.png"/></span></p><p class="s8" style="text-indent: 0pt;line-height: 10pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s8" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(<span class="s6">Q</span><span class="s14">θ</span><span class="s25">i,j</span><span class="s33"> </span>(<span class="h2">o</span><span class="s6">, </span><span class="h2">a</span>) <span class="s7">− </span><span class="s6">y</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 10pt;text-indent: 0pt;text-align: left;">(5)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 12pt;text-indent: 0pt;text-align: left;">Here, <i>E </i>denotes the initial embedded observation features</p><p class="s37" style="padding-top: 1pt;padding-left: 66pt;text-indent: 0pt;text-align: left;">+<span class="s8"> </span><span class="s6">α</span><span class="s12">CQL</span><span class="s16">L</span><span class="s12">CQL_reg</span>(<span class="s6">θ</span><span class="s15">i,j </span>)</p><p style="padding-left: 66pt;text-indent: 0pt;line-height: 79%;text-align: left;">derived from the online observation <span class="h2">o</span><span class="s14">t</span><span class="s15"> </span>via a learnable linear projection layer, and <i>W</i><span class="s22">Q</span>, <i>W</i><span class="s23">K</span><span class="s15"> </span>, <i>W</i><span class="s23">V</span><span class="s15"> </span>, <i>W</i><span class="s23">O</span><span class="s15">  </span>denote trainable</p><p class="s17" style="text-indent: 0pt;line-height: 9pt;text-align: left;">—       <span class="s7"> </span><span class="s15">j  θ</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="p">Here, </span>y <span class="s8">= </span>r <span class="s8">+ </span>γ<span class="s8">(1   </span>d<span class="s8">) min </span>Q <span class="s33">′</span></p><p class="s33" style="text-indent: 0pt;line-height: 4pt;text-align: right;">i,j</p><p class="s8" style="text-indent: 0pt;line-height: 11pt;text-align: left;">(<span class="h2">o</span><span class="s20">′</span><span class="s6">, π</span><span class="s14">ϕ</span><span class="s25">i</span><span class="s33"> </span>(<span class="h2">o</span><span class="s20">′</span>)) <span class="p">denotes the</span></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">weight matrices.</p><p class="s15" style="padding-left: 6pt;text-indent: 0pt;line-height: 3pt;text-align: left;">i     i      i</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">TD target computed using target Q networks and the current</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">policy.</p><p style="padding-left: 6pt;text-indent: 9pt;line-height: 12pt;text-align: justify;">The policy network <i>π</i><span class="s14">ϕ</span><span class="s25">i</span><span class="s33"> </span>is trained by minimizing the BC loss along with maximizing the expected conservative Q- value:</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 9pt;text-align: left;">Online learning proceeds via an interact-learn loop. Agents generate trajectories:</p><p class="s38" style="text-indent: 0pt;line-height: 9pt;text-align: left;">role                           <span class="s34"> </span><span class="s39">}</span><span class="s15">t</span><span class="s40">=0</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="padding-top: 3pt;padding-left: 15pt;text-indent: 3pt;line-height: 15pt;text-align: left;"><span class="s6">τ </span><span class="s8">= </span><span class="s7">{</span><span class="s8">(</span><span class="h2">o</span>t<span class="s6">, </span><span class="h2">a</span>t<span class="s6">, r</span>t<span class="s40">+1</span><span class="s6">, V</span>ψ<span class="s15"> </span><span class="s8">(</span><span class="h2">o</span>t<span class="s8">)</span><span class="s6">, </span><span class="s8">log </span><span class="s6">π</span>ϕ   <span class="s15"> </span><span class="s8">(</span><span class="h2">a</span>t<span class="s15"> </span><span class="s7">| </span><span class="h2">o</span>t<span class="s8">)) </span><span class="s22">T          </span><span class="s15"> </span><span class="p">(10) Advantage estimates and returns are computed using gen-</span></p><p class="s29" style="padding-top: 6pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><span class="s41">L</span>π<span class="s15"> </span><span class="s42">(</span><span class="s6">ϕ</span>i<span class="s42">)</span><span class="s8"> =</span><span class="s10">E</span><span class="s43">o</span><span class="s21">∼D</span><span class="s34">role</span><span class="s26">=</span><span class="s33">i</span></p><p class="s10" style="padding-left: 17pt;text-indent: 86pt;line-height: 5pt;text-align: left;"> </p><p class="s44" style="text-indent: 0pt;line-height: 9pt;text-align: left;">—         <span class="s7"> </span><span class="s33">i,j        i</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s8" style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;line-height: 11pt;text-align: left;">min <span class="s6">Q</span><span class="s14">θ   </span><span class="s15"> </span>(<span class="h2">o</span><span class="s6">, π</span><span class="s14">ϕ</span><span class="s15"> </span>(<span class="h2">o</span>))</p><p class="s15" style="padding-left: 15pt;text-indent: 0pt;line-height: 6pt;text-align: left;">j<span class="s40">=1</span>,<span class="s40">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 14pt;text-indent: 0pt;line-height: 9pt;text-align: left;">(6)</p><p style="padding-left: 12pt;text-indent: 0pt;text-align: left;">eralized advantage estimation (GAE):</p><p class="s15" style="padding-top: 3pt;padding-left: 47pt;text-indent: 0pt;line-height: 9pt;text-align: left;">T <span class="s21">−</span>t<span class="s21">−</span><span class="s40">1</span></p><h2 style="text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s45">∼D</span><span class="s34">role</span><span class="s26">=</span><span class="s33">i            i </span><span class="s30">(</span>o<span class="s8">) </span><span class="s7">− </span>a<span class="s7">∥</span></h2><p style="text-indent: 0pt;text-align: left;"/><h4 style="padding-left: 6pt;text-indent: 69pt;line-height: 18pt;text-align: left;"><span class="s42">+</span><span class="s8"> </span><span class="s6">λ</span><span class="s12">BC</span><span class="s46">E</span><span class="s40">(</span>o<span class="s15">,</span>a<span class="s40">)            </span><span class="s47">l</span><span class="s44">∥</span><span class="s6">π</span><span class="s15">ϕ                   </span><span class="s48">2</span><span class="s47"> </span></h4><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">where <i>α</i><span class="s9">CQL  </span><span class="s12"> </span>and <i>λ</i><span class="s9">BC  </span><span class="s12"> </span>denotes hyperparameters controlling the strength of CQL regularization and BC imitation, respec-</p><p class="s49" style="text-indent: 0pt;line-height: 11pt;text-align: left;">t    <span class="s15"> </span><span class="s8">=</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s50" style="padding-left: 6pt;text-indent: 0pt;line-height: 14pt;text-align: left;">A<span class="s51">ˆ</span><span class="s12">GAE</span></p><p class="s10" style="padding-left: 6pt;text-indent: 0pt;line-height: 4pt;text-align: left;">\</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">l<span class="s40">=0</span></p><p class="s8" style="text-indent: 0pt;text-align: right;">(<span class="s6">γλ</span>)<span class="s24">l</span><span class="s6">δ</span><span class="s14">t</span><span class="s40">+</span><span class="s15">l</span><span class="s6">,   δ</span><span class="s14">t</span><span class="s15"> </span>= <span class="s6">r</span><span class="s14">t</span><span class="s40">+1 </span>+<span class="s6">γV</span><span class="s14">ψ</span><span class="s15"> </span>(<span class="h2">o</span><span class="s14">t</span><span class="s40">+1</span>)<span class="s7">−</span><span class="s6">V</span><span class="s14">ψ</span><span class="s15"> </span>(<span class="h2">o</span><span class="s14">t</span>)</p><p style="padding-top: 6pt;text-indent: 0pt;line-height: 10pt;text-align: right;">(11)</p><p class="s15" style="text-indent: 0pt;line-height: 7pt;text-align: left;">t</p><p style="text-indent: 0pt;text-align: left;"/><p class="s8" style="text-indent: 0pt;line-height: 14pt;text-align: right;"><span class="s6">R</span><span class="s42">ˆ</span><span class="s14">t</span><span class="s15"> </span>= <span class="s6">A</span><span class="s42">ˆ</span><span class="s52">GAE</span><span class="s12"> </span>+ <span class="s6">V</span><span class="s14">ψ</span><span class="s15"> </span>(<span class="h2">o</span><span class="s14">t</span>)                    <span class="p">(12)</span></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">tively.</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 9pt;line-height: 90%;text-align: justify;">Each role-specific actor <i>π</i><span class="s14">ϕ</span><span class="s53">role</span><span class="s34"> </span>and critic <i>Q</i><span class="s14">θ</span><span class="s53">role</span><span class="s34"> </span>are pa- rameterized by multi-layer perceptrons (MLPs), which take normalized state inputs <i>s</i><span class="s14">t</span><span class="s15"> </span>sampled from <span class="s7">D</span><span class="s9">role</span>. The output corresponds to action distribution parameters or state-action</p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;">values.</p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 5pt;text-align: left;">Self-attention are not introduced at this stage  to focus</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 9pt;text-align: left;">To enhance data efficiency, we adopt prioritized experience replay (PER). Each transition <i>t </i>is assigned a priority <i>p</i><span class="s14">t</span></p><p class="s7" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="p">proportional to its absolute TD error </span>|<span class="s6">δ</span><span class="s14">t</span>|<span class="p">, and sampled with</span></p><p class="s6" style="padding-left: 6pt;text-indent: 0pt;line-height: 88%;text-align: left;"><span class="p">probability </span>P <span class="s8">(</span>t<span class="s8">) </span><span class="s7">∝ </span>p<span class="s14">t</span><span class="p">. To correct the bias introduced by this non-uniform sampling, importance sampling (IS) weights are</span></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">applied:</p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">training on extracting robust role-based patterns using stan- dard MLPs. Training stability is further improved by soft</p><p class="s6" style="padding-top: 7pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">w<span class="s14">t</span><span class="s15"> </span><span class="s8">=</span></p><p class="s10" style="text-indent: 0pt;line-height: 13pt;text-align: center;">       <span class="s35">1     </span><span class="s8"> </span><span class="s49">β</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="46" height="1" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_005.png"/></span></p><p class="s6" style="text-indent: 0pt;text-align: center;">B <span class="s7">· </span>P <span class="s8">(</span>t<span class="s8">)</span></p><p style="padding-top: 7pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">(13)</p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">target updates and the adam optimizer. Successful offline training yields a set of pre-trained weights for the role- conditioned actor and critic networks, which are reused during the online fine-tuning phase to accelerate learning and enhance performance.</p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">Here, <i>B </i>denotes the size of the replay buffer, and <i>β </i>denotes an exponent that controls the amount of importance sampling correction.</p><p style="padding-left: 6pt;text-indent: 9pt;text-align: left;">The shared critic is updated to minimize the weighted value loss:</p></li><li style="padding-top: 6pt;padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s6" style="display: inline;"><a name="bookmark9">Online Fine-tuning: Networks and Algorithm</a></p></li></ol><p class="s21" style="text-indent: 0pt;line-height: 6pt;text-align: left;">∼</p><p style="text-indent: 0pt;text-align: left;"/><p class="s6" style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">L<span class="s24">V</span><span class="s15"> F </span><span class="s8">(</span>ψ<span class="s8">) = </span><span class="s10">E</span><span class="s14">t</span></p><p class="s12" style="text-indent: 0pt;line-height: 7pt;text-align: left;">PER      <span class="s15">t   ψ    t         t</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s8" style="padding-left: 12pt;text-indent: 0pt;line-height: 18pt;text-align: left;"><span class="s54">r</span><span class="s6">w </span>(<span class="s6">V  </span>(<span class="h2">o </span>) <span class="s7">− </span><span class="s6">R</span><span class="s42">ˆ</span> )<span class="s11">2</span><span class="s54">1       </span><span class="s10"> </span><span class="p">(14)</span></p><p class="s6" style="padding-top: 2pt;padding-left: 6pt;text-indent: 9pt;line-height: 12pt;text-align: justify;"><span class="p">The online fine-tuning  phase employs the  MAPPO al- gorithm [28], selected for its effectiveness in multi-agent coordination. This stage builds upon the offline models by integrating role-specific actor networks (</span>π<span class="s14">ϕ</span><span class="s53">left</span><span class="s34"> </span>, π<span class="s14">ϕ</span><span class="s53">straight</span><span class="s34"> </span>, π<span class="s14">ϕ</span><span class="s53">right</span><span class="s34"> </span><span class="p">) with a shared critic network </span>V<span class="s14">ψ</span><span class="s15"> </span><span class="p">.</span></p><p style="padding-top: 4pt;padding-left: 15pt;text-indent: -10pt;line-height: 15pt;text-align: left;">where <i>R</i><span class="s42">ˆ</span><span class="s14">t</span><span class="s15"> </span>denotes the target return calculated in Eq.(12).</p><p style="padding-left: 6pt;text-indent: 9pt;line-height: 91%;text-align: justify;">Each role-specific actor <i>π</i><span class="s14">ϕ</span><span class="s53">role</span><span class="s34"> </span>is trained using the following weighted objective, which includes the PPO clipped surro- gate loss and an entropy bonus <i>S</i><span class="s8">[</span><span class="s7">·</span><span class="s8">]</span>:</p><p style="padding-left: 6pt;text-indent: 9pt;text-align: justify;">To improve reasoning over dynamic environments, we augment both actor and critic networks with multi-head self- attention (MHSA). MHSA allows the model to jointly attend</p><p class="s21" style="text-indent: 0pt;line-height: 6pt;text-align: left;">∼</p><p style="text-indent: 0pt;text-align: left;"/><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s50">L</span><span class="s12">CLIP+S</span>(<span class="s6">ϕ</span><span class="s56">role</span>)<span class="s8"> = </span><span class="s10">E</span><span class="s57">t</span></p><p class="s6" style="text-indent: 0pt;line-height: 10pt;text-align: left;">w</p><p style="text-indent: 0pt;text-align: left;"/><p class="s10" style="text-indent: 0pt;line-height: 4pt;text-align: left;">r</p><p style="text-indent: 0pt;text-align: left;"/><p class="s12" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">PER     <span class="s15">t</span><span class="s13"> </span></p><p class="s15" style="text-indent: 0pt;line-height: 7pt;text-align: left;">t</p><p style="text-indent: 0pt;text-align: left;"/><ul id="l4"><li style="padding-left: 10pt;text-indent: -9pt;line-height: 14pt;text-align: left;"><p class="s6" style="display: inline;">L<span class="s52">CLIP</span><span class="s8">(</span>ϕ<span class="s9">role</span><span class="s8">)</span></p></li><li style="padding-left: 10pt;text-indent: -9pt;line-height: 21pt;text-align: left;"><p class="s6" style="display: inline;">c<span class="s27">2</span><span class="s40"> </span><span class="s7">· </span>S<span class="s8">[</span>π<span class="s14">ϕ</span><span class="s53">role</span><span class="s34"> </span><span class="s8">](</span><span class="h2">o</span><span class="s14">t</span><span class="s8">)</span><span class="s58">)</span></p></li></ul><p class="s10" style="padding-top: 10pt;text-indent: 0pt;text-align: left;">1    <span class="p">(15)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="268" height="141" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_006.png"/></span></p><p class="s4" style="padding-top: 4pt;padding-left: 2pt;text-indent: 0pt;text-align: center;"><a name="bookmark10">(a)</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="268" height="139" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_007.png"/></span></p><p class="s4" style="padding-top: 4pt;padding-left: 2pt;text-indent: 0pt;text-align: center;">(b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: center;">Fig. 3: Experimental scenario and generalization scenario settings (a) CARLA example map, (b) Real intersection map</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="text-indent: 0pt;line-height: 7pt;text-align: left;">t</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;">The PPO surrogate loss <i>L</i><span class="s59">CLIP </span><span class="s12"> </span>is defined as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="313" height="172" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_008.png"/></span></p><p style="padding-left: 59pt;text-indent: 0pt;text-align: left;"><a name="bookmark11">Fig. 4: Offline pre-training results</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l5"><li style="padding-left: 5pt;text-indent: 1pt;text-align: left;"><p class="s6" style="display: inline;"><a name="bookmark13">Baselines and Evaluation Metrics</a></p><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 9pt;line-height: 12pt;text-align: justify;">We used online-only MAPPO as part of an ablation study; we trained a MAPPO agent that undergoes no offline pre- training and starts training directly in CARLA from scratch (with the network structure and algorithm parameters being the same as the online phase of our proposed method), used to compare and evaluate the performance of our proposed algorithm. We will primarily compare them in terms of convergence speed. Additionally, we employ the open-source autonomous driving software stack, Autoware Universe [29] as a representative of traditional autonomous driving systems. In our experiments, Autoware is configured to control a single vehicle navigating the intersection within the identical</p><p class="s50" style="text-indent: 0pt;line-height: 12pt;text-align: left;">L<span class="s12">CLIP</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s49" style="padding-left: 35pt;text-indent: 0pt;line-height: 12pt;text-align: left;">t    <span class="s15"> </span><span class="s8">= min</span></p><p class="s6" style="text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s54">(</span>r<span class="s14">t</span>A<span class="s42">ˆ</span><span class="s14">t</span>, <span class="p">clip</span><span class="s8">(</span>r<span class="s14">t</span>, <span class="s8">1 </span><span class="s7">− </span>ϵ, <span class="s8">1 + </span>ϵ<span class="s8">)</span>A<span class="s42">ˆ</span><span class="s14">t</span></p><p style="padding-left: 27pt;text-indent: 0pt;line-height: 10pt;text-align: left;">(16)</p><p style="padding-left: 12pt;text-indent: 0pt;line-height: 12pt;text-align: left;">scenario used for our single-agent RL tests (i.e., 1 Autoware- controlled vehicle, 4 background Traffic Manager vehicles,</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">where <i>ϵ </i>denotes the PPO clipping hyperparameter, and <i>r</i><span class="s14">t</span></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;">denotes the probability ratio between the current policy and the old policy:</p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;">and 3 pedestrians). Its performance provides a benchmark for comparison against established methods.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 19pt;text-indent: -13pt;line-height: 5pt;text-align: left;"><p class="s6" style="display: inline;"><a name="bookmark14">Offline Pre-training Results</a></p><p class="s29" style="text-indent: 0pt;line-height: 7pt;text-align: right;"><span class="s41">π</span>ϕ<span class="s34">role </span><span class="s42">(</span><span class="h2">a</span>t<span class="s15"> </span><span class="s44">|</span><span class="s7"> </span><span class="h2">o</span>t<span class="s42">)</span></p><p style="padding-top: 4pt;padding-left: 66pt;text-indent: 0pt;line-height: 2pt;text-align: left;">(17)</p><p style="text-indent: 0pt;text-align: left;"><span><img width="72" height="1" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_009.png"/></span></p><p class="s6" style="text-indent: 0pt;line-height: 17pt;text-align: right;">r<span class="s14">t</span><span class="s15"> </span><span class="s8">= </span><span class="s60">π</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s29" style="text-indent: 0pt;text-align: left;">ϕ<span class="s34">old</span></p><p class="s8" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">(<span class="h2">a</span><span class="s14">t</span></p><p class="s7" style="padding-top: 3pt;padding-left: 1pt;text-indent: 0pt;line-height: 15pt;text-align: left;">| <span class="h2">o</span><span class="s14">t</span><span class="s8">)</span></p><p style="padding-top: 5pt;padding-left: 89pt;text-indent: 0pt;text-align: left;">The purpose of the offline pre-training phase is to extract</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">We further enhance training with adam optimizer and gradient clipping. These stabilizing techniques enable robust fine-tuning and effective adaptation to dynamic, multi-agent environments.</p></li></ol></li><li style="padding-top: 3pt;padding-left: 15pt;text-indent: 39pt;line-height: 16pt;text-align: left;"><p style="display: inline;"><a name="bookmark12">E</a><span class="s4">XPERIMENTS AND </span>A<span class="s4">NALYSIS </span>Experiments were conducted using the CARLA simula-</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark10" class="s5">tor paired with Unreal Engine in synchronous mode. The primary scenario involves one intersection within CARLA’s Town03 map (shown in Fig. </a>3). In this simulation scenario, a total of 5 vehicles are present: a variable number (1 to</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">3) CAVs, designated &quot;red&quot;, are controlled by the proposed system, while the remaining background vehicles, designated &quot;blue&quot;, are managed by CARLA’s Traffic Manager. Addition- ally, up to 3 pedestrians are randomly spawned on sidewalks and programmed to cross the road. A real intersection map based on the Institute of Science Tokyo campus was utilized for generalization testing. We assume the RSU possesses BEV perception and performs inference using the decision model obtained after online fine-tuning to determine driving strategies, subsequently sending control signals to the CAVs via simulated V2I communication.</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">effective driving priors from the InD dataset, providing a</p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark11" class="s5">high-quality model initialization for the online phase. As shown in Fig. </a>4, which displays the changes in the critic networks’ Q1 and Q2 losses and the reward improvement metric during offline training, we can observe the learning progress. The loss values steadily converge as training pro- gresses, indicating that the critic network effectively learned state-action value relationships from the offline data and that the training process possessed good stability. Furthermore, the reward improvement metric eventually stabilizes around 112%, exceeding the 100% baseline. This demonstrates that the policy learned via CQL combined with BC outperforms the average behavior present in the dataset in terms of optimizing the offline reward objective, successfully learning strategies beyond mere imitation, and providing a quality initialization basis for online fine-tuning.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l6"><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s6" style="display: inline;"><a name="bookmark15">Online Training Results</a></p><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark17" class="s5">To validate the effectiveness of  online  fine-tuning  and the value of offline pre-training, we compare the training progress of our proposed hybrid method against the online- only baseline. Fig. </a>5 presents the convergence curves for both</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="259" height="178" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_010.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 95pt;text-indent: 0pt;text-align: center;"><a name="bookmark17">(a)</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="266" height="178" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_011.png"/></span></p><p class="s4" style="padding-top: 6pt;padding-left: 95pt;text-indent: 0pt;text-align: center;">(b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Fig. 5: Comparison of online training w/ and w/o offline RL,</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">(a) reward, (b) success rate.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">average episode reward and success rate during the external online training phase for both methods.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark17" class="s5">As shown in Fig. </a>5, our proposed hybrid method, lever- aging the pre-trained model, exhibits a much higher initial average episode reward compared to the online-only method starting from scratch. Furthermore, the hybrid approach converges more rapidly to its final reward level, indicating that offline pre-training significantly accelerates the online learning process and contributes to achieving strong final performance. Additionally, the convergence trend for success rate mirrors that of the reward. Our proposed method reaches high success rates considerably faster, whereas the online- only method requires significantly more training episodes to approach similar levels of reliability. This comparison further demonstrates that offline pre-training markedly enhances both the efficiency of the online learning phase and the robustness of the ultimately learned policy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><p class="s6" style="display: inline;"><a name="bookmark16">Final Model Performance Evaluation and Generalization Analysis</a></p></li></ol><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark19" class="s5">We evaluated the model through 10,000 performance test episodes on both the Town03 intersection and the custom map, comparing it against baselines. A summary of key performance indicator comparisons is shown in Fig. </a><a href="#bookmark18" class="s5">6 and Tab. </a>I. The proposed algorithmic model demonstrates high safety and reliability across all test scenarios on the Town03</p><p style="padding-top: 3pt;text-indent: 0pt;text-align: center;"><a name="bookmark18">TABLE I: Performance comparison summary</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:22.192pt" cellspacing="0"><tr style="height:9pt"><td style="width:96pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s61" style="padding-left: 19pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Method / Scenario</p></td><td style="width:63pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s61" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Failure rate (%)</p></td><td style="width:51pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s61" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Avg. time (s)</p></td></tr><tr style="height:8pt"><td style="width:96pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="padding-left: 12pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Ours (1 Agent, Town03)</p></td><td style="width:63pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0.01</p></td><td style="width:51pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="text-indent: 0pt;line-height: 7pt;text-align: center;">5.52</p></td></tr><tr style="height:8pt"><td style="width:96pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="padding-left: 12pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Ours (2 Agent, Town03)</p></td><td style="width:63pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0.03</p></td><td style="width:51pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="text-indent: 0pt;line-height: 7pt;text-align: center;">5.49</p></td></tr><tr style="height:9pt"><td style="width:96pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="padding-left: 12pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Ours (3 Agent, Town03)</p></td><td style="width:63pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0.02</p></td><td style="width:51pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="text-indent: 0pt;line-height: 7pt;text-align: center;">5.25</p></td></tr><tr style="height:9pt"><td style="width:96pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Autoware (1 Agent, Town03)</p></td><td style="width:63pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="text-indent: 0pt;line-height: 7pt;text-align: center;">5.31</p></td><td style="width:51pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="text-indent: 0pt;line-height: 7pt;text-align: center;">5.77</p></td></tr><tr style="height:9pt"><td style="width:96pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="padding-left: 10pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Ours (3 Agent, Real Map)</p></td><td style="width:63pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0.02</p></td><td style="width:51pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s62" style="text-indent: 0pt;line-height: 7pt;text-align: center;">5.15</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="257" height="120" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_012.png"/></span></p><p class="s4" style="padding-top: 8pt;padding-left: 120pt;text-indent: 0pt;text-align: center;"><a name="bookmark19">(a)</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="257" height="120" alt="image" src="2025-Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections/Image_013.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 120pt;text-indent: 0pt;text-align: center;">(b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Fig. 6: Final model performance evaluation results (a) suc- cess rate by testing episodes, (b) average travel time by testing episodes</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">map. When controlling a single vehicle, the failure rate was 0.01%. Compared to the 5.31% failure rate exhibited by the Autoware baseline in the identical single-vehicle scenario, our single-agent controller shows higher performance.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Notably, despite the significant increase in coordination complexity when scaling from single- to multi-vehicle sce- narios, our system did not exhibit a marked decline in success rate. Specifically, failure rate was 0.03% in the two- vehicle coordination scenario and decreased to 0.02% in the three-vehicle coordination scenario. The combination of the RSU’s BEV perspective and the self-attention mechanism contributes to this robustness, demonstrating our method’s effectiveness in handling complex multi-agent cooperative tasks.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark19" class="s5">As shown in Fig. </a><a href="#bookmark18" class="s5">6 and Tab. </a>I, the traffic  efficiency results indicate that our method also demonstrated strong performance. The average travel time in the single-vehicle scenario was 5.52 seconds, outperforming the 5.77 seconds recorded by the Autoware baseline. As the number of con- trolled vehicles increased, the average travel time showed a slight downward trend: 5.49 seconds for the two-vehicle scenario and 5.25  seconds for the  three-vehicle scenario. This indicates that the multiple RL agents coordinated by our system formed a highly effective collaborative passage pattern. Their interactions proved more efficient than those with a larger number of background vehicles controlled by</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">the Traffic Manager.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Finally, in the generalization test, the three-vehicle model trained in Town03 was deployed on the real intersection map. The model achieved an extremely low failure rate of 0.02% and an average travel time of 5.15 seconds in this novel environment. This suggests that the expanded collective field of view inherent in the three-vehicle setup mitigated the impact of individual visual blind spots. Furthermore, as this map featured shorter traversal distance, it enabled our system to operate more smoothly and efficiently. This result strongly validates the excellent generalization capability of the learned policy, showcasing its adaptability to different environmental characteristics and providing a solid foundation for the practical application of the method.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 58pt;text-indent: -14pt;text-align: left;"><p style="display: inline;"><a name="bookmark20">C</a><span class="s4">ONCLUSION AND </span>F<span class="s4">UTURE </span>W<span class="s4">ORKS</span></p></li></ol><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">This research addresses the complex coordination chal- lenges at unsignalized intersections by proposing an RSU- based centralized cooperative driving framework. The frame- work employs a two-stage method to train the decision model: offline pre-training initializes policies, followed by online fine-tuning in the simulation environment. Extensive experiments demonstrate the method’s effectiveness, achiev- ing high success rate and strong coordination robustness in scenarios with up to three controlled vehicles. Future primary work involves the proof-of-concept (PoC) experiments to fully validate the system effectiveness in the real world.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 98pt;text-indent: 0pt;text-align: center;"><a name="bookmark21">R</a><span class="s4">EFERENCES</span></p><p class="s4" style="padding-top: 7pt;padding-left: 23pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[1] K. F. Chu, A. Lam, and V. Li, “Traffic signal control using end-to- end off-policy deep reinforcement learning,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. PP, pp. 1–12, 2021.</p><p style="padding-left: 23pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a href="https://highways.dot.gov/sites/fhwa.dot.gov/files/2024-08/MIRE_2.1_FINAL_508v3.pdf" class="a" target="_blank">[2] U.S. Department of Transportation, Federal Highway Administration. (2024, August) Mire 2.1: Model inventory of roadway elements. Re- port No. FHWA-SA-24-052. [Online]. Available: https://highways.dot. gov/sites/fhwa.dot.gov/files/2024-08/MIRE_2.1_FINAL_508v3.pdf</a></p><p style="padding-left: 23pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a href="https://nhtsa.gov/press-releases/nhtsa-2023-traffic-fatalities-2024-estimates" class="a" target="_blank">[3] National Highway Traffic Safety Administration. (2024, April) Nhtsa estimates 39,345 traffic fatalities in 2024. U.S. Department of Transportation. [Online]. Available: https://nhtsa.gov/press-releases/ nhtsa-2023-traffic-fatalities-2024-estimates</a></p><p class="s4" style="padding-left: 23pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[4] S. Chen, X. Hu, J. Zhao, R. Wang, and M. Qiao, “A review of decision-making and planning for autonomous vehicles in intersection environments,” <i>World Electric Vehicle Journal</i>, vol. 15, no. 3, p. 99, 2024.</p><p class="s4" style="padding-left: 23pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[5] K. Wang, C. She, Z. Li, T. Yu, Y. Li, and K. Sakaguchi, “Roadside units assisted localized automated vehicle maneuvering: An offline reinforcement learning approach,” in <i>2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC)</i>, 2024, pp. 1709–1715.</p><p class="s4" style="padding-left: 23pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[6] Z. Li, K. Wang, T. Yu, and K. Sakaguchi, “Het-SDVN: SDN-based radio resource management of heterogeneous V2X for cooperative perception,” <i>IEEE Access</i>, vol. 11, pp. 76 255–76 268, 2023.</p><p class="s4" style="padding-left: 23pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[7] D. Suo, B. Mo, J. Zhao, and S. E. Sarma, “Proof of travel for trust- based data validation in V2I communication,” <i>IEEE Internet of Things Journal</i>, vol. 10, no. 11, pp. 9565–9584, 2023.</p><p class="s4" style="padding-left: 23pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[8] H. Alemayehu and A. Sargolzaei, “Testing and verification of con- nected and autonomous vehicles: A review,” <i>Electronics</i>, vol. 14, no. 3, p. 600, 2025.</p><p class="s4" style="padding-left: 23pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[9] F. Sana, N. L. Azad, and K. Raahemiar, “Autonomous vehicle decision-making and control in complex and unconventional scenar- ios—a review,” <i>Machines</i>, vol. 11, no. 7, p. 676, 2023.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[10] Y. Zhu, Z. He, and G. Li, “A bi-hierarchical game-theoretic approach for network-wide traffic signal control using trip-based data,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 23, no. 9, pp. 15 408–15 419, 2022.</p><p class="s4" style="padding-top: 4pt;padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[11] M. Gallo, “Combined optimisation of traffic light control parameters and autonomous vehicle routes,” <i>Smart Cities</i>, vol. 7, no. 3, pp. 1060– 1088, 2024.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[12] M. Ghadi, “A grid-based framework for managing autonomous vehi- cles’ movement at intersections,” <i>Periodica Polytechnica Transporta- tion Engineering</i>, vol. 52, no. 3, pp. 235–245, 2024.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[13] Y. Shi, H. Dong, C. He, Y. Chen, and Z. Song,  “Mixed  vehicle platoon forming: A multi-agent reinforcement learning approach,” <i>IEEE Internet of Things Journal</i>, vol. PP, pp. 1–1, 01 2025.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[14] H. Taghavifar, C. Hu, C. Wei, A. Mohammadzadeh, and C. Zhang, “Behaviorally-aware multi-agent rl with dynamic optimization for autonomous driving,” <i>IEEE Transactions on Automation Science and Engineering</i>, vol. PP, pp. 1–1, 01 2025.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[15] Y. Zhang, R. Hao, T. Zhang, X. Chang, Z. Xie, and Q. Zhang, “A trajectory optimization-based intersection coordination framework for cooperative autonomous vehicles,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 23, no. 9, pp. 14 674–14 688, 2021.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[16] S. Iqbal and F. Sha, “Actor-attention-critic for multi-agent reinforce- ment learning,” <i>arXiv preprint arXiv:1810.02912</i>, 2019, iCML 2019 camera ready version.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[17] R. Younas, H. M. Raza Ur Rehman, I. Lee, B.-W. On, S. Yi, and G. S. Choi, “Sa-marl: Novel self-attention-based multi-agent reinforcement learning with stochastic gradient descent,” <i>IEEE Access</i>, vol. 13, pp. 35 674–35 687, 2025.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[18] W.-C. Tseng, T.-H. J. Wang, Y.-C. Lin, and P. Isola, “Offline multi- agent reinforcement learning with knowledge distillation,” in <i>Advances in Neural Information Processing Systems</i>, vol. 35. Curran Associates, Inc., 2022, pp. 226–237.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[19] Z. Li, F. Nie, Q. Sun, F. Da, and H. Zhao, “Boosting offline reinforce- ment learning for autonomous driving with hierarchical latent skills,” in <i>2024 IEEE International Conference on Robotics and Automation (ICRA)</i>, 2024, pp. 18 362–18 369.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[20] Z. Wang, G. Wu, and M. J. Barth, “Cooperative eco-driving at signalized intersections in a partially connected and automated ve- hicle environment,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 21, no. 5, pp. 2029–2038, 2019.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[21] Z. Wang, K. Han, and P. Tiwari, “Digital twin-assisted cooperative driving at non-signalized intersections,” <i>IEEE Transactions on Intelli- gent Vehicles</i>, vol. 7, no. 2, pp. 198–209, 2021.</p><p class="s4" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">[22]  K. Wang, Z. Li, K. Nonomura, T. Yu, K. Sakaguchi, O. Hashash, and</p><p class="s4" style="padding-left: 23pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">W. Saad, “Smart mobility digital twin based automated vehicle navi- gation system: A proof of concept,” <i>IEEE Transactions on Intelligent Vehicles</i>, pp. 1–14, 2024.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[23] K. Wang, T. Yu, Z. Li, K. Sakaguchi, O. Hashash, and W. Saad, “Dig- ital twins for autonomous driving: A comprehensive implementation and demonstration,” in <i>2024 International Conference on Information Networking (ICOIN)</i>, 2024, pp. 452–457.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[24] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA: An open urban driving simulator,” in <i>Proceedings of the 1st Annual Conference on Robot Learning</i>, 2017, pp. 1–16.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[25] J. Bock, R. Krajewski, T. Moers, S. Runde, L. Vater, and L. Eckstein, “The ind dataset: A drone dataset of naturalistic road user trajectories at german intersections,” in <i>2020 IEEE Intelligent Vehicles Symposium (IV)</i>, 2020, pp. 1929–1934.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[26] A. Kumar, A. Zhou, G. Tucker, and S. Levine, “Conservative q- learning for offline reinforcement learning,” in <i>Advances in Neural Information Processing Systems</i>, vol. 33. Curran Associates, Inc., 2020, pp. 1179–1191.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[27] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural network,” in <i>Advances in Neural Information  Processing  Systems</i>, vol. 1.   Morgan-Kaufmann, 1988.</p><p class="s4" style="padding-left: 23pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[28]  C.  Yu,  A.  Velu,  E.   Vinitsky,   J.   Gao,   Y.   Wang,   A.   Bayen, and Y. Wu, “The surprising effectiveness of ppo in cooperative, multi-agent games,” <i>arXiv preprint arXiv:2103.01955</i><a href="https://arxiv.org/abs/2103.01955" class="a" target="_blank">, 2021, accepted at NeurIPS 2022 Datasets and Benchmarks. [Online]. Available: https://arxiv.org/abs/2103.01955</a></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a href="https://autoware.org/" class="a" target="_blank">[29]  Autoware. [Online]. Available: https://autoware.org/.</a></p></body></html>
