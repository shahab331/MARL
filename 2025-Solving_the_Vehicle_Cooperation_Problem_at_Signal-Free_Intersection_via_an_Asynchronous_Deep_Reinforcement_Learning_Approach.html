<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Solving the Vehicle Cooperation Problem at Signal-Free Intersection via an Asynchronous Deep Reinforcement Learning Approach</title><meta name="author" content="Shuai Wang"/><meta name="description" content="IEEE Internet of Things Journal;2025;12;13;10.1109/JIOT.2025.3552058"/><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 24pt; }
 .s2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s4 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s5 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 28.5pt; }
 .a { color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s7 { color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s8 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s9 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s10 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s11 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s12 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s13 { color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 .s14 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s15 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s16 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s17 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s18 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s19 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -8pt; }
 .s20 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s21 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 8pt; }
 .s22 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -2pt; }
 .s23 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s24 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s25 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 6pt; }
 .s26 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -1pt; }
 .s27 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -1pt; }
 .s28 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 7pt; }
 .s29 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s30 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s31 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s32 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -3pt; }
 .s33 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s34 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 5pt; }
 .s35 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 4pt; }
 .s36 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 7.5pt; }
 .s37 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -7pt; }
 .s38 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -2pt; }
 .s39 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s40 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 4pt; }
 .s41 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 10pt; }
 .s42 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s43 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s44 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s45 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s46 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 4pt; }
 .s47 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s48 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s49 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -1pt; }
 .s50 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s51 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s52 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 11pt; }
 .s53 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s54 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4.5pt; }
 .s55 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 3.5pt; }
 .s56 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 4pt; }
 .s57 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -1pt; }
 .s58 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 9pt; }
 .s59 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -2pt; }
 .h2, h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s60 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s61 { color: #FFF; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s62 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -3pt; }
 .s63 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -3pt; }
 .s64 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -2pt; }
 .s65 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -5pt; }
 .s67 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 3pt; }
 .s68 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -5pt; }
 .s69 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 5pt; }
 .s70 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -3pt; }
 .s71 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 5pt; }
 .s72 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s73 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s74 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s75 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 2pt; }
 .s76 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 1pt; }
 .s77 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 h4 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 0; }
 #l1> li:before {counter-increment: c1; content: counter(c1, upper-roman)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l2 {padding-left: 0pt;counter-reset: d1 0; }
 #l2> li:before {counter-increment: d1; content: counter(d1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l3 {padding-left: 0pt;counter-reset: e1 0; }
 #l3> li:before {counter-increment: e1; content: counter(e1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l4 {padding-left: 0pt;counter-reset: f1 0; }
 #l4> li:before {counter-increment: f1; content: counter(f1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l5 {padding-left: 0pt;counter-reset: f2 0; }
 #l5> li:before {counter-increment: f2; content: counter(f2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l6 {padding-left: 0pt;counter-reset: g1 0; }
 #l6> li:before {counter-increment: g1; content: counter(g1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l7 {padding-left: 0pt;counter-reset: g2 0; }
 #l7> li:before {counter-increment: g2; content: counter(g2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l8 {padding-left: 0pt;counter-reset: g2 0; }
 #l8> li:before {counter-increment: g2; content: counter(g2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l9 {padding-left: 0pt;counter-reset: h1 0; }
 #l9> li:before {counter-increment: h1; content: counter(h1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l10 {padding-left: 0pt;counter-reset: g1 1; }
 #l10> li:before {counter-increment: g1; content: counter(g1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l11 {padding-left: 0pt;counter-reset: g2 2; }
 #l11> li:before {counter-increment: g2; content: counter(g2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l12 {padding-left: 0pt;counter-reset: h2 0; }
 #l12> li:before {counter-increment: h2; content: "("counter(h2, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 #l13 {padding-left: 0pt;counter-reset: i1 0; }
 #l13> li:before {counter-increment: i1; content: counter(i1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
</style></head><body><p class="s1" style="padding-left: 34pt;text-indent: 8pt;text-align: center;">Solving the Vehicle Cooperation Problem at Signal-Free Intersection via an Asynchronous Deep Reinforcement Learning Approach</p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_001.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_002.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_003.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_004.png"/></span></p><p class="s2" style="padding-top: 6pt;padding-left: 63pt;text-indent: 0pt;text-align: center;">Shuai Wang   , Yuhao Ding   , Xiaoqi Ding   , and Xiaojun Tan   , <i>Senior Member, IEEE</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: justify;">Abstract<span class="h3">—With the rapid development of modern intelli- gent transportation systems, connected and automated vehicles (CAVs) have garnered significant attention due to their advanced communication and decision-making capabilities. Intelligent col- laborative decision-making in dynamic traffic scenarios poses significant challenges in the research of CAV technology. Strategies and methods have been developed to tackle the collaboration problem toward different scenarios. However, most existing studies have not been fully investigated the performance optimization and practicability  of  the  vehicle  collaboration at signal-free intersections. Meanwhile, the urban signal-free intersections represent a critical application scenario for vehicle cooperation, where  a thorough  study has  not been  given yet. Therefore, this study intends to solve the vehicle collaboration problem utilizing the deep reinforcement learning approach. Initially, the problem is formulated as an elaborated Markov decision process, comprising the state space, the action space, and the reward function. Then, a shared advantage actor–critic (A2C) model is proposed to effectively extract temporal and spatial features through the shared network, thereby improving the consistency of feature learning processes between the Actor and Critic networks. Furthermore, the asynchronous training strategy employed in this study involves multiple training processes concurrently, thereby enhancing the model’s convergence speed and stability. Finally, the effectiveness of the proposed method is verified in two typical intersection scenarios. Simulation results reveal that our method exhibits competitive performance compared with existing  approaches,  and  an  improvement  up to 30% and 40% can be achieved in the retreat time and the averaged delay time. Additionally, the field experiments have been conducted on a miniaturized autonomous driving platform, verifying the considerable potential of the proposed method for real-world applications.</span></p><p class="s4" style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: justify;">Index Terms<span class="h3">—Connected and automated vehicles (CAVs), deep reinforcement learning (DRL), miniaturized autonomous driving platform, signal-free intersections, vehicle cooperation.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 13pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Received 5 September 2024; revised 15 December 2024, 9 January 2025,</p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">and  18  February  2025;  accepted  12  March  2025.  Date  of  publication</p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">18 March 2025; date of current version 27 June 2025. This work was supported in part by the Guangdong Basic and Applied Basic Research Foundation under Grant 2024A1515010238; in part by the Shenzhen Science and Technology Program under Grant KJZD20231023100204008; and in part by the National Natural Science Foundation of China under Grant 62203477. <i>(Corresponding author: Xiaojun Tan.)</i></p><p class="s5" style="padding-left: 5pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">Shuai Wang, Yuhao Ding, and Xiaojun Tan are with the School of Intelligent Systems Engineering, Sun Yat-sen Univerity (Shenzhen), Shenzhen 518107, China (e-mail: wangsh368@mail.sysu.edu.cn; dingyh26@mail2.sysu.edu.cn; tanxj@mail.sysu.edu.cn).</p><p class="s5" style="padding-left: 5pt;text-indent: 7pt;text-align: justify;">Xiaoqi Ding is with the School of Computer Science and Engineering, Sun Yat-sen Univerity (Guangzhou), Guangzhou 510000, China (e-mail: dingxq7@mail2.sysu.edu.cn).</p><p class="s5" style="padding-left: 13pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Digital Object Identifier 10.1109/JIOT.2025.3552058</p><ol id="l1"><li style="padding-top: 3pt;padding-left: 104pt;text-indent: -11pt;text-align: left;"><p style="display: inline;">I<span class="s5">NTRODUCTION</span></p><h1 style="text-indent: 0pt;line-height: 29pt;text-align: left;">T</h1><p style="text-indent: 0pt;text-align: left;"/><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 20pt;text-align: right;"><a href="#bookmark76" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark0">HE RAPID advancement of connected and automated vehicle (CAV) technology has propelled vehicle coop- eration in dynamic traffic scenarios to an essential research frontier within the autonomous driving domain </a>[1]<a href="#bookmark77" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. The col- laborative management of signal-free intersections represents significance for CAVs in urban traffic. Intersections without traffic  lights  have  become  a  critical  challenge  in  modern intelligent  transportation  systems  (ITSs)  as  the  number  of vehicles continues to increase </a>[2]<a href="#bookmark78" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[3]<a href="#bookmark79" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. In the United States and Europe, over 30% of traffic jams and accidents occur at signal-free intersections </a>[4]<a href="#bookmark80" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[5]<span style=" color: #000;">. Therefore, it is crucial to design an effective signal-free intersection cooperation system</span><a name="bookmark1">&zwnj;</a><a name="bookmark2">&zwnj;</a></p><p style="padding-left: 15pt;text-indent: -10pt;text-align: left;">to enhance traffic throughput and ensure driving safety.</p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: right;"><a href="#bookmark81" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark3">Advances  in  vehicle-to-infrastructure  (V2I)  technologies have mitigated communication constraints </a>[6]<a href="#bookmark82" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[7]<a href="#bookmark83" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, rendering multivehicle collaboration feasible in the intersection man- agement. It is considered practical for vehicles to cooperate efficiently  under  the  coordination  of  a  centralized  system deployed along the roadside </a>[8]<a href="#bookmark84" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[9]<a href="#bookmark85" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Vehicles can share their driving  states  and  intentions  with  the  central  coordinator. Such coordinator can then direct the vehicles to leave the intersection safely according to an effective collaborative algo- rithm, thereby optimizing the management of the intersection. In  recent  years,  significant  research  efforts  have  been expended on addressing the challenges of coordinating with vehicles at intersections </a>[10]<a href="#bookmark86" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. These challenges have been for- mulated as optimization and game theory problems, employing optimization techniques to derive vehicle control strategies that achieve a global Nash equilibrium by modeling vehicle inter- actions </a>[11]<a href="#bookmark87" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Additionally, some researchers have considered the graph theory to graphically model the vehicle road network and  facilitate  the  cooperative  decision-making  of  vehicles through  the  analysis  and  processing  of  these  graphs  </a>[12]<a href="#bookmark88" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Deep reinforcement learning (DRL) methods </a>[13]<a href="#bookmark89" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[14] <span style=" color: #000;">have been demonstrated to be with efficient learning ability and distinct flexible decision-making capability. By learning from the agent’s interactions with the environment, the coordinator can  train  effective  strategies  to  guide  the  vehicle  through collaborative tasks. However, among the existing methods, the practicality and high performance in intersection management remain a significant challenge. Some methods may struggle to  provide  real-time  solutions  under  complex  intersection environments, while others have performance improvement</span><a name="bookmark4">&zwnj;</a><a name="bookmark5">&zwnj;</a><a name="bookmark6">&zwnj;</a><a name="bookmark7">&zwnj;</a><a name="bookmark8">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">potential.</p><p class="s5" style="padding-top: 3pt;padding-left: 58pt;text-indent: 0pt;line-height: 11pt;text-align: left;">2327-4662 <span class="s8">Q</span>c</p><p class="s5" style="padding-top: 5pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">2025 IEEE. All rights reserved, including rights for text and data mining, and training of artificial intelligence</p><p class="s5" style="padding-left: 86pt;text-indent: 0pt;line-height: 8pt;text-align: center;">and similar technologies. Personal use is permitted, but republication/redistribution requires IEEE permission.</p><p class="s5" style="padding-left: 83pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a href="http://www.ieee.org/publications/rights/index.html" class="s9" target="_blank">See </a>https://www.ieee.org/publications/rights/index.html for more information.</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Given the aforementioned limitations of existing methods, this study introduces a DRL approach to  enable  multive- hicle interconnection coordination at intersections. We first introduce a hierarchical coordination framework that segments continuous traffic flow into distinct batches of vehicles. For each batch, the cooperation problem is formulated as a Markov decision process, with a customized state space, action space, and reward function. Subsequently, a shared advantage actor–critic (A2C) network model is employed to effectively learn and extract environmental features through the shared network architecture. Furthermore, an asynchronous training strategy is developed to enhance the speed and stability of model training. The simulation results demonstrate that the proposed method significantly reduces retreat time and averaged delay time by 30% and  40%,  respectively. Field experiments conducted on a miniaturized autonomous driving platform further validate our proposed method’s appli- cation potential and practical value in real-world scenarios. Additionally, the limitations of communication delay factors on the actual deployment of our method are tested and analyzed in detail.</p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;">The contributions of this article are as follows.</p><ol id="l2"><li style="padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">In terms of the problem representative approach. Considering the practical challenges of urban signal-free intersections, this study proposes a novel paradigm for the intersection vehicle coordination problem, modeling it as a Markov decision model. The  DRL  frame- work is employed, and effective state representations are developed for the intersection environment, vehicle action representations, and reward functions to guide the vehicles’ decision-making.</p></li><li style="padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">In terms of the training strategy. To address the performance limitations of existing methods, this study introduces an elaborate parameter sharing A2C model to improve the consistency and effectiveness of feature learning. The asynchronous training strategy is also employed to expand the exploration of the environment, accelerating the training process. With these strategies, the overall performance of the model can be enhanced dramatically.</p></li><li style="padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">In terms of the test scene. Numerical experiments have been conducted to validate that our proposed method can significantly reduce the retreat time and averaged delay time at signal-free intersections with less computation time. Additionally, field experiments have also been involved to verify the practical applicability under real- world scenarios. The robustness of our method against specific communication delays has been verified across various scenarios. A competitive solver can be offered to decision-makers to address traffic jams and intersection managements.</p></li></ol><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark9" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The remainder of this article is organized as follows. Section </a>II <a href="#bookmark21" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">reviews and summarizes  the  related  work. Section </a>III <a href="#bookmark44" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">is the problem statement. In Section </a>IV<a href="#bookmark52" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, our proposed method is introduced in detail. The simulation results are presented in Section </a>V<a href="#bookmark66" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Section </a>VI <a href="#bookmark75" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">demonstrates the field experiments deployed on the miniaturized autonomous driving platform. Section </a>VII <span style=" color: #000;">summarizes this article.</span></p></li><li style="padding-top: 2pt;padding-left: 104pt;text-indent: -15pt;text-align: left;"><p style="display: inline;"><a name="bookmark9">R</a><span class="s5">ELATED </span>W<span class="s5">ORK</span></p><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark81" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark10">In the modern ITSs, multivehicle cooperative driving enhances the intelligence, safety, and efficiency of traffic man- agement </a>[6]<a href="#bookmark85" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[10]<a href="#bookmark90" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[15]<span style=" color: #000;">. Consequently, the extensive research has focused on developing vehicle coordination strategies for various typical collaborative driving scenarios.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l3"><li style="padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s10" style="display: inline;">Methods Based on Optimization or Game Theory</p><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark91" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark11">In traffic management, vehicle cooperation problems are initially modeled as the optimization or game theory problems. Zhao et al. </a>[16] <a href="#bookmark83" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">transformed the cooperation problem at signal-free intersections into a multiobjective optimization problem and developed a privacy policy based on affine masking to safeguard data privacy. An integrated-oriented two-layer framework was developed by Li et al. </a>[8]<a href="#bookmark92" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. This framework transformed the intersection coordination problem into a sequential linear programming problem and proposed a low-complexity optimization scheme. Ji et al. </a>[17] <a href="#bookmark93" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">modeled and evaluated the aggression of interactive vehicles during overtaking tasks and proposed an adaptive decision-making strategy for overtaking based  on  game  theory.  To  achieve the optimal overtaking strategy in the dynamic environment, Gong et al. </a>[18] <a href="#bookmark94" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">proposed a game theory algorithm based on an interactive behavior model to calculate safe overtaking timing and reasonable overtaking methods. In the on-ramp merging scenario, Yang et al. </a>[19] <span style=" color: #000;">proposed a multivehicle cooperative strategy based on cooperative game theory, which considered the cost functions of multiple driving factors to determine the optimal confluence sequence of vehicles in different lanes.</span><a name="bookmark12">&zwnj;</a><a name="bookmark13">&zwnj;</a><a name="bookmark14">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s10" style="display: inline;">Methods Based on Graph Theory</p><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark95" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark15">The graph theory effectively represents the communication and interaction relationships between vehicles, leading to the recent application of graph-based methods in vehicle cooper- ation. Chen et al. </a>[20] <a href="#bookmark96" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">decomposed the intersection vehicle cooperation problem into an optimal sequence determination problem, and proposed IDFST method and MCC method through optimizing the ergodic search method. Li et al. </a>[21] <a href="#bookmark97" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">divided the intersection area into multiple subareas and developed a heuristic search algorithm based on conflict graphs. Shi et al. </a>[22] <a href="#bookmark98" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">represented the vehicle information for on-ramp vehicles and trunk vehicles engaged in collaborative lane changing within an information space. They introduced a dynamic conflict graph and a heuristic search strategy to derive the optimal control strategy. Shi et al. </a>[23] <span style=" color: #000;">proposed a vehicle final state phase diagram model with flexible junction points for ramp convergence. They incorporated heuristic pruning rules and rolling time domain optimization methods into the depth-first search strategy to enhance overall traffic efficiency and reduce vehicle delays.</span><a name="bookmark16">&zwnj;</a><a name="bookmark17">&zwnj;</a><a name="bookmark18">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: left;"><p class="s10" style="display: inline;">Methods Based on Deep Reinforcement Learning</p></li></ol><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark99" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark19">DRL methods can continuously learn and optimize strategies through interactions with the environment, demon- strating strong adaptive learning capabilities and robustness. Consequently, it has emerged as a prominent research direc- tion  in  vehicle  cooperation.  Guan  et  al.  </a>[24]  <span style=" color: #000;">introduced  a</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="289" height="276" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_005.png"/></span></p><p style="padding-top: 6pt;padding-left: 268pt;text-indent: 0pt;text-align: justify;">graph-based methods offer efficient computing and processing capability, but the simplification of vehicle interaction results in the redundancy of traffic resources and reducing cooper- ative efficiency. The DRL methods offer several advantages, including adaptability, real-time performance, and enhanced decision accuracy. However, the application of them in the context of intersection management has not been thoroughly explored and demonstrated. A rational and efficient vehicle cooperation strategy for signal-free intersections remains to be addressed.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark20">Fig. 1.   Typical signal-free intersection scenario.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 6pt;text-indent: 0pt;line-height: 223%;text-align: left;">Conflict Zone Preparatory Zone</p><p class="s11" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 285%;text-align: left;">Vehicle (First Batch) Vehicle (Other Batches) Central Coordinator</p></li><li style="padding-top: 3pt;padding-left: 93pt;text-indent: -19pt;text-align: left;"><p style="display: inline;"><a name="bookmark21">P</a><span class="s5">ROBLEM </span>S<span class="s5">TATEMENT</span></p><ol id="l4"><li style="padding-top: 3pt;padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s10" style="display: inline;">Signal-Free Intersection Scene</p><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark20" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Fig. </a>1 <a href="#bookmark100" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">presents a typical signal-free intersection scenario, which can be divided into preparatory and conflict zones. According to the assumption made without loss of general- ity </a>[25]<span style=" color: #000;">, the preparatory zone is defined as the driving area within a ring with an outer radius of 100 m and an inner radius of 15 m. The conflict zone is defined as the driving area</span></p><p class="s7" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark100" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark22">prior model into the proximal policy optimization (PPO) algorithm within the intersection scenario, which  acceler- ated the training process and  enhanced  sample  efficiency. Luo et al. </a>[25] <a href="#bookmark101" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">developed a unified collaborative trajectory planning framework aimed at maximizing traffic throughput. They implemented a centralized coordinator based on the twin delayed deep deterministic policy gradient (TD3) algorithm for model training. Guo et al. </a>[26] <a href="#bookmark102" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">combined the vehicle conflict graph with DRL to guide CAVs through intersections efficiently and safely by employing a heuristic action mask. Peng et al. </a>[27] <a href="#bookmark103" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">applied PPO  method  to  optimize  traffic flow and enhance traffic efficiency at the 8-shaped uncon- trolled intersection through a leader-follow  driving  mode. Wu et al. </a>[28]  <a href="#bookmark104" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">combined  the  artificial  potential  field  with the TD3 algorithm to develop  a  DRL  method  based  on risk perception in vehicle overtaking and lane change tasks, aiming to reduce the potential  driving  risks.  To  enhance the overall efficiency of vehicle collaborative  overtaking, Qian et al. </a>[29] <span style=" color: #000;">employed a parameterized dueling </span><span class="s10">Q</span><a href="#bookmark105" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">-network algorithm to learn vehicle driving strategies and designs a dynamic event-triggering mechanism based on multiple cri- teria and confidence  intervals.  Chen  et  al.  </a>[30]  <a href="#bookmark106" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">developed an efficient  and  scalable  DRL framework for  the on-ramp converging task, incorporating parameter sharing and a local reward mechanism to enhance cooperation among vehicles. Zhang et al. </a>[31] <span style=" color: #000;">proposed an independent PPO (IPPO) method, significantly improving the decision success rate of CAVs in the on-ramp converging task.</span><a name="bookmark23">&zwnj;</a><a name="bookmark24">&zwnj;</a><a name="bookmark25">&zwnj;</a><a name="bookmark26">&zwnj;</a><a name="bookmark27">&zwnj;</a><a name="bookmark28">&zwnj;</a></p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark107" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark29">In general, research  on  vehicle  cooperation  plays  a crucial role in the development  of  modern  ITSs.  Signal- free intersections are  a critical component of urban traffic networks. The study of collaborative strategies for signal-free intersections will facilitate the application and popularization of CAV technology, significantly enhancing the intelligence of urban traffic systems </a>[32]<a href="#bookmark108" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[33]<a href="#bookmark109" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[34]<span style=" color: #000;">. In the existing literature, while methods based on optimization and game theory can achieve detailed vehicle interaction modeling, they exhibit significant drawbacks, including low computational efficiency and poor real-time performance in complex scenarios. The</span></p><p class="s7" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark110" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark30">within a circle with a radius of 15 m. Within the preparatory zone, vehicles approach the intersection by driving along lanes in different directions, ultimately converging into the central conflict zone. In the conflict zone, there are 12 reference driving routes that are spatially blended relative to each other. Vehicles engage in continuous and complex interactions until they leave the zone. Simultaneously, a roadside central coordinator is deployed near the  intersection  to  assist  in the collaborative driving of vehicles. According to the V2I communication standard, the optimal communication range between the central coordinator and vehicles is between 300 and 500 m </a>[35]<span style=" color: #000;">, which is sufficient to cover the entire intersection area.</span></p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark111" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark31">In the decision-making problem at signal-free intersections, the primary challenge lies in facilitating effective cooperation among vehicles with conflicting routes within the conflict area. To facilitate the analysis, the following assumptions are made without loss of generality </a>[36]<span style=" color: #000;">.</span></p><ol id="l5"><li style="padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">In order to ensure the driving safety, overtaking, lane change and U-turn of vehicles in the preparatory zone are prohibited.</p></li><li style="padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">In the conflict zone, vehicles are expected to adhere to pe-defined driving routes.</p></li><li style="padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">Vehicles and the central coordinator are equipped with communication systems that yield low-delay and zero- packet-loss conditions. Vehicles can share their driving state—including speed, position, and driving route— with the central coordinator in real-time. In turn, the coordinator can broadcast the calculation results to the vehicles in a timely manner.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Although favorable communication conditions are assumed in the assumptions, the robustness of the proposed method to communication delays is discussed in the experimental sections.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s10" style="display: inline;">Vehicle Model</p><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark33" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">In order to improve the confidence of the scene, the employed vehicle model is the bicycle kinematics model, as depicted in Fig. </a>2<span style=" color: #000;">. The bicycle model is extensively utilized in</span></p><p class="s5" style="padding-top: 3pt;padding-left: 82pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark32">TABLE I</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="464" height="14" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_006.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="48" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_007.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_008.png"/></span></p><p class="s5" style="padding-left: 82pt;text-indent: 0pt;line-height: 9pt;text-align: center;">S<span class="s12">CENE AND </span>C<span class="s12">ONTROLLER </span>P<span class="s12">ARAMETERS IN </span>S<span class="s12">IMULATION </span>E<span class="s12">XPERIMENTS</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="462" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_009.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="464" height="86" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_010.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 96pt;text-indent: 0pt;line-height: 7pt;text-align: left;">	<span><img width="100" height="32" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_011.png"/></span><span><img width="93" height="10" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_012.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="462" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_013.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-top: 5pt;text-indent: 0pt;text-align: right;">(     )</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 5pt;text-indent: 0pt;text-align: right;"><a name="bookmark33">Fig. 2.   Bicycle kinematics model.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="141" height="143" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_014.png"/></span></p><p class="s12" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">Route</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="323" height="171" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_015.png"/></span></p><p class="s12" style="text-indent: 0pt;text-align: left;">Preview Point</p><p style="text-indent: 0pt;text-align: left;"><span><img width="81" height="59" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_016.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="71" height="94" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_017.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="148" height="57" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_018.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="29" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_019.png"/></span></p><p class="s5" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark34">TABLE II</a></p><p class="s5" style="padding-left: 38pt;text-indent: 0pt;line-height: 9pt;text-align: center;">P<span class="s12">ROPOSED </span>M<span class="s12">ETHOD </span>P<span class="s12">ARAMETERS</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark112" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark35">research on vehicle cooperative strategies </a>[37]<a href="#bookmark113" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[38]<a href="#bookmark114" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[39]<a href="#bookmark115" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[40] <a href="#bookmark116" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">due to its robust physical interpretability and computational efficiency </a>[41]<span style=" color: #000;">, which effectively enhance training efficiency and improve the stability of strategies. The state vector of the vehicle is defined as [</span><span class="s10">x</span><span class="s15">, </span><span class="s10">y</span><span class="s15">,θ, </span><span class="s10">v</span><span style=" color: #000;">]</span><span class="s16">T</span><span class="s17"> </span><span style=" color: #000;">, and the control vector is [</span><span class="s10">a</span><span class="s15">, δ</span><span style=" color: #000;">]</span><span class="s16">T</span><span class="s17"> </span><span style=" color: #000;">. The state transition equation is given as follows:</span><a name="bookmark36">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 77pt;text-indent: 0pt;line-height: 9pt;text-align: center;">TABLE III</p><p class="s5" style="padding-left: 78pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark37">PPO M</a><span class="s12">ETHOD </span>P<span class="s12">ARAMETERS</span></p><p class="s18" style="text-indent: 0pt;line-height: 19pt;text-align: right;">⎡<span class="s19">x</span><span class="s20">˙ </span>⎤</p><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 17pt;text-align: left;">⎡<span class="s19">x</span>⎤</p><p style="text-indent: 0pt;text-align: left;"><span><img width="321" height="19" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_020.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="322" height="117" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_021.png"/></span></p><p class="s21" style="padding-left: 10pt;text-indent: 0pt;line-height: 17pt;text-align: left;">⎡<span class="s18"> </span><span class="p">cos</span><span class="s15">(θ ) </span>⎤</p><p class="s18" style="text-indent: 0pt;line-height: 9pt;text-align: right;">⎢<span class="s22">y</span><span class="s20">˙ </span>⎥</p><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 9pt;text-align: left;">⎢<span class="s22">y</span>⎥</p><p class="s23" style="padding-left: 10pt;text-indent: 0pt;line-height: 9pt;text-align: left;">⎢<span class="s18"> </span><span class="p">sin</span><span class="s15">(θ)</span></p><p class="s10" style="padding-left: 3pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="s21">⎥</span>v<span class="s20">∗</span>t                   <span class="p">(1)</span></p><p class="s23" style="padding-left: 63pt;text-indent: 0pt;line-height: 0pt;text-align: left;">⎣<span class="s15">θ</span><span class="s24">˙</span><span class="s18">⎦ </span><span class="s25">=</span><span class="s20"> </span>⎣<span class="s15">θ </span>⎦<span class="s18"> </span><span class="s25">+</span><span class="s20"> </span>⎣<span class="p">tan</span><span class="s15">(δ)/</span><span class="s10">L</span>⎦</p><p class="s18" style="padding-left: 63pt;text-indent: 0pt;line-height: 4pt;text-align: center;">⎢ ⎥  ⎢ ⎥</p><p class="s10" style="padding-top: 7pt;padding-left: 63pt;text-indent: 0pt;text-align: center;">v<span class="s20">˙          </span>v</p><p class="s18" style="padding-left: 9pt;text-indent: 0pt;line-height: 4pt;text-align: center;">⎢       ⎥</p><p class="s10" style="padding-top: 8pt;padding-left: 9pt;text-indent: 0pt;text-align: center;">a<span class="s15">/</span>v</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">where <i>x </i>and <i>y </i>represent the position of the vehicle, <span class="s15">θ </span>and <i>v </i>denote the heading angle and velocity, <i>a </i>and <span class="s15">δ </span>represent the vertical acceleration and the lateral steering angle, <i>L </i>indicates the longitudinal axis length of the model.</p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark38"><span style=" color: #000;">According to assumption 2, vehicles are designed to follow the predetermined routes. To achieve precise lateral control, </span></a><span class="s15">δ </span><a href="#bookmark117" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">is calculated by the classic pure pursuit algorithm, which is frequently integrated with the bicycle model to ensure efficient and smooth path following in autonomous driving </a>[42]<a href="#bookmark118" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[43]<a href="#bookmark33" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. As illustrated in Fig. </a>2<span style=" color: #000;">, the target route is discrete into a series of points. At each time step, the preview point on the trajectory is determined based on the vehicle’s current position and the</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">where <span class="s15">α(</span><i>t</i><span class="s15">) </span>represents the angle between the preview point and the vehicle body, <i>l</i><span class="s26">d </span><span class="s17"> </span>indicates the previewing distance.</p><p style="padding-left: 5pt;text-indent: 9pt;line-height: 11pt;text-align: left;">To enhance the realism of the model, the critical factors</p><p class="s7" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark32" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">affecting vehicle dynamics are also considered, and the fol- lowing dynamic constraints are incorporated. The relevant parameter descriptions are provided in Tables </a>I <a href="#bookmark71" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">and </a><a href="#bookmark71" class="a">VII</a></p><p class="s18" style="text-indent: 0pt;line-height: 4pt;text-align: left;">⎪</p><p style="text-indent: 0pt;text-align: left;"/><p class="s20" style="padding-left: 76pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s21">⎧</span><span class="s18"> </span><span class="p">0 </span>≤ <span class="s10">v </span>≤ <span class="s10">v</span><span class="s27">max</span></p><p class="s18" style="padding-left: 76pt;text-indent: 0pt;line-height: 5pt;text-align: left;">⎪</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">previewing distance. Subsequently, the required steering angle is calculated using the angle deviation and distance deviation, enabling the vehicle to accurately track the target path. The</p><p class="s20" style="padding-left: 15pt;text-indent: -9pt;line-height: 11pt;text-align: left;"><span class="s28">⎨</span><span class="s18"> </span><span class="s29">δ</span><span class="s30">min </span><span class="s31">≤</span> <span class="s15">δ </span>≤ <span class="s15">δ</span><span class="s30">max</span></p><p class="s20" style="text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s32">⎪        </span><span class="s18"> </span>≤ <span class="s10">a </span>≤ <span class="s10">a</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s33" style="padding-left: 15pt;text-indent: 0pt;line-height: 6pt;text-align: left;">a<span class="s30">min                   max</span></p><p class="s18" style="padding-left: 5pt;text-indent: 0pt;line-height: 3pt;text-align: left;">⎪</p><p class="s15" style="padding-left: 5pt;text-indent: 0pt;line-height: 16pt;text-align: left;"><span class="s34">⎩</span><span class="s18"> </span><span class="s20">∗</span>δ<span class="s27">min</span><span class="s30"> </span><span class="s20">≤ ∗</span>δ <span class="s20">≤ ∗</span>δ<span class="s27">max</span>.</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">(3)</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">calculation formula is as follows:</p><p class="s15" style="padding-left: 122pt;text-indent: 0pt;line-height: 13pt;text-align: center;"><a name="bookmark39"><span class="s28">(</span></a><span class="s18"> </span><span class="p">2Lsin</span>(α(<span class="s10">t</span>)) <span class="s28"> </span></p><p class="s20" style="text-indent: 0pt;line-height: 9pt;text-align: left;">=</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-left: 71pt;text-indent: 0pt;line-height: 8pt;text-align: left;">δ(<span class="s10">t</span>)  <span class="p">tan</span><span class="s35">−</span><span class="s30">1    </span><span class="s36">                         </span></p><p class="s10" style="text-indent: 0pt;line-height: 10pt;text-align: right;">l<span class="s26">d</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">(2)</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Since communication delays are inevitable in real-world scenarios, a redundancy mechanism is introduced to mitigate the  negative  effects  as  much  as  possible.  Specifically,  in</p><p class="s5" style="padding-top: 3pt;padding-left: 82pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark40">TABLE IV</a></p><p class="s5" style="padding-left: 82pt;text-indent: 0pt;line-height: 9pt;text-align: center;">S<span class="s12">TATISTICAL </span>R<span class="s12">ESULTS OF </span>ANOVA</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 96pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="444" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_022.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 96pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="439" height="23" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_023.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 96pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="446" height="34" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_024.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 96pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="446" height="34" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_025.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 96pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="446" height="34" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_026.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 96pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="446" height="35" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_027.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="81" height="103" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_028.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="33" height="3" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_029.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="311" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_030.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="311" height="14" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_031.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="8" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_032.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_033.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="8" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_034.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="21" height="8" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_035.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="19" height="5" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_036.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="311" height="14" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_037.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="23" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_038.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="23" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_039.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="23" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_040.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="23" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_041.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="23" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_042.png"/></span></p><p style="padding-left: 61pt;text-indent: -25pt;line-height: 144%;text-align: left;"><span><img width="32" height="16" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_043.png"/></span> <span class="s12">OBB with Safety Redundancy OBB Projection</span></p><p class="s5" style="padding-top: 3pt;padding-left: 34pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark41">TABLE V</a></p><p class="s5" style="padding-left: 35pt;text-indent: 0pt;line-height: 9pt;text-align: center;">C<span class="s12">OMPUTATIONAL </span>T<span class="s12">IME OF </span>T<span class="s12">ESTED </span>M<span class="s12">ETHODS</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;line-height: 3pt;text-align: left;"><span><img width="132" height="84" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_044.png"/></span>	<span><img width="83" height="4" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_045.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark42">Fig. 3.   OBB collision detection.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="240" height="5" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_046.png"/></span></p><p class="s14" style="text-indent: 0pt;text-align: right;">The axis</p><p style="text-indent: 0pt;text-align: left;"><span><img width="24" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_047.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="8" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_048.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_049.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="8" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_050.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="22" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_051.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="24" height="8" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_052.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="311" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_053.png"/></span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">preparatory area to the conflict area, need to stop in front of the conflict area or maintain a safe distance from the vehicle ahead. The initial batch of vehicles departing the conflict central zone triggers an update to the serial numbers of the subsequent batches, enabling the new first batch to continue</p><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark100" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">vehicle interactions, vehicle positioning information may be inaccurate due to communication delays or noise. To reduce the impact of such issues on vehicle cooperation tasks, the oriented bounding  box  (OBB)  algorithm  </a>[25]  <a href="#bookmark42" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">is  employed to  model  vehicle  interactions.  As  shown  in  Fig. </a>3<span style=" color: #000;">,  each</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">entering the conflict zone and coordinate operations.</p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark119" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark43">When vehicles operate in the preparatory zone, the intel- ligent driver model (IDM) </a>[44] <span style=" color: #000;">is employed for vertical control to accurately characterize driver behavior in real-world scenarios. The control command for IDM is given as follows:</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">vehicle is represented by an OBB, in which appropriate safety redundancy is added to the box to account for potential noise and inaccuracies in the positioning data. The Separation Axis</p><p class="s30" style="padding-top: 10pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><span class="s33">a</span>IDM <span class="s31">=</span><span class="s20"> </span><span class="s10">a</span>max</p><p class="s18" style="padding-left: 21pt;text-indent: 0pt;line-height: 14pt;text-align: center;">( <span class="s37">v  </span><span class="s10"> </span><span class="s38">β</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_054.png"/></span></p><p class="s20" style="text-indent: 0pt;line-height: 9pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: left;">1</p><p class="s10" style="padding-left: 17pt;text-indent: 0pt;line-height: 10pt;text-align: center;">v<span class="s27">0</span></p><p class="s10" style="padding-left: 11pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s28">(</span><span class="s18"> </span>s<span class="s39">∗</span><span class="s15">(</span>v<span class="s15">, </span><span class="s20">∗</span>v<span class="s15">) </span><span class="s40">2</span><span class="s41"> </span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="51" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_055.png"/></span></p><p class="s20" style="padding-left: 1pt;text-indent: 0pt;line-height: 16pt;text-align: left;">—       <span class="s37">s</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Theorem is employed to determine whether there is an overlap</p><p class="s10" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">s<span class="s35">∗</span><span class="s15">(</span>v<span class="s15">, </span><span class="s20">∗</span>v<span class="s15">) </span><span class="s20">= </span>s<span class="s27">0</span><span class="s30"> </span><span class="s20">+ </span><span class="p">max</span></p><p class="s18" style="text-indent: 0pt;line-height: 4pt;text-align: left;">(</p><p style="padding-left: 4pt;text-indent: 0pt;line-height: 9pt;text-align: left;">0<span class="s15">, </span><i>vT </i><span class="s20">+</span></p><p class="s42" style="padding-left: 1pt;text-indent: 0pt;line-height: 9pt;text-align: left;"> v<span class="s43">∗</span>v  <span class="s10"> </span><span class="s28"> </span></p><p class="s20" style="padding-left: 6pt;text-indent: 0pt;line-height: 5pt;text-align: left;">√<span class="s44">    </span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">(4)</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">between bounding boxes, serving as an accurate and effective</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">vehicle collision detection method. If there exists an axis such that the projections of the two OBBs onto the axis do not overlap, then there is no collision between the two vehicles. Safety redundancies <i>d</i><span class="s26">r</span><span class="s30">1 </span>and <i>d</i><span class="s26">r</span><span class="s30">2 </span>for collision detection will be activated during the training phase and deactivated during the model validation phase.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s10" style="display: inline;">Hierarchical Coordination Framework</p></li></ol><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark20" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">To reduce the burden on the central coordinator and improve the computational efficiency, a hierarchical coordi- nation framework is adopted to facilitate the coordination of vehicles in a continuous traffic flow. Specifically, the vehicles are divided into distinct batches. The vehicles on each lane that are closest to the conflict zone comprise the first batch, while the remaining vehicles are divided into subsequent batches based on their lane and distance factors, as shown in Fig. </a>1<span style=" color: #000;">. The first batch of vehicles tend to navigate through the conflict zone cooperatively, with the assistance of the centralized coordinator. The rest of candidate vehicles departing from the</span></p><p style="padding-top: 1pt;padding-left: 161pt;text-indent: 0pt;text-align: left;">2   <i>ab</i></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: right;">where  <i>v</i><span class="s27">0</span><span class="s30">   </span>represents  the  desired  speed  of  the  vehicle,  <span class="s15">β </span>represents the acceleration index, <i>b </i>represents the comfortable deceleration, <span class="s20">∗</span><i>v </i>and <i>s </i>represent the speed difference and the distance between the vehicle and its front vehicle, respectively. When the vehicle is about to enter the conflict zone, if it is not part of the first batch, a virtual vehicle is assumed to exist at the boundary between the conflict zone and the preparatory</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">zone, which ensures that the vehicle can stop smoothly in front of the conflict zone in accordance with IDM method. If the vehicle has been updated to the first batch, it may enter the conflict zone at its current speed or restart its maneuver. Given the inherent randomness of the traffic scene, the speed at which vehicles enter the conflict zone remains random. In the conflict zone, vertical control of vehicles is handled by the central coordinator. After completing the collaborative task at intersections, they leave the conflict zone at the natural traffic flow speed <i>v</i><span class="s27">flow</span>.</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: right;">In addition to vertical control commands, lateral control</p><p class="s7" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark39" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">commands are derived from </a>(2)<span style=" color: #000;">. By integrating the vertical</span></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">and lateral control algorithms, the overall control vector [<i>a</i><span class="s15">, δ</span>]<span class="s16">T</span><span class="s17"> </span>is transmitted to the vehicle model at each control interval to update the traffic scene.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 106pt;text-indent: -18pt;text-align: left;"><p style="display: inline;"><a name="bookmark44">M</a><span class="s5">ETHODOLOGY</span></p><ol id="l6"><li style="padding-top: 3pt;padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s10" style="display: inline;">Deep Reinforcement Learning Framework</p><p class="s10" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark120" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark45">Reinforcement learning (RL) frameworks are typically for- mulated as Markov decision processes (MDPs) </a><span class="s7">[45]</span><span class="p">. An MDP is defined as a quintuple </span><span class="s15">(</span>S<span class="s15">, </span>A<span class="s15">, </span>R<span class="s15">, </span>P<span class="s15">,γ )</span><span class="p">, which mathematically describes the interaction between the environment and the decision-making of the RL agent. Specifically, </span>S <span class="p">represents the observable state space in the environment, and </span>A <span class="p">represents the decision action space taken by the RL agent. The reward function, </span>R<span class="p">, describes the immediate reward obtained after</span></p><p class="s10" style="padding-left: 5pt;text-indent: 0pt;line-height: 81%;text-align: justify;"><span class="p">taking action </span>a<span class="s26">t</span><span class="s17"> </span><span class="s20">∈ </span>A <span class="p">in the state </span>s<span class="s26">t</span><span class="s17"> </span><span class="s20">∈ </span>S<span class="p">, typically expressed as </span>r<span class="s26">t</span><span class="s15">(</span>s<span class="s26">t</span><span class="s15">, </span>a<span class="s26">t</span><span class="s15">) </span><span class="s20">∈ </span>R<span class="p">. </span>P <span class="p">is the state transition probability, which represents the probability of transitioning to the next state </span>s<span class="s26">t</span><span class="s45">+</span><span class="s30">1 </span><span class="p">after taking action </span>a<span class="s26">t</span><span class="s17">  </span><span class="p">in the current state </span>s<span class="s26">t</span><span class="p">. </span><span class="s15">γ </span><span class="p">acts as the</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">discount factor to balance the importance of immediate and future rewards, within the range of [0, 1).</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In the intersection scenario, at each time step <i>t</i>, the central coordinator observes the current state <i>s</i><span class="s26">t</span><span class="s17"> </span>and selects an action</p><p class="s10" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">a<span class="s26">t</span><span class="s17">  </span><span class="p">by executing the policy </span><span class="s15">π(</span>a<span class="s26">t</span><span class="s20">|</span>s<span class="s26">t</span><span class="s15">)</span><span class="p">. The environment then</span></p><p class="s10" style="padding-left: 5pt;text-indent: 0pt;line-height: 87%;text-align: justify;"><span class="p">transits to the next state </span>s<span class="s26">t</span><span class="s45">+</span><span class="s30">1 </span><span class="p">according to the vehicle model, and the coordinator receives a reward </span>r<span class="s26">t</span><span class="s15">(</span>s<span class="s26">t</span><span class="s15">, </span>a<span class="s26">t</span><span class="s15">)</span><span class="p">. Finally, the accumulated experience is utilized to update the policy. The</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">expected payoff of the policy <span class="s15">π </span>could be calculated as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 12pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="317" height="102" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_056.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark46">Fig. 4.   Encoding of </a><i>d </i>for different driving intentions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">zone. The observation state space can be defined as a series of states of the controlled vehicles, i.e.,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 7pt;text-indent: 0pt;text-align: right;">s <span class="s20">= </span><span class="p">[</span>s<span class="s27">1</span><span class="s15">, </span>s<span class="s27">2</span><span class="s15">,..., </span>s<span class="s26">n</span><span class="p">]                             (8)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: right;">where <i>n </i>denotes the number of vehicles in the control batch. Initially, the fundamental status information for each vehi- cle encompasses its position <span class="s15">(</span><i>x</i><span class="s15">, </span><i>y</i><span class="s15">)</span>, velocity <i>v</i>, and steering angle <span class="s15">θ </span>. The inclusion of state information with varying mean- ings and scales enhances the complexity of the optimization problem,  necessitating  the  agent  to  allocate  more  time  to learn  the  optimal  policy.  Consequently,  both  velocity  and steering  angle  variables  are  incorporated,  and  the  <i>x  </i>and  <i>y </i>axis projections of the velocity, <i>v</i><span class="s26">x</span><span class="s17">  </span>and <i>v</i><span class="s26">y</span>, respectively, are employed to streamline the representation of state information. Then, to effectively capture the driving intention of the vehicle, a  distance  intention  code  is  employed,  denoted  as  <i>d</i>,  to represent  the  route  intention  of  the  vehicle,  as  illustrated</p><p class="s10" style="padding-left: 84pt;text-indent: 0pt;line-height: 7pt;text-align: left;">V<span class="s46">π</span><span class="s47"> </span><span class="s15">(</span>s<span class="s26">t</span><span class="s15">) </span><span class="s20">= </span><span class="s48">E</span><span class="s49">τ</span></p><p class="s10" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;line-height: 6pt;text-align: left;"><span class="s49">π</span><span class="s47"> </span><span class="p">[</span>G <span class="s20">|</span>s <span class="p">]                            (5)</span></p><p class="s7" style="padding-left: 9pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark46" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">in Fig. </a>4<span style=" color: #000;">. Finally, the state of each vehicle is expressed as</span></p><p class="s45" style="text-indent: 0pt;line-height: 9pt;text-align: right;">∼      <span class="s17">t   t</span></p><p class="s15" style="padding-top: 4pt;padding-left: 98pt;text-indent: 0pt;line-height: 5pt;text-align: left;">(<span class="s10">x</span>, <span class="s10">y</span>, <span class="s10">v </span>, <span class="s10">v </span>, <span class="s10">d</span>)<span class="p">.</span></p><p class="s17" style="padding-left: 82pt;text-indent: 0pt;line-height: 6pt;text-align: center;">x     y</p><p class="s17" style="text-indent: 0pt;line-height: 9pt;text-align: left;">i<span class="s45">=</span>t</p><p style="text-indent: 0pt;text-align: left;"/><p class="s17" style="padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="p">where </span><span class="s10">G</span><span class="s26">t</span>   <span class="s20">= </span><span class="s28">}.</span><span class="s35">∞</span><span class="s45"> </span><span class="s15">γ </span><span class="s16">i</span><span class="s45">−</span>t<span class="s10">r</span><span class="s26">i</span>  <span class="p">is  the  discounted  return,  </span><span class="s15">τ </span><span class="s20">∼ </span><span class="s15">π</span></p><p class="s15" style="padding-left: 5pt;text-indent: 0pt;line-height: 80%;text-align: left;"><span class="p">denotes the trajectory </span>(<span class="s10">s</span><span class="s26">t</span>, <span class="s10">a</span><span class="s26">t</span>, <span class="s10">s</span><span class="s26">t</span><span class="s45">+</span><span class="s30">1</span>, <span class="s10">a</span><span class="s26">t</span><span class="s45">+</span><span class="s30">1</span>, . . .) <span class="p">generated by the agent acts according by the policy </span>π <span class="p">.</span></p><p class="s15" style="padding-left: 5pt;text-indent: 9pt;line-height: 12pt;text-align: justify;"><span class="p">The value function formula is commonly used to estimate the </span><span class="s10">V</span><span class="s50">π</span><span class="s47"> </span>(<span class="s10">s</span><span class="s26">t</span>)<span class="p">, the objective of RL is to learn the policy </span>π <span class="s39">∗</span><span class="s45"> </span><span class="p">that maximizes the expected return across all states </span><span class="s10">s</span><span class="s26">t</span></p><p class="s10" style="padding-top: 1pt;padding-left: 5pt;text-indent: 9pt;line-height: 12pt;text-align: justify;">2) Action Space: <a href="#bookmark21" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The action space is defined as the set of actions available to the vehicles. To reduce the complexity of training, the vehicle model defined in Section </a><span class="s7">III </span><span class="p">is employed, where the lateral control of the vehicles is handled by the pure pursuit algorithm, and the agent is only responsible for the vertical control, i.e., the acceleration of the vehicles. The acceleration settings for each vehicle are defined as a discrete</span></p><p class="s10" style="padding-left: 45pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Q<span class="s46">π</span><span class="s47"> </span><span class="s15">(</span>s<span class="s26">t</span><span class="s15">, </span>a<span class="s26">t</span><span class="s15">) </span><span class="s20">= </span>r<span class="s26">t</span><span class="s17"> </span><span class="s20">+ </span><span class="s15">γ </span><span class="s48">E</span><span class="s49">τ</span></p><p class="s45" style="text-indent: 0pt;line-height: 7pt;text-align: left;">∼</p><p style="text-indent: 0pt;text-align: left;"/><p class="s47" style="padding-left: 4pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="s49">π</span> <span class="s21">r</span><span class="s10">Q</span><span class="s46">π</span> <span class="s15">(</span><span class="s10">s</span></p><p class="s17" style="text-indent: 0pt;line-height: 10pt;text-align: left;">t<span class="s45">+</span><span class="s30">1</span></p><p class="s29" style="text-indent: 0pt;line-height: 10pt;text-align: left;">,<span class="s15"> </span><span class="s10">a</span><span class="s17">t</span><span class="s45">+</span><span class="s30">1</span></p><p class="s15" style="text-indent: 0pt;line-height: 8pt;text-align: left;">)<span class="s21">l           </span><span class="s18"> </span><span class="p">(6)</span></p><p style="padding-left: 9pt;text-indent: 0pt;line-height: 10pt;text-align: left;">set, <i>a</i><span class="s27">available</span><span class="s30"> </span><span class="s20">= {−</span>2<span class="s15">, </span><span class="s20">−</span>1<span class="s15">, </span>0<span class="s15">, </span>1<span class="s15">, </span>2<span class="s20">}</span>, in <i>m</i><span class="s15">/</span><i>s</i><span class="s51">2</span>. The action space is</p><p class="s47" style="text-indent: 0pt;line-height: 7pt;text-align: left;">π</p><p style="text-indent: 0pt;text-align: left;"/><p class="s47" style="padding-left: 73pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s15">π </span><span class="s35">∗ </span><span class="s20">= </span><span class="p">arg max </span><span class="s48">E</span><span class="s49">τ</span> <span class="s45">∼</span>π <span class="s21">r</span><span class="s10">Q</span></p><p class="s15" style="text-indent: 0pt;line-height: 10pt;text-align: left;">(<span class="s10">s</span><span class="s26">t</span>, <span class="s10">a</span><span class="s26">t</span>)<span class="s21">l</span>.          <span class="p">(7)</span></p><p style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 8pt;text-align: left;">expressed as</p><p class="s47" style="padding-left: 19pt;text-indent: 0pt;line-height: 7pt;text-align: center;">π</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">To enhance the agents’ understanding,  decision-making, and generalization abilities,  the DRL  scheme is  developed by integrating deep neural networks with RL. This approach leverages neural networks to effectively approximate the pol- icy and value functions, enabling the agent to accurately comprehend the environment’s state. Consequently, the agent can learn and make optimal decisions with better efficiency, thereby achieving superior performance on complicated tasks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s10" style="display: inline;">Problem Formulation</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;line-height: 12pt;text-align: justify;">The design of the MDP tuple elements must be tailored to the specific requirements of the task, enabling the DRL agent to learn the optimal policy <span class="s15">π </span><span class="s39">∗</span>. In the intersection collaboration task, our MDP elements are designed as follows.</p><ol id="l7"><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s10" style="display: inline;">Observation State Space: <span class="p">According to the hierarchical coordination framework, the central coordinator primarily concentrates on dispatching vehicles closest to the conflict</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">a <span class="s20">= </span><span class="p">[</span>a<span class="s27">1</span><span class="s15">, </span>a<span class="s27">2</span><span class="s15">,..., </span>a<span class="s26">n</span><span class="p">]                            (9)</span></p><p style="padding-top: 10pt;padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;">where  <i>n  </i>denotes  the  number  of  controlled  vehicles,  <i>a</i><span class="s26">k</span><span class="s17">   </span><span class="s20">∈</span></p><p class="s10" style="padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: left;">a<span class="s27">available</span><span class="p">, </span>k <span class="s20">≤ </span>n<span class="p">, </span>k <span class="s20">∈ </span><span class="s48">N</span><span class="s39">+</span><span class="p">.</span></p><p class="s10" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">3) Reward Functions: <span class="p">Reasonable reward function design is crucial for the effective training of the agent. The reward function can be divided into positive and negative com- ponents. Positive rewards provide encouraging signals that incentivize vehicles to collaborate spatiotemporally in the conflict zone, enabling efficient task completion. Conversely, negative rewards serve as deterrents against undesirable vehi- cle behaviors such as collisions, by introducing penalties.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Positive rewards in training can be categorized into individ- ual and group-based rewards. The individual positive reward <i>r</i><span class="s27">ind</span><span class="s30"> </span>is given to  a  vehicle  when  it  successfully  completes the crossing task by leaving the conflict center area. In contrast, group positive reward <i>r</i><span class="s27">group </span><span class="s30"> </span>represents an overall</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">synergistic reward provided when all vehicles in the current batch successfully exit the conflict center area</p><p class="s17" style="padding-top: 4pt;padding-left: 107pt;text-indent: 0pt;line-height: 5pt;text-align: center;">n</p><p class="s33" style="padding-left: 66pt;text-indent: 0pt;line-height: 16pt;text-align: left;">r<span class="s30">positive </span><span class="s31">=</span><span class="s20"> </span><span class="s52">)</span><span class="s18"> </span>c<span class="s17">i</span>r<span class="s30">ind </span><span class="s31">+</span><span class="s20"> </span><span class="s10">c</span><span class="s30">all</span>r<span class="s30">group                    </span><span class="s53">(10)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s54" style="padding-left: 5pt;text-indent: 0pt;text-align: center;">(                    )</p><p class="s55" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Vehicle 1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s54" style="padding-left: 5pt;text-indent: 0pt;text-align: center;">(                    )</p><p class="s55" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;line-height: 4pt;text-align: center;">Vehicle 2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s55" style="padding-left: 4pt;text-indent: 3pt;line-height: 115%;text-align: left;">Flatten Normalization</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="322" height="120" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_057.png"/></span></p><p class="s55" style="padding-left: 5pt;text-indent: 3pt;text-align: left;">Update</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s55" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Tanh</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s55" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Tanh</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Update</p><p class="s17" style="padding-top: 2pt;padding-left: 107pt;text-indent: 0pt;text-align: center;">i<span class="s45">=</span><span class="s30">1</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where <i>r</i><span class="s27">ind</span><span class="s30"> </span>denotes the individual positive reward, <i>r</i><span class="s27">group</span><span class="s30"> </span>represents the group positive reward, <i>c</i><span class="s26">i</span><span class="s17"> </span>indicates whether the vehicle <i>i </i>has completed its crossing task, <i>c</i><span class="s27">all</span><span class="s30"> </span>indicates whether</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Update</p><p class="s55" style="padding-left: 2pt;text-indent: 0pt;line-height: 4pt;text-align: left;">Tanh</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s55" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Calculate</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s55" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Update</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">all controlled vehicles have completed overall crossing task, they are assigned as 1 if the task succeeds, and 0 otherwise.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The negative reward scheme is structured into time-based and collision-based penalties. The time-based penalty  <i>r</i><span class="s27">time</span><span class="s30"> </span>aims to impose a cost on the duration taken by a batch of vehicles to accomplish the cooperative intersection crossing. Simultaneously, a significant negative reward <i>r</i><span class="s27">collision</span><span class="s30"> </span>is issued and the mission is immediately aborted upon any collision, marking a failure in the collaborative task</p><p class="s30" style="padding-top: 6pt;padding-left: 5pt;text-indent: 62pt;text-align: left;"><span class="s33">r</span>negative <span class="s31">=</span><span class="s20"> </span><span class="s10">r</span>time<span class="s33">t</span><span class="s10"> </span><span class="s20">+ </span><span class="s10">s</span>col<span class="s33">r</span>collision                           <span class="s53">(11)</span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where <i>r</i><span class="s27">time</span><span class="s30"> </span>denotes the time-based penalty, <i>r</i><span class="s27">collision</span><span class="s30"> </span>denotes the collision-based penalty, <i>t </i>represents the time spent, <i>s</i><span class="s27">col</span><span class="s30"> </span>indicates the signal whether a collision has occurred, which is assigned as 1 if it has occurred, and 0 otherwise.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The design of parameters in the reward function is directly associated with the convergence effects and the agent’s behavior patterns. In the context of vehicle cooperation  at intersections, the primary objective is to  facilitate  effec- tive vehicle collaboration to exit the intersection safely. Consequently, <i>r</i><span class="s27">group</span><span class="s30"> </span>is prioritized above all other parameters, and its absolute value should significantly exceed those of parameters <i>r</i><span class="s27">ind</span><span class="s30"> </span>and <i>r</i><span class="s27">time</span>. For safety considerations, the penalty <i>r</i><span class="s27">collision</span><span class="s30"> </span>is assigned with a substantial negative value to effectively deter and  penalize risky vehicle behaviors.  The parameters <i>r</i><span class="s27">ind</span><span class="s30"> </span>and <i>r</i><span class="s27">time</span><span class="s30"> </span>are employed to reduce vehicle latency, enhance traffic efficiency, and provide immediate reward feedback. Consequently, they are typically assigned with smaller values. After meticulously debugging the param-</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 85%;text-align: justify;">eters, the values for <i>r</i><span class="s27">group</span>, <i>r</i><span class="s27">collision</span>, <i>r</i><span class="s27">ind</span>, and <i>r</i><span class="s27">time</span><span class="s30"> </span>are set to 100 or 150, <span class="s20">−</span>100, 20, and <span class="s20">−</span>0.15, respectively, to enhance the performance of the method. Further parameter details are</p><p class="s7" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark34" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">described in Table </a>II<span style=" color: #000;">.</span></p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark47" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Equation </a>(12) <span style=" color: #000;">depicts the ultimate reward function, wherein two reward mechanisms synergistically facilitate the agent’s learning process by balancing exploration and exploitation</span></p><p class="s31" style="padding-top: 6pt;padding-left: 82pt;text-indent: 0pt;text-align: left;"><a name="bookmark47"><span class="s33">r</span></a><span class="s17">t </span>=<span class="s20"> </span><span class="s10">r</span><span class="s30">positive </span>+<span class="s20"> </span><span class="s10">r</span><span class="s30">negative</span><span class="s29">.</span><span class="s15">                  </span><span class="p">(12)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s10" style="display: inline;">Shared Advantage Actor–Critic Architecture</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">The shared A2C network architecture is employed for train- ing the intersection collaboration model. The model comprises three types of neural networks: 1) a shared network; 2) an actor network; and 3) a critic network. The shared network extracts significant latent feature information from the traffic scene, while the Actor network generates specific policy actions based on the shared feature information. The Critic network</p><p class="s5" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark48">Fig. 5.   Shared actor–critic architecture.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">policy learning with value function estimation to optimize the parameter learning of the multi networks.</p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark48" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The utilized network architecture is illustrated in Fig. </a>5<span style=" color: #000;">, where the shared network component comprises the  long- short term memory (LSTM) layer and the fully connected layers. In the context of the intersection scenario, gathered experience data is stored as serialized information. Given the extensive temporal dependencies resulting from environmental state changes, the agent’s decisions are influenced by temporal memory. LSTM network can leverage their memory units to learn the temporal correlation features inherent in the sequence data. After normalizing the initial data input, the LSTM layer is employed to capture and retain the long-term dependencies present within the observed sequence data. Next, a multilayer fully connected network is employed to process input data in a layer-by-layer manner. This hierarchical feature extraction method allows the network to capture deep patterns and relationships within the data, particularly in high-dimensional data. The Tanh activation function is utilized to transfer feature information between network layers. It introduces nonlinear characteristics that enable the neural network to learn and approximate complex nonlinear functions, thereby enhancing the model’s feature extraction and expression capabilities.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: right;">Then, the output of the shared network layers is passed through the Actor and Critic components of the fully con- nected layers. The Actor component generates specific vehicle actions for execution, while the Critic component evaluates the current intersection environment and provides the policy loss calculation that can be utilized to update the Actor component. Additionally, the reward function supplies the Critic with a value function loss calculation for updating. By utilizing the shared network component, the model can reduce the total number of parameters, promote the transmission and sharing of information features, and improve the consistency between Actor and Critic, enhancing the overall learning performance. The  network  is  updated  by  employing  the  generalized advantage estimation (GAE), which estimates the advantage function through a weighted combination of multistep advan- tage estimates. This approach effectively reduces the deviation and variance of the advantage estimates, thereby improving the stability of the estimation process. The calculation formula</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">for GAE is as follows:</p><p class="s15" style="padding-top: 6pt;padding-left: 88pt;text-indent: 0pt;line-height: 15pt;text-align: left;">δ<span class="s26">t</span><span class="s17"> </span><span class="s20">= </span><span class="s10">r</span><span class="s26">t</span><span class="s17"> </span><span class="s20">+ </span>γ <span class="s10">V</span>(<span class="s10">s</span><span class="s26">t</span><span class="s45">+</span><span class="s30">1</span>) <span class="s20">− </span><span class="s10">V</span>(<span class="s10">s</span><span class="s26">t</span>)</p><p class="s17" style="text-indent: 0pt;line-height: 8pt;text-align: center;">T<span class="s45">−</span>t<span class="s45">−</span><span class="s30">1</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">evaluates  the  value  of  the  policy  produced  by  the  Actor network. During the training process, the model integrates</p><p class="s15" style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span class="p">GAE</span>(<span class="s10">t</span>) <span class="s20">=</span></p><p class="s18" style="padding-left: 5pt;text-indent: 0pt;line-height: 4pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">k<span class="s45">=</span><span class="s30">0</span></p><p class="s15" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">(γ λ)<span class="s56">k</span>δ<span class="s26">t</span></p><p class="s57" style="padding-top: 2pt;text-indent: 0pt;text-align: left;">+<span class="s17">k                                   </span><span class="p">(13)</span></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where  <span class="s15">δ</span><span class="s26">t  </span><span class="s17"> </span>represents  the  advantaged  estimate  at  time  step <i>t</i>, <i>V</i><span class="s15">(</span><i>s</i><span class="s26">t</span><span class="s15">) </span>indicates the value function estimate at state <i>s</i><span class="s26">t</span>, <span class="s15">λ </span>represents the hyperparameter that controls the tradeoff bias and variance, and <i>T </i>denotes the last time step.</p><p class="s15" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><span class="p">The total training loss is calculated as the weighted sum of the policy loss and the value function loss, given as follows: loss</span><span class="s27">policy</span><span class="s30"> </span><span class="s20">= </span><span class="s58">)</span><span class="s18"> </span><span class="s52">(</span><span class="s20">−</span><span class="p">log</span><span class="s59">probs</span>(<span class="s10">t</span>) <span class="s20">· </span><span class="p">GAE</span>(<span class="s10">t</span>) <span class="s20">− </span><span class="p">Ent</span><span class="s27">coe</span><span class="s30"> </span><span class="s20">· </span><span class="p">Ent</span>(<span class="s10">t</span>)<span class="s52"> </span></p><p class="s17" style="padding-left: 12pt;text-indent: 0pt;line-height: 6pt;text-align: center;">t</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 20pt;text-align: center;"><span class="p">loss</span><span class="s27">value</span><span class="s30"> </span><span class="s20">= </span><span class="s58">)</span><span class="s18"> </span><span class="s15">(</span>r<span class="s26">t</span><span class="s17"> </span><span class="s20">− </span>V<span class="s15">(</span>s<span class="s26">t</span><span class="s15">))</span><span class="s40">2</span></p><p class="s17" style="padding-left: 12pt;text-indent: 0pt;line-height: 8pt;text-align: center;">t</p><p class="s27" style="padding-left: 5pt;text-indent: 15pt;line-height: 15pt;text-align: left;"><span class="p">loss</span>total<span class="s30"> </span><span class="s20">= </span><span class="s10">w</span>1<span class="s30"> </span><span class="s20">· </span><span class="p">loss</span>policy<span class="s30"> </span><span class="s20">+ </span><span class="s10">w</span>2<span class="s30"> </span><span class="s20">· </span><span class="p">loss</span>value<span class="s30">                             </span><span class="p">(14)</span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where log<span class="s59">probs</span><span class="s15">(</span><i>t</i><span class="s15">) </span>represents the logarithmic probability at time step <i>t</i>, Ent<span class="s27">coe</span><span class="s30">  </span>denotes  the  coefficient  used  to  control the regularization of the policy entropy, Ent<span class="s15">(</span><i>t</i><span class="s15">) </span>represents the policy entropy, <i>w</i><span class="s27">1</span><span class="s30"> </span>and <i>w</i><span class="s27">2</span><span class="s30"> </span>indicate weights.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In this  work, the  hyperparameter <span class="s15">λ </span>is set  to  1, indicat- ing that the GAE utilizes multistep return estimations on a more accurate evaluation of long-term returns. This setting helps effectively capture the long-term dependencies of tasks, particularly at the signal-free intersections where a long-term policy is essential. Simultaneously, in the presence of reward delays, it facilitates a more stable update of the policy. The coefficient Ent<span class="s27">coe</span><span class="s30"> </span>is set to 0.01, which is generally regarded as a reasonable choice in deep RL. This setting aids the agent in achieving a balance between exploration and exploitation, thereby enhancing the robustness of the policy.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The gradient of the total loss is computed via backpropaga- tion, and then clipped to prevent gradient explosion. The Adam optimizer is  subsequently employed to update  the model’s parameters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s10" style="display: inline;">Asynchronous Training Strategy</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">An asynchronous training approach leveraging the multi- process concurrency capabilities of the operating system has been implemented to enhance the training efficiency of A2C. By creating parallel environments, multiple processes can interact with their environment independently, initiate training simultaneously, and share model parameters through a param- eter synchronization mechanism. This approach effectively enhances data efficiency and improves the effectiveness of exploration.</p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark50" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">In the asynchronous training strategy, multiple processes serve as the training processes,  with  one  designated  as the testing process, and  a  parameter  sharing  pool  utilized for facilitating the sharing of model parameters across all processes. Fig. </a>6 <span style=" color: #000;">summarizes the asynchronous shared A2C structure of the signal-free intersection  cooperation  model. For each training process, they generate simulated intersection environments using distinct random seeds to enhance the exploration toward intricate environments. Upon resetting the environments, they load the A2C model parameters from the sharing pool and engage with the environments to collect experience. The parameter sharing pool employs a mutual exclusion mechanism to manage update to the model param- eters during the entire training process. When a training process  requires  an  update  operation,  it  enters  a  blocked</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="333" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_058.png"/></span></p><h2 style="padding-left: 11pt;text-indent: 0pt;text-align: justify;">Algorithm  1:  <span class="p">Training  Strategy  of  the  Asynchronous</span></h2><p class="s44" style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a name="bookmark49">  Shared A2C-Based Coordinator                                               </a></p><p style="padding-left: 33pt;text-indent: -11pt;text-align: left;"><span class="s5">1:  </span>Initialize several training processes, training experience pools, a testing process, a testing experience pool, shared Actor-Critic parameters <span class="s15">θ </span>, a parameter sharing pool,</p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;line-height: 83%;text-align: left;">parameters <span class="s15">γ </span>, <span class="s15">λ</span>, maximum epoch duration <i>t</i><span class="s26">max</span>, Completion signal <i>s</i><span class="s20">=</span>false.</p><h2 style="padding-left: 22pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s5">2:  </span>while <span class="p">not </span><i>s </i>do</h2><h2 style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><span class="s5">3:       </span>for <span class="p">each training process </span>do</h2><p style="padding-top: 1pt;padding-left: 53pt;text-indent: -31pt;line-height: 89%;text-align: left;"><span class="s5">4:           </span>Initialize the intersection environment, termination sign <i>done </i><span class="s20">= </span>false, time <i>t </i><span class="s20">= </span>0.</p><p style="padding-left: 53pt;text-indent: -31pt;line-height: 10pt;text-align: left;"><span class="s5">5:           </span>Load model parameters <span class="s15">θ </span>from the parameter</p><p style="text-indent: 0pt;text-align: center;">sharing pool.</p><h2 style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><span class="s5">6:            </span>while <span class="p">not </span><i>done </i>do</h2><p style="padding-left: 22pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s5">7:                </span>Observed environment state <i>s</i><span class="s26">t</span>.</p><p class="s10" style="padding-left: 22pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s5">8:                 </span><b>if </b>t <span class="s15">&gt; </span>t<span class="s26">max </span><span class="s17"> </span><b>then</b></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 11pt;text-align: left;">9:                      <span class="p">Break.</span></p><p class="s5" style="padding-left: 18pt;text-indent: 0pt;line-height: 11pt;text-align: left;">10:                 <span class="h2">end if</span></p><p class="s10" style="padding-left: 18pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s5">11:                </span><span class="p">Calculate the agent’s decision </span>a<span class="s26">t</span><span class="s17"> </span><span class="s20">= </span><span class="s15">π(</span>s<span class="s26">t</span><span class="s15">)</span><span class="p">.</span></p><p style="padding-left: 18pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s5">12:                </span>Update the environment with <i>a</i><span class="s26">t</span>, store</p><p class="s15" style="padding-left: 63pt;text-indent: 0pt;line-height: 85%;text-align: left;">(<span class="s10">s</span><span class="s26">t</span>, <span class="s10">a</span><span class="s26">t</span>, <span class="s10">r</span><span class="s26">t</span>, <span class="s10">s</span><span class="s26">t</span><span class="s45">+</span><span class="s30">1</span>, <span class="s10">done</span>) <span class="p">in the training experience pool.</span></p><p class="s10" style="padding-left: 18pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s5">13:                 </span>t <span class="s20">= </span>t <span class="s20">+ </span><span class="p">1.</span></p><p class="s5" style="padding-left: 18pt;text-indent: 0pt;line-height: 11pt;text-align: left;">14:            <span class="h2">end while</span></p><h2 style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span class="s5">15:            </span>if <span class="p">not obtain the mutex </span>then</h2><p class="s5" style="padding-left: 18pt;text-indent: 0pt;text-align: left;">16:                 <span class="p">Request and wait for the mutex.</span></p><p class="s5" style="padding-left: 18pt;text-indent: 0pt;line-height: 11pt;text-align: left;">17:            <span class="h2">else</span></p><p style="padding-left: 63pt;text-indent: -45pt;text-align: left;"><span class="s5">18:                </span>Update the model parameters <span class="s15">θ </span>in the parameter sharing pool.</p><p class="s5" style="padding-left: 18pt;text-indent: 0pt;text-align: left;">19:                 <span class="p">Release the mutex.</span></p><p class="s5" style="padding-left: 18pt;text-indent: 0pt;text-align: left;">20:            <span class="h2">end if</span></p><p class="s5" style="padding-left: 18pt;text-indent: 0pt;text-align: left;">21:       <span class="h2">end for</span></p><h2 style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span class="s5">22:       </span>for <span class="p">the testing process </span>do</h2><p class="s5" style="padding-left: 18pt;text-indent: 0pt;text-align: left;">23:           <span class="p">Evaluate the current model at regular intervals.</span></p><h2 style="padding-left: 18pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s5">24:            </span>if <span class="p">the model has converged </span>then</h2><p class="s5" style="padding-left: 18pt;text-indent: 0pt;line-height: 14pt;text-align: left;">25:                 <span class="s10">s</span><span class="s20">=</span><span class="p">true.</span></p><p class="s5" style="padding-left: 18pt;text-indent: 0pt;line-height: 11pt;text-align: left;">26:            <span class="h2">end if</span></p><p class="s5" style="padding-left: 18pt;text-indent: 0pt;text-align: left;">27:       <span class="h2">end for</span></p><p class="s5" style="padding-left: 18pt;text-indent: 0pt;text-align: left;">28:  <span class="h2">end while</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="333" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_059.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">state to ensure exclusive access to the sharing pool. This blocking prevents other training  processes  from  modifying the parameters concurrently. Once  the  update  is  complete, the mutex is released, allowing other training processes to continue updating the model parameters. To assess the validity and robustness of the model in real-time during the training, the testing process is initiated to evaluate the A2C model in real-time.</p><p class="s7" style="padding-left: 6pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark49" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Algorithm </a>1 <span style=" color: #000;">illustrates the whole process of the proposed method. Intersection scenarios are constructed concurrently by multiple training processes. These processes load the latest A2C model from the parameter sharing pool and interact with their environments to collect experience. Each training process continuously requests and releases the mutex. Upon acquiring the mutex, it extracts the experience to calculate the total loss of the A2C network model for updating the model parameters</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="text-indent: 0pt;text-align: right;">Process 1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="450" height="265" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_060.png"/></span></p><p class="s12" style="padding-top: 4pt;padding-left: 15pt;text-indent: 2pt;line-height: 118%;text-align: left;">Create Initialize</p><p class="s12" style="padding-top: 4pt;text-indent: 0pt;text-align: right;">Interact</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s60" style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">[(   ,    ,    ,        ,   )]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="text-indent: 0pt;text-align: right;">Process 2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s60" style="padding-left: 132pt;text-indent: 0pt;text-align: center;">[(   ,    ,    ,        ,   )]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-top: 4pt;text-indent: 0pt;text-align: right;">Process n</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s60" style="padding-left: 132pt;text-indent: 0pt;text-align: center;">[(   ,    ,    ,        ,   )]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="text-indent: 0pt;text-align: right;">Process 1</p><p class="s12" style="padding-top: 4pt;padding-left: 15pt;text-indent: 2pt;line-height: 135%;text-align: left;">Create Initialize</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="text-indent: 0pt;text-align: center;">Evaluation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="text-indent: 0pt;line-height: 6pt;text-align: right;">Request</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s61" style="padding-left: 22pt;text-indent: -2pt;line-height: 107%;text-align: left;">Policy Loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 10pt;text-indent: 0pt;line-height: 2pt;text-align: left;">Training</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 88pt;text-indent: 0pt;text-align: left;">Multi-process execution</p><p class="s12" style="padding-top: 4pt;padding-left: 39pt;text-indent: -1pt;line-height: 107%;text-align: left;">Release Mutex</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 54pt;text-indent: 0pt;text-align: left;">Update</p><p class="s12" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">Mutex</p><p class="s61" style="padding-top: 5pt;padding-left: 24pt;text-indent: -1pt;line-height: 107%;text-align: left;">Value Loss</p><p class="s12" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">Load</p><p class="s61" style="padding-top: 4pt;padding-left: 17pt;text-indent: 7pt;line-height: 107%;text-align: left;">Model Parameter</p><p class="s12" style="padding-left: 35pt;text-indent: 0pt;line-height: 6pt;text-align: left;">Completed</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-top: 4pt;padding-left: 24pt;text-indent: 0pt;text-align: center;">Asynchronous Update</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark50">Fig. 6.   Asynchronous shared A2C structure of the signal-free intersection cooperation model.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="321" height="163" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_061.jpg"/></span></p><p class="s7" style="padding-left: 268pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark32" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">benchmark algorithms based on optimization and graph theory principles. The scene parameters and controller parameters are shown in  Table </a>I<a href="#bookmark34" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,  and the  parameters of  the  deep  RL algorithms are shown in Tables </a>II <a href="#bookmark37" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">and </a>III<span style=" color: #000;">.</span></p><p style="padding-left: 268pt;text-indent: 9pt;text-align: justify;">To reasonably evaluate the scheduling performance between strategies, two metrics are considered: 1) retreat time and 2) averaged delay time, which can represent the traffic efficiency and smoothness of traffic scenarios. Assuming there are <i>N</i></p><p class="s17" style="text-indent: 0pt;line-height: 8pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 268pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">vehicles in the scenario, <i>t</i><span class="s56">in</span><span class="s17"> </span>represents the time point at which</p><p class="s17" style="text-indent: 0pt;line-height: 8pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 268pt;text-indent: 0pt;line-height: 12pt;text-align: left;">vehicle  <i>i  </i>enters  the  conflict  zone,  <i>t</i><span class="s51">out</span></p><p style="padding-left: 3pt;text-indent: 0pt;text-align: left;">represents  the  time</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark51">Fig. 7. (a) and (b) represent the T-shaped intersection and the Cross intersection in SUMO, yellow vehicles represent the first batch of vehicles, and green vehicles represent other batches of vehicles.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">before releasing the mutex. The testing process is employed to monitor the training convergence of the model. The testing</p><p class="s62" style="text-indent: 0pt;line-height: 12pt;text-align: left;">t<span class="s30">flow</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">point at which vehicle <i>i </i>leaves the conflict central zone, and <span class="s63">i      </span><span class="s17"> </span>represents the time from entering the preparatory zone to leaving the conflict central zone assuming that it has been using the natural traffic flow speed.</p><ol id="l8"><li style="padding-left: 30pt;text-indent: -14pt;text-align: left;"><p class="s10" style="display: inline;">Retreat Time: <span class="p">It is  defined  as the latest  time for all vehicles to leave the conflict central zone</span></p><p class="s10" style="padding-top: 2pt;padding-left: 78pt;text-indent: 0pt;line-height: 4pt;text-align: left;">T<span class="s27">ret</span><span class="s30"> </span><span class="s20">= </span><span class="p">max </span>t<span class="s40">out                            </span><span class="s30"> </span><span class="s45">+</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">process is employed to monitor the model’s convergence and</p><p class="s10" style="padding-left: 5pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span class="s64">i  </span><span class="s17"> </span><span class="s15">, </span>i <span class="s20">≤ </span>N<span class="s15">, </span>i <span class="s20">∈ </span><span class="s48">N</span></p><p class="s15" style="padding-left: 4pt;text-indent: 0pt;line-height: 3pt;text-align: left;">.          <span class="p">(15)</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">to issue the termination signal.</p></li><li style="padding-left: 20pt;text-indent: -14pt;line-height: 10pt;text-align: left;"><p class="s10" style="display: inline;">Averaged Delay Time: <span class="p">It is defined as the average of the</span></p><p style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><a name="bookmark52">difference between the actual travel time and the ideal travel time for all vehicles</a></p></li></ol></li></ol></li><li style="padding-left: 91pt;text-indent: -14pt;line-height: 9pt;text-align: left;"><p style="display: inline;">E<span class="s5">MPIRICAL </span>A<span class="s5">NALYSIS                                                                        </span><span class="s65">1  </span> <span class="s17">N</span></p><p class="s18" style="text-indent: 0pt;line-height: 5pt;text-align: right;"><u> </u>  )<span class="s28">(</span> <span class="s30">out</span></p><p class="s10" style="padding-left: 13pt;text-indent: 0pt;line-height: 4pt;text-align: left;"><span class="s40">in      </span><span class="s30"> flow  </span><span class="s15">, </span>i <span class="s20">≤ </span>N<span class="s15">, </span>i <span class="s20">∈ </span><span class="s48">N</span><span class="s35">+</span><span class="s15">.  </span><span class="p">(16)</span></p><ol id="l9"><li style="padding-top: 1pt;padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s10" style="display: inline;">Experimental Settings</p><p style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;line-height: 10pt;text-align: left;">The simulation experiments are conducted on a personal</p><p class="s33" style="padding-left: 5pt;text-indent: 0pt;line-height: 17pt;text-align: left;">T<span class="s67">¯</span><span class="s30">delay </span><span class="s31">=</span><span class="s20"> </span><span class="s68">N</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">i<span class="s45">=</span><span class="s30">1</span></p><p class="s10" style="padding-left: 4pt;text-indent: 0pt;line-height: 12pt;text-align: left;">t<span class="s64">i   </span><span class="s17"> </span><span class="s20">− </span>t<span class="s64">i </span><span class="s17"> </span><span class="s20">− </span>t<span class="s64">i</span></p><p class="s7" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark121" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark53">computer with an Intel  i7  CPU  and  16GB  RAM,  using the Python language. Two typical intersection scenarios are constructed in the SUMO software </a>[46]<a href="#bookmark51" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, which is dedicated to traffic simulation, as illustrated in Fig. </a>7<a href="#bookmark65" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. To facilitate compar- ison with the mainstream methods, the two intersection scenes are restricted to single-lane configurations. The extensibility of our method in the multilane intersection scenario is also discussed in Section </a>V-E<span style=" color: #000;">.</span></p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark99" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark54">Our proposed method is compared with several main- stream methods, including the PPO method </a>[24]<a href="#bookmark122" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, the DFST method </a>[47]<a href="#bookmark95" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, the  MCC  method  </a>[20]<a href="#bookmark96" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,  and  the  CGTS method </a>[21] <span style=" color: #000;">which are recently-proposed. The PPO method is a popular deep RL approach, while the remaining methods are</span></p><p style="padding-left: 30pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">Meanwhile,  the metric  of computational  time, which</p><p style="padding-left: 30pt;text-indent: 0pt;text-align: justify;">quantifies the duration required for average execution of the algorithm, has been employed to assess the implementation performance.</p><ol id="l10"><ol id="l11"><li style="padding-left: 30pt;text-indent: -14pt;text-align: left;"><p class="s10" style="display: inline;">Computational  Time:  <span class="p">It  is  defined  as  the  averaged execution duration of the algorithm.</span></p></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s10" style="display: inline;">Ablation Experiments</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">To effectively evaluate the contributions of each component of the proposed method, a series of ablation experiments have been conducted to compare the training performance.</p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark55" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">To verify the effectiveness of asynchronous training strategy, Fig. </a>8<span style=" color: #000;">(a)  and  (b)  illustrates  the  reward  curves  for  various</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="293" height="164" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_062.png"/></span>	<span><img width="291" height="164" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_063.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s30" style="padding-left: 126pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span><img width="12" height="10" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_064.png"/></span>	<span><img width="12" height="10" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_065.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="292" height="164" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_066.png"/></span>	<span><img width="291" height="164" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_067.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s30" style="padding-left: 128pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span><img width="12" height="10" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_068.png"/></span>	<span><img width="13" height="10" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_069.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark55">Fig. 8. Training curves of different number of processes. (a) T-shaped intersection. (b) Cross intersection.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">training processes. The results indicate that, in terms of convergence speed, the agent  with  2  training  processes  at the T-shaped intersection gradually converges to the optimal policy after approximately 1000 episodes (300 s), while the agent with 8 training processes converges to the optimal policy after about 600 episodes (80 s). At the Cross Intersection, the agent with 2 training processes converges to the optimal policy after approximately 2000 episodes (680 s), whereas the agent with 8 training processes converges to the optimal policy after about 1000 episodes (140 s). In terms of sta- bility, an increased number of training processes results in reduced oscillation of the reward curve, leading to improved smoothness and enhanced stability. In terms of performance, the expected reward value of the optimal policy with a greater number of training processes is slightly higher than that of the policy with fewer training processes. This indicates that increasing the number of training processes contributes to improved performance. The results presented above can be attributed to the asynchronous training strategy, where the agent’s exploration behavior across multiple training processes complements one another, enabling the model to achieve a better balance between exploration and exploitation. As the number of training processes increases, the model becomes more adept at learning the dynamic characteristics of the environment, thereby optimizing its policy learning and allow- ing for the acquisition of superior strategies in a reduced timeframe and with fewer episodes.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The ablation experiments have been conducted on the designed network components. In the original network archi- tecture, the LSTM layer is replaced by a fully connected layer,</p><p class="s5" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark56">Fig. 9. Ablation experiments of the designed network. (a) T-shaped intersection. (b) Cross intersection.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark56" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">which is referred to as Method 1. Additionally, the shared network layer is substituted with isolated Actor and Critic network architectures, which are designated  as  Method  2. Fig. </a>9<span style=" color: #000;">(a) and (b) indicates their reward curves during train- ing (8 training processes). Regarding training speed, the original method employs an LSTM layer, which increases computational overhead, resulting in a longer elapsed time. However, given the same number of episodes, the original method achieves convergence the fastest, while Method 2 exhibits the slowest convergence rate. In terms of performance, both Method 1 and Method 2 exhibit slightly decreased expected reward values than the original method. The observed performance indicates that the addition of the LSTM layer is beneficial for capturing the significant long-term dependencies of decisions in  driving  scenarios.  Additionally,  the  design of the shared network allows both the Actor and Critic to learn a more unified and consistent representation of envi- ronmental features, rather than developing features that are independent of one another. Together, these elements included in the designed network can accelerate the convergence of the training strategy and improve the overall performance of the model.</span></p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark57" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The contrast between the PPO method and our method is illustrated in Fig. </a>10<span style=" color: #000;">(a) and (b). In the PPO method, the Actor and Critic networks with fully connected layers are trained independently. Our method achieves a faster convergence rate, a smoother reward curve, and a higher expected reward. This further demonstrates that the developed network facilitates a better collaboration between policy learning and value function estimation, thereby enhancing the training performance of the method.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 127pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span><img width="12" height="10" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_070.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Fig. 10.   Training curve comparison between our method and PPO method.</p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark57">(a) T-shaped intersection. (b) Cross intersection.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s10" style="display: inline;">Metrics Performance</p><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark95" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">In the simulation, different traffic densities are modeled by adjusting the number of vehicles in the scene as indicated in </a>[20]<span style=" color: #000;">. For the T-shaped intersection, 6, 9, and 12 vehicles are generated in each lane to represent low, medium, and high densities, respectively. In the case of the cross intersection, we generate 5, 8, and 11 vehicles. The vehicle arrival rate on the road is set to 1200 vehicles per hour, and the initial positions and speeds of the vehicles are randomized  to  simulate  a more realistic traffic scenario. The hierarchical coordination framework is adopted to coordinate continuous traffic flow, and the scheduling performance of the proposed method is measured by two metrics.</span></p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark58" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The experimental results are shown in the Figs. </a>11 <a href="#bookmark59" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">and </a>12<span style=" color: #000;">, which demonstrate that our method consistently achieves better operational performance over the baseline approaches. At the T-shaped intersection, our method reduces retreat time by 27.71%, 29.99%, 23.92%, 14.41%, and averaged delay time</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">by 35.06%, 32.21%, 24.75%, 8.95% compared with methods DFST, MCC, CGTS, and PPO at low traffic density. Under high traffic density,  the reduction in retreat time increases to 33.60%, 35.72%, 28.79%, 11.55%, and the reduction in</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 106%;text-align: justify;">averaged delay time increases to 32.15%, 28.22%, 21.65%, 12.23%. At the cross intersection, under the low density, our method  reduces  retreat  time  by  28.30%,  23.96%,  19.01%,</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">11.07%, and averaged time by 43.34%, 21.32%, 20.85%, 16.91%.  Under  the  high  density,  the  reduction  in  retreat</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">time increases to 32.53%, 27.86%, 17.29%, 11.23%, and the</p><p style="text-indent: 0pt;text-align: left;"><span><img width="286" height="159" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_071.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="286" height="175" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_072.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="292" height="172" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_073.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="292" height="172" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_074.png"/></span></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: justify;">Fig. 11.  Comparison of scheduling metrics performance between our method</p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark58">and benchmark algorithms at the T-shaped intersection.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="292" height="172" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_075.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="292" height="172" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_076.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark59">Fig. 12. Comparison of scheduling metrics performance between our method and benchmark algorithms at the cross intersection.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark40" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">conducted across different  traffic  conditions,  with  analysis of variance  (ANOVA)  employed  for  statistical  significance analysis. Statistical results have been reported in Table </a>IV<span style=" color: #000;">.</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;">reduction in average delay time increases to 42.96%, 24.59%,    <i>A</i><span class="s69">T</span><span class="s17">        T            C                C</span></p><p style="padding-left: 275pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s59">ret</span>, <i>A</i><span class="s70">delay</span>, <i>A</i><span class="s59">ret</span><span class="s30">  </span>and <i>A</i><span class="s70">delay</span><span class="s30">  </span>represent the ANOVA statistics of</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">20.59%, 16.24%.</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">the two metrics, while <i>h</i><span class="s16">T</span><span class="s17">  </span>, <i>h</i><span class="s16">T</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">, <i>h</i><span class="s16">C</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">and <i>h</i><span class="s56">C</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">indicate the</p><p class="s30" style="text-indent: 0pt;line-height: 7pt;text-align: right;">ret</p><p class="s30" style="padding-left: 9pt;text-indent: 0pt;line-height: 7pt;text-align: left;">delay</p><p class="s30" style="padding-left: 9pt;text-indent: 0pt;line-height: 7pt;text-align: left;">ret</p><p class="s30" style="padding-left: 25pt;text-indent: 0pt;line-height: 7pt;text-align: left;">delay</p><p style="padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: left;">To  more  effectively  highlight  the  performance  disparity</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">between our method and others, 40 simulations have been</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">test values of ANOVA under the two intersection scenarios.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">The experiments assume the null hypothesis that our method</p><p class="s5" style="padding-top: 3pt;padding-left: 82pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark60">TABLE VI</a></p><p class="s5" style="padding-left: 82pt;text-indent: 0pt;line-height: 9pt;text-align: center;">S<span class="s12">UCCESS AND </span>A<span class="s12">CCIDENT </span>R<span class="s12">ATES </span>U<span class="s12">NDER </span>D<span class="s12">IFFERENT </span>D<span class="s12">ELAYS AND </span>N<span class="s12">OISE</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 90pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="458" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_077.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 90pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="459" height="14" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_078.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 90pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="461" height="14" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_079.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 90pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="455" height="14" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_080.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">is not significantly different from other algorithms. However, with the significance level set at 0.05, all test values equal 1, indicating that the statistical analyses reject the hypothesis in every case. Significant statistics reveal that our method significantly outperforms other methods. Based on the afore- mentioned statistical analyses, it can be concluded that the disparities in results between our method and other algorithms are not due to mere chance, but rather attributed to the distinctive properties of the algorithms.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">This performance is attributed  to  the  incentive  strategy for both collision risks and travel time costs, aiming to enabling each batch of vehicles to exit the intersection quickly without collision, thereby achieving optimal collaboration among the vehicles. Compared to the PPO method, our method designs a more comprehensive representation of envi- ronmental features and employs the asynchronous training strategy that enables multiple agents to operate independently in different environments. This diversified experience col- lection  method  effectively  reduces  the  correlation  between</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark123" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark61">delays. Some studies </a>[48]<a href="#bookmark124" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[49] <span style=" color: #000;">suggest that the communica- tion delay for V2I and V2V in the future is unlikely to exceed 500 ms. Consequently, the communication delays have been set to 0, 100, 200, 300, 400, and 500 ms, respectively.</span></p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark125" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark62">The employed strategy has been trained under the optimal communication condition (without delay or noise) and subse- quently test it under various delay scenarios. The double-ended queue data structure is utilized to store vehicle information during the simulation, which is often used to model information lag during communication </a>[50]<span style=" color: #000;">. Real-time vehicle information is stored at the front of the queue, while historical information is retrieved from the back to update the current state. By controlling the length of the queue and the simula- tion frequency, varying degrees of communication delay can be simulated. Additionally, Gaussian noise is introduced to the vehicle’s position and speed information to more accu- rately simulate the uncertainty present in real-world scenarios. The probability density function of noise and the vehicle information following noise processing are presented in</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">samples,  enhances  the  generalization  ability  and  mitigates</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">overfitting. Consequently, this leads to higher expected returns and improved task performance compared to the PPO method.</p><p class="s10" style="padding-top: 10pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">P<span class="s15">(</span>x<span class="s15">) </span><span class="s20">=</span></p><p style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="37" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_081.png"/></span></p><p class="s20" style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;">√   <span class="s10">e</span><span class="s71">−</span></p><p class="s15" style="text-indent: 0pt;line-height: 9pt;text-align: center;">o  <span class="p">2</span>π</p><p style="text-indent: 0pt;text-align: left;"><span><img width="26" height="0" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_082.png"/></span></p><p class="s72" style="padding-top: 6pt;padding-left: 3pt;text-indent: -3pt;line-height: 82%;text-align: left;">(<span class="s73">x</span><span class="s74">−</span>μ)<span class="s75">2</span><span class="s12"> 2</span>σ <span class="s76">2</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Other methods, including DFST, MCC, and CGTS, although planning for a large number of  vehicles  simultaneously, such methods prioritize absolute safety redundancy in the central conflict zone and do not adequately consider the spatiotemporal nonoverlap of vehicle trajectories. In contrast, our approach, which employs the hierarchical coordination framework, enables vehicles to learn strategies for generat- ing conflict-free space-time trajectories, thereby reducing the retreat time and average delay time at the intersection.</p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark41" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The computational time of various methods is presented in Table </a>V<span style=" color: #000;">. The calculation time of  our  method  and  the PPO method is at the submillisecond level, in contrast to the millisecond level observed for other methods. This discrepancy can be attributed to the advanced reasoning and computational capabilities of deep neural networks, which allows for direct end-to-end processing of input data without the need for additional processing steps. Consequently, our method is well- suited to meet  the  real-time  requirements  of  applications. In general, our method achieves better collaborative vehicle performance in intersection scenarios than other algorithms with fewer computational resources, proving the effectiveness and superiority.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s10" style="display: inline;">Communication Delay and Noise Robustness</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">In real-world scenarios, communication conditions are often less than the ideal situation. Therefore, this section discusses the robustness of our method in relation to communication</p><p class="s17" style="text-indent: 0pt;line-height: 8pt;text-align: left;">p</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 73pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Pos<span class="s26">n</span><span class="s17"> </span><span class="s20">= </span>Pos<span class="s27">true</span><span class="s30"> </span><span class="s20">+ </span><i>N</i><span class="s52">(</span><span class="s15">μ</span><span class="s26">p</span><span class="s15">,σ </span><span class="s40">2</span><span class="s52"> </span></p><p class="s17" style="text-indent: 0pt;line-height: 8pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s10" style="padding-left: 5pt;text-indent: 75pt;line-height: 23pt;text-align: left;">V<span class="s26">n</span><span class="s17"> </span><span class="s20">= </span>V<span class="s27">true</span><span class="s30"> </span><span class="s20">+ </span>N<span class="s52">(</span><span class="s15">μ</span><span class="s26">v</span><span class="s15">,σ </span><span class="s40">2</span><span class="s30">                                  </span><span class="p">(17)</span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where Pos<span class="s26">n</span><span class="s17"> </span>and <i>V</i><span class="s26">n</span><span class="s17"> </span>represent the information after noise processing, <span class="s15">μ</span><span class="s26">p</span><span class="s17"> </span>and <span class="s15">μ</span><span class="s26">v</span><span class="s17"> </span>denote the mean value in the noise with the value of 0, <span class="s15">σ</span><span class="s26">p</span><span class="s17"> </span>and <span class="s15">σ</span><span class="s26">v</span><span class="s17"> </span>indicate the standard deviations, with values of 1 and 0.5, respectively.</p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark60" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Under varying communication conditions, 100 experiments have been conducted to evaluate  the  cooperation  success rate and accident rate of vehicles at intersections. Results of these experiments are presented in Table </a>VI<span style=" color: #000;">. Communication delays and noise introduce uncertainty and unreliability in decision-making information, which can significantly impact the accident rate in vehicle cooperation. This effect becomes more pronounced under complex scenarios and greater delays. At the T-shaped intersection, our approach attained a 100% success rate and a 0% accident rate with a delay of less than 300 ms, as well as a success rate of 96% and an accident rate of 4% at a delay of 500 ms. At the cross intersection with more complexity, our method achieved a success rate of 93% and an accident rate of 7% with a delay of 500 ms. Communication delays and noise can hinder vehicles’ ability to respond in a timely and accurate manner during collaborative efforts, thereby increasing the risk of collisions. Nevertheless, our method demonstrates significant robustness in the presence of high latency and noise, effectively reducing the accident rate to 7% or lower. This effectiveness primarily stems from the safety redundancy mechanism designed for the vehicle, along with</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="121" height="123" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_083.png"/></span>	<span><img width="176" height="100" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_084.png"/></span></p><ol id="l12"><li style="padding-top: 3pt;padding-left: 181pt;text-indent: -128pt;text-align: left;"><p class="s77" style="display: inline;">(b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark63">Fig. 13. (a) and (b) represent the multilane scenario constructed in SUMO and the training performance of our method, respectively.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">the incorporation of stringent penalties for collisions within the reward function. In the presence of communication delays and noise, OBBs can alleviate the effects of inaccurate positioning information and elevate the safety threshold for vehicle dis- tance. Furthermore, the imposition of severe collision penalties encourages learning the policy that avoid aggressive, low- safety maneuvers.</p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark126" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark64">Since this work focuses on the effectiveness and efficiency of vehicle coordination strategies, the safety redundancy uti- lized is a fixed value. In future work, incorporating flexible security redundancy </a>[51] <span style=" color: #000;">and the delay indicator into the algo- rithm design is anticipated to further enhance the algorithm’s robustness against communication delays and noise, thereby reducing the accident rate.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s10" style="display: inline;"><a name="bookmark65">Multilane Scenario</a></p></li></ol><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark63" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">For other types of methods, increasing the  number  of lanes may lead to an exponential increase in computational complexity. However, our method can be adapted for multilane intersections with straightforward adjustments. We designed an intersection with multiple lanes, as illustrated in Fig. </a>13<a href="#bookmark63" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">(a). By adjusting the input and output dimensions of the neural network, our collaborative strategy can be effectively extended to multilane scenarios. Fig. </a>13<span style=" color: #000;">(b) presents the convergence performance of our method.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In summary, the proposed method demonstrates effective operation at intersections of various shapes and lane con- figurations. It can adapt to  different  traffic  environments and requirements, showcasing its significant flexibility and adaptability.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 96pt;text-indent: -19pt;text-align: left;"><p style="display: inline;"><a name="bookmark66">F</a><span class="s5">IELD </span>E<span class="s5">XPERIMENTS</span></p><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark67" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">A miniaturized autonomous driving platform has been developed to evaluate the  effectiveness  and  feasibility  of the proposed method in real-world scenarios. Section </a>VI-A <a href="#bookmark70" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">outlines the infrastructure of the platform. Section </a>VI-B <a href="#bookmark73" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">details the specific experimental settings, and Section </a>VI-C <span style=" color: #000;">analyzes the experimental results.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l13"><li style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><p class="s10" style="display: inline;"><a name="bookmark67">Infrastructure of the Miniaturized Autonomous Vehicle Platform</a></p><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark68" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">As illustrated in Fig. </a>14<span style=" color: #000;">, our platform constitutes an autonomous driving system scaled 10:1, comprising an upper</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 44pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="232" height="267" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_085.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark68">Fig. 14.   Components of the miniaturized autonomous driving platform.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="267" height="203" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_086.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark69">Fig. 15.    Intersection site used in the field experiments, where the dashed line area is the conflict zone, and the rest is the preparatory zone.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">PC, a communication unit, autonomous vehicles, and position- ing modules.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The positioning modules simulates GPS functionality, enabling the acquisition and broadcasting of vehicle driving information. The autonomous vehicle is powered by the RK3399 core board running the robot operating system (ROS), which facilitates the deployment and operation of control algo- rithms. The upper PC emulates the central coordinator, which can accept the vehicle information and make decisions through the trained vehicle cooperation model. The communication unit utilizes a WIFI link to facilitate real-time communication between the vehicle and the upper PC. Equipped with which, the information transmission from vehicle to PC, and the subsequent vehicles’ feedback on the executing decisions can be realized.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s10" style="display: inline;"><a name="bookmark70">Experimental Settings</a></p><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark69" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">In the field experiments, the intersection scenario utilized is depicted in Fig. </a>15<span style=" color: #000;">. A total of four autonomous vehicles are deployed in this scenario, starting simultaneously from various locations within the preparatory zone and entering the conflict</span></p><p class="s5" style="padding-top: 3pt;padding-left: 82pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark71">TABLE VII</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="480" height="14" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_087.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="48" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_088.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="7" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_089.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="81" height="59" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_090.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="87" height="94" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_091.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="148" height="57" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_092.png"/></span></p><p class="s5" style="padding-left: 82pt;text-indent: 0pt;line-height: 9pt;text-align: center;">S<span class="s12">CENE AND </span>C<span class="s12">ONTROLLER </span>P<span class="s12">ARAMETERS IN </span>F<span class="s12">IELD </span>E<span class="s12">XPERIMENTS</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="478" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_093.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="480" height="74" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_094.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 90pt;text-indent: 0pt;line-height: 7pt;text-align: left;">	<span><img width="100" height="32" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_095.png"/></span><span><img width="93" height="10" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_096.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="478" height="1" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_097.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 10pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="321" height="93" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_098.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="29" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_099.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="322" height="187" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_100.png"/></span></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark72">Fig. 16.   Experimental snapshots of our platform.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">zone. Throughout this process, all infrastructure components within the intersection cooperated fully to facilitate the vehi- cles’ crossing maneuvers without incidents.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="322" height="187" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_101.png"/></span></p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark71" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The experimental parameters are presented in Table </a>VII<span style=" color: #000;">. Given the scaling characteristics of the autonomous driving platform,  the  relevant  parameters  have  been  appropri- ately scaled to maintain consistency with the real-world environment.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 3pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s10" style="display: inline;"><a name="bookmark73">Experimental Results</a></p></li></ol><p class="s7" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark72" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Various collaborative algorithms are deployed on the upper computer, while  the  underlying  control  is  handled by the vehicle’s core board. The  retreat  time  and  aver- aged delay time  of  four  vehicles  are  recorded,  controlled by different collaborative algorithms, in completing the cooperative task. Fig. </a>16  <span style=" color: #000;">presents  a  series  of  experimen- tal snapshots. Some demo videos can be accessed at https://github.com/Dingyh26/DRL_Cooperation_Demos.</span></p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark74" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Ten independent realizations have been conducted to obtain averaged values. As shown in Fig. </a>17<span style=" color: #000;">, a comparative anal- ysis between different algorithms indicates that our method maintains outstanding performance in the practical scenario. Specifically, our method reduces the retreat time by 37.83%, 37.03%, 34.06%, and 22.98%, respectively, and the averaged</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">delay time by 50.82%, 44.65%, 27.18%, and 18.26% respec- tively, compared to methods including DFST, MCC, CGTS, and PPO. The experiments conducted on the autonomous driv- ing platform further demonstrate the potential and superiority of our proposed method in the collaborative application of intelligent vehicles in real-world scenarios.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Additionally, since the  experiments  have  been  con- ducted using real physical components, various environmental</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Fig. 17.  Retreat time and averaged delay time results in the field experiments.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark74">uncertainties—including communication delay, processing delay, and device response delay—have been evaluated on the platform, with the total delay remaining at 300 ms or less. Our method successfully completes the cooperation task even under the maximum delay of 300 ms, demonstrating its robust ability to mitigate delay risks in real-world scenarios.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 6pt;padding-left: 114pt;text-indent: -23pt;text-align: left;"><p style="display: inline;"><a name="bookmark75">C</a><span class="s5">ONCLUSION</span></p></li></ol><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">In this  article,  a  deep  RL  approach  has  been  devised to facilitate vehicle collaboration at signal-free intersections. The cooperative problem is first modeled as the Markov decision process, and the mathematical formulation of the model is meticulously tailored to meet the task’s requirements.</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The framework of DRL has been rectified, a shared A2C network architecture that employs LSTM and fully connected layers is introduced to efficiently capture the spatial-temporal characteristics of the involved states. The design of partially shared Actor and Critic enhances  the  consistency  of  fea- ture learning processes, promotes more efficient collaboration between policy learning and value function estimation, thereby improving the model’s performance. To enhance the speed and stability of the model training procedure, an asynchronous training strategy that simultaneously drives multiple training processes to share and train the network model is considered. Simulation results demonstrate that the proposed method can be effectively applied to common intersection scenarios, with a significantly reduction on vehicle retreat time and averaged delay time compared to other benchmark algorithms, consum- ing less computational budget. Furthermore, field experiments conducted through a miniaturized autonomous driving plat- form have further proven the potential and practical value of the proposed method in real-world applications. Finally, to account for the uncertainties present in the real world, the effects of communication delay on our vehicle collaboration method are also tested and analyzed.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In the future, the design of flexible security redundancies and constraints will be further explored to mitigate uncertain communication delays and reduce the accident rate. Further investigation is worthy to expanding the environmental con- figurations and to be better aligned with practical conditions. We will delve deeper into the potential challenges of actual deployment, including more  complex environmental uncer- tainties and interactions with non-CAV traffic, to expand the proposed method to full-size vehicles in real world.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: center;">R<span class="s5">EFERENCES</span></p><p class="s5" style="padding-top: 6pt;padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark76">[1] S. E. Shladover, “Connected and automated vehicle systems: Introduction and overview,” </a><i>J.  Intell.  Transp.  Syst.</i>,  vol.  22,  no.  3, pp. 190–200, 2018.<a name="bookmark77">&zwnj;</a></p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark78">[2] S.  E.  Li,  S.  Xu,  X.  Huang,  B.  Cheng,  and  H.  Peng,  “Eco- departure of connected vehicles with V2X communication at signalized intersections,” </a><i>IEEE Trans. Veh. Technol.</i>, vol. 64, no. 12, pp. 5439–5449, Dec. 2015.</p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark79">[3] Q. Guo, L. Li, and X. J. Ban, “Urban traffic signal control with connected and automated vehicles: A survey,” </a><i>Transp. Res. Part-C, Emerg. Technol.</i>, vol. 101, pp. 313–334, Apr. 2019.</p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark80">[4] R. Azimi, G. Bhatia, R. R. Rajkumar, and P. Mudalige, “STIP: Spatio- temporal intersection protocols for autonomous vehicles,” in </a><i>Proc. ACM/IEEE Int. Conf. Cyber-Phys. Syst. (ICCPS)</i>, 2014, pp. 1–12.</p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark81">[5] S. Wang,  J. Zhang, and X.  Tan, “PDLC-LIO: A precise  and direct SLAM system  toward large-scale  environments with  loop closures,”  </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 25, no. 1, pp. 626–637, Jan. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark82">[6] C. Wu, Z. Cai, Y. He, and X. Lu, “A review of vehicle group intelligence in a connected environment,” </a><i>IEEE Trans. Intell. Veh.</i>, vol. 9, no. 1, pp. 1865–1889, Jan. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark83">[7] B. Ji et al., “Survey on the Internet of Vehicles: Network architectures and applications,” </a><i>IEEE Commun. Stand. Mag.</i>, vol. 4, no. 1, pp. 34–41, Mar. 2020.</p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark84">[8] D. Li et al., “A tightly coupled bi-level coordination framework for CAVs at road intersections,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 25, no. 7, pp. 7832–7847, Jul. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark85">[9] P. Cai, J. He, and Y. Li, “Hybrid cooperative intersection management for connected automated vehicles and pedestrians,” </a><i>J. Intell. Connected Veh.</i>, vol. 6, no. 2, pp. 91–101, Jun. 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[10] W. Liu et al., “A systematic survey of control techniques and applications in connected and automated vehicles,” <i>IEEE Internet Things J.</i>, vol. 10, no. 24, pp. 21892–21916, Dec. 2023.</p><p class="s5" style="padding-top: 4pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark86">[11] F. Ahmad, O. Almarri, Z. Shah, and L. Al-Fagih, “Game theory applications in traffic management: A review of authority-based travel modelling,” </a><i>Travel Behav. Soc.</i>, vol. 32, Jul. 2023, Art. no. 100585.<a name="bookmark87">&zwnj;</a></p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark88">[12] C. Chen et al., “A graph-based conflict-free cooperation method for intelligent electric vehicles at unsignalized intersections,” in </a><i>Proc. IEEE Int. Intell. Transp. Syst. Conf. (ITSC)</i>, 2021, pp. 52–57.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark89">[13] N. P. Farazi, B. Zou, T. Ahamed, and L. Barua, “Deep reinforcement learning in transportation research: A review,” </a><i>Transp. Res. Interdiscipl. Perspect.</i>, vol. 11, Sep. 2021, Art. no. 100425.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark90">[14] A. Haydari and Y. Yılmaz, “Deep reinforcement learning for intelligent transportation systems: A survey,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 23, no. 1, pp. 11–32, Jan. 2020.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark91">[15] C. Creß, Z. Bing, and A. C. Knoll, “Intelligent transportation systems using roadside infrastructure: A literature survey,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 25, no. 7, pp. 6309–6327, Jul. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark92">[16] Y. Zhao, D. Gong, S. Wen, L. Ding, and G. Guo, “A privacy-preserving- based distributed collaborative scheme for connected autonomous vehicles at multi-lane signal-free intersections,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 25, no. 7, pp. 6824–6835, Jul. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark93">[17] K. Ji, N. Li, M. Orsag, and K. Han, “Hierarchical and game-theoretic decision-making for connected and automated vehicles in overtaking scenarios,” </a><i>Transp. Res. Part-C, Emerg. Technol.</i>, vol. 150, May 2023, Art. no. 104109.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark94">[18] X. Gong, S. Liang, B. Wang, and W. Zhang, “Game theory-based decision-making and iterative predictive lateral  control  for  coopera- tive obstacle avoidance of guided vehicle platoon,” </a><i>IEEE Trans. Veh. Technol.</i>, vol. 72, no. 6, pp. 7051–7066, Jun. 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark95">[19] L. Yang et al., “Multi-lane coordinated control strategy of connected and automated vehicles for on-ramp merging area based on coop- erative game,” </a><i>IEEE Trans.  Intell.  Transp.  Syst.</i>,  vol.  24,  no.  11, pp. 13448–13461, Nov. 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark96">[20] C. Chen, Q. Xu, M. Cai, J. Wang, J. Wang, and K. Li, “Conflict-free cooperation method for connected and automated vehicles at unsignal- ized intersections: Graph-based modeling and optimality analysis,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 23, no. 11, pp. 21897–21914, Nov. 2022.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark97">[21] Y. Li, M. Liu, Q. Yang, Z. Shen, and W. Wu, “Collision-free autonomous scheduling at unsignalized intersection using conflict graph tree search,”  </a><i>IEEE Internet Things J.</i>, vol. 11, no. 8, pp. 14563–14578, Apr. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark98">[22] J. Shi, Y. Luo, P.  Li,  J.  Wang,  and  K.  Li,  “Collaborative  multi- lane on-ramp merging strategy for connected and automated vehicles using dynamic conflict graph,” </a><i>J. Intell. Connected Veh.</i>, vol. 7, no. 1, pp. 38–51, 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark99">[23] J. Shi, K. Li, C. Chen, W. Kong, and Y. Luo, “Cooperative merging strategy in mixed traffic based on optimal final-state phase diagram with flexible highway merging points,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 24, no. 10, pp. 11185–11197, Oct. 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark100">[24] Y. Guan, Y. Ren, S. E. Li, Q. Sun, L. Luo, and K. Li, “Centralized cooperation for connected and automated vehicles at intersections by proximal policy optimization,” </a><i>IEEE Trans.  Veh.  Technol.</i>,  vol.  69, no. 11, pp. 12597–12608, Nov. 2020.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark101">[25] J. Luo et al., “Real-time cooperative vehicle coordination at unsignalized road intersections,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 24, no. 5, pp. 5390–5405, May 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark102">[26] Z. Guo, Y. Wu, L. Wang, and J. Zhang, “Heuristic-based multi-agent deep reinforcement learning approach for coordinating connected and automated vehicles at non-signalized intersection,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 25, no. 11, pp. 16235–16248, Nov. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark103">[27] B. Peng, M. F. Keskin, B. Kulcsár, and H. Wymeersch, “Connected autonomous vehicles for improving mixed traffic efficiency in unsignal- ized intersections with deep reinforcement learning,” </a><i>Commun. Transp. Res.</i>, vol. 1, Dec. 2021, Art. no. 100017.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark104">[28] S. Wu, D. Tian, X. Duan, J. Zhou, D. Zhao, and D. Cao, “Continuous decision-making in lane changing and overtaking maneuvers for unmanned vehicles: A risk-aware reinforcement learning approach with task decomposition,” </a><i>IEEE Trans. Intell. Veh.</i>, vol. 9, no. 4, pp. 4657– 4674, Apr. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark105">[29] H. Qian et al., “Collaborative overtaking strategy for enhancing overall effectiveness of mixed connected and connectionless vehicles,” </a><i>IEEE Trans. Mobile Comput.</i>, vol. 23, no. 12, pp. 13556–13572, Dec. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark106">[30] D. Chen et al., “Deep multi-agent reinforcement learning for highway on-ramp merging in mixed traffic,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 24, no. 11, pp. 11623–11638, Nov. 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[31] X. Zhang, L. Wu, H. Liu, Y. Wang, H. Li, and B. Xu, “High-speed ramp merging behavior decision for autonomous vehicles based on multiagent reinforcement learning,” <i>IEEE Internet Things J.</i>, vol. 10, no. 24, pp. 22664–22672, Dec. 2023.</p><p class="s5" style="padding-top: 2pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark107">[32] X. Gao et al., “A survey of collaborative perception in intelligent vehicles at intersections,” </a><i>IEEE Trans. Intell. Veh.</i><a href="http://dx.doi.org/10.1109/TIV.2024.3395783" class="s9" target="_blank">, early access, May 1, 2024, doi: </a><span style=" color: #00F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;">10.1109/TIV.2024.3395783</span>.<a name="bookmark108">&zwnj;</a></p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;text-align: justify;"><a name="bookmark109">[33] S. Chen, X. Hu, J. Zhao, R. Wang, and M. Qiao, “A review of decision-making and planning for autonomous vehicles in intersection environments,” </a><i>World Elect. Veh. J.</i>, vol. 15, no. 3, p. 99, 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark110">[34] K. Zhang, Z. Cui, and W. Ma, “A survey on reinforcement learning- based control for signalized intersections with connected automated vehicles,” </a><i>Transp. Rev.</i>, vol. 44, no. 6, pp. 1187–1208, 2024.</p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">[35]  V. Iordache, M. Minea, R. A. Gheorghiu, F. Badau, V.  Stoica, and</p><p class="s5" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark111">V. A. Stan, “Assessing performance of long-range ZigBee for road infrastructure communications,” in </a><i>Proc. 16th Int. Conf. Electron., Comput. Artif. Intell. (ECAI)</i>, 2024, pp. 1–5.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;text-align: justify;"><a name="bookmark112">[36] A. A. Malikopoulos, C. G. Cassandras, and Y. J. Zhang, “A decentralized energy-optimal control framework for connected automated vehicles at signal-free intersections,” </a><i>Automatica</i>, vol. 93, pp. 244–256, Jul. 2018.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark113">[37] P. Hang,  C.  Huang,  Z.  Hu,  and  C.  Lv,  “Driving  conflict  resolution of autonomous vehicles at unsignalized intersections: A differential game approach,” </a><i>IEEE/ASME Trans.  Mechatronics</i>,  vol.  27,  no.  6, pp. 5136–5146, Dec. 2022.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark114">[38] W. Xiao et al., “Decision-making for autonomous vehicles in random task scenarios at unsignalized  intersection using  deep reinforcement learning,” </a><i>IEEE Trans. Veh. Technol.</i>, vol. 73, no. 6, pp. 7812–7825, Jun. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark115">[39] R. Chen and Z. Yang, “Cooperative ramp merging strategy at multi-lane area for automated vehicles,” </a><i>IEEE Trans. Veh. Technol.</i>, vol. 73, no. 10, pp. 14326–14340, Oct. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark116">[40] Y. Li, Z. Chen, T. Wang, X. Zeng, and Z. Yin, “Data-driven hierar- chical model predictive control for automated overtaking maneuver via gaussian process regression,” </a><i>IEEE Trans. Veh. Technol.</i>, vol. 74, no. 1, pp. 263–278, Jan. 2025.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark117">[41] Y. Kebbati, N. Ait-Oufroukh, D. Ichalal, and V. Vigneron, “Lateral control for autonomous wheeled vehicles: A technical review,” </a><i>Asian J. Control</i>, vol. 25, no. 4, pp. 2539–2563, 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark118">[42] X. Chen, P. Li, Q. Zhang, and Z. Jin, “Research and application of improved pure pursuit algorithm in low-speed driverless vehicle system,” in </a><i>Proc. IEEE Int. Conf. Adv. Elect. Eng. Comput. Appl. (AEECA)</i>, 2022, pp. 1483–1488.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark119">[43] S. Kim, J. Lee, K. Han, and S. B. Choi, “Vehicle path tracking control using pure pursuit with MPC-based look-ahead distance optimization,”  </a><i>IEEE Trans. Veh. Technol.</i>, vol. 73, no. 1, pp. 53–66, Jan. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;text-align: justify;"><a name="bookmark120">[44] M. Treiber, A. Hennecke, and D. Helbing, “Congested traffic states in empirical observations and microscopic simulations,” </a><i>Phys. Rev. E</i>, vol. 62, no. 2, p. 1805, 2000.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark121">[45] O. Sigaud and O. Buffet, </a><i>Markov Decision Processes in Artificial Intelligence</i>. Hoboken, NJ, USA: Wiley, 2013.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark122">[46] P. A. Lopez et al., “Microscopic traffic simulation using SUMO,” in </a><i>Proc. 21st IEEE Int. Conf. Intell. Transp. Syst.</i>, 2018, pp. 2575–2582. [Online]. Available: https://elib.dlr.de/124092/</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark123">[47] B. Xu et al., “Distributed conflict-free cooperation for multiple con- nected vehicles at unsignalized intersections,” </a><i>Transp. Res. Part-C, Emerg. Technol.</i>, vol. 93, pp. 322–334, Aug. 2018.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;text-align: justify;"><a name="bookmark124">[48] S. Zeadally, J. Guerrero, and J. Contreras, “A tutorial survey on vehicle-to-vehicle communications,” </a><i>Telecommun. Syst.</i>, vol. 73, no. 3, pp. 469–489, 2020.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark125">[49] C. Sun, Y. Cui, N.-D. Ðào, W. Shi, and A. Khajepour, “Delay mitigation for V2I-based cooperative autonomous driving applica- tions,” in </a><i>Proc. IAVSD  Int.  Symp.  Dyn.  Veh.  Roads  Tracks</i>,  2023, pp. 502–510.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[50] R. Xu, Y. Guo, X. Han, X. Xia, H. Xiang, and J. Ma, “OpenCDA: An open cooperative driving automation framework integrated with co- simulation,” in <i>Proc. IEEE Int. Intell. Transp. Syst. Conf. (ITSC)</i>, 2021, pp. 1155–1162.</p><p class="s5" style="padding-top: 2pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark126">[51] H. Xu, W. Xiao, C. G. Cassandras, Y. Zhang, and L. Li, “A general framework for decentralized safe optimal control of connected and automated vehicles in multi-lane signal-free intersections,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 23, no. 10, pp. 17382–17396, Oct. 2022.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="95" height="120" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_102.jpg"/></span></p><h4 style="padding-left: 88pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Shuai Wang <span class="s5">received the B.S. and Ph.D. degrees from Xidian University, Xi’an, China, in 2015 and 2020, respectively.</span></h4><p class="s5" style="padding-left: 88pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">He is currently with the School of Intelligent Systems Engineering, Sun Yat-sen University, Shenzhen, Guangdong, China. His research interests include complex networks and intelligent systems.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="96" height="120" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_103.jpg"/></span></p><h4 style="padding-left: 88pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Yuhao Ding <span class="s5">received the B.E. degree from the School of Intelligent Engineering with Sun Yat-sen University, Shenzhen, China, in 2023, where he is currently pursuing the master’s degree.</span></h4><p class="s5" style="padding-left: 88pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">His research interests include connected and autonomous vehicles and intelligent transportation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="96" height="120" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_104.jpg"/></span></p><h4 style="padding-left: 88pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Xiaoqi Ding <span class="s5">is currently  pursuing  the  B.E. degree with the School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China.</span></h4><p class="s5" style="padding-left: 88pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">Her research interests include robot cluster col- laboration and decision making.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="96" height="120" alt="image" src="2025-Solving_the_Vehicle_Cooperation_Problem_at_Signal-Free_Intersection_via_an_Asynchronous_Deep_Reinforcement_Learning_Approach/Image_105.jpg"/></span></p><h4 style="padding-left: 88pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Xiaojun Tan <span class="s5">(Senior Member, IEEE) received the Ph.D. degree from the Department of Electronic Engineering, Sun Yat-sen University, Shenzhen, China, in 2005.</span></h4><p class="s5" style="padding-left: 88pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">He had been serving with the School of Engineering, Sun Yat-sen University since July 2005, where  he  is  with the  School  of  Intelligent Systems Engineering from 2017, he has been the Head of the New Energy Vehicles Research Center. Meanwhile, he acts also as the core member of innovation team of Southern Marine Science and</p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Engineering Guangdong Laboratory, Zhuhai, China.</p></body></html>
