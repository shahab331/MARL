<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 24pt; }
 .s2 { color: #7F7F7F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 20pt; }
 .s3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s4 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s5 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s7 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 28.5pt; }
 .a { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s8 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s9 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s10 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s11 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s12 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 h3 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 5.5pt; }
 h4 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 4.5pt; }
 .s13 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s14 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s15 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s16 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 3.5pt; }
 .s17 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -2pt; }
 .s18 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s19 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s20 { color: #F00; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s21 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s22 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 .s23 { color: #4671C4; font-family:"Adobe Song Std L", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; vertical-align: -2pt; }
 .s24 { color: #4671C4; font-family:"Adobe Song Std L", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s25 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s26 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s27 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s28 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s29 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s30 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s31 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s32 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s33 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s34 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -2pt; }
 .s35 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s36 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s37 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s38 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s39 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s40 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s41 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s42 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s43 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s44 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -7pt; }
 .s45 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -9pt; }
 .s46 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -3pt; }
 .s47 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s48 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 3pt; }
 .s49 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s50 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s51 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s52 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s53 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s54 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 10pt; }
 .s55 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 6pt; }
 .s56 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -3pt; }
 .s57 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 5pt; }
 .s58 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s59 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 9pt; }
 .s60 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 7pt; }
 .s61 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -6pt; }
 .s62 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s63 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s64 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s65 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s66 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -1pt; }
 .s67 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 0; }
 #l1> li:before {counter-increment: c1; content: counter(c1, upper-roman)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l2 {padding-left: 0pt;counter-reset: d1 0; }
 #l2> li:before {counter-increment: d1; content: counter(d1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l3 {padding-left: 0pt;counter-reset: d2 0; }
 #l3> li:before {counter-increment: d2; content: counter(d2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l4 {padding-left: 0pt;counter-reset: d2 0; }
 #l4> li:before {counter-increment: d2; content: counter(d2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l5 {padding-left: 0pt; }
 #l5> li:before {content: "• "; color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 #l6 {padding-left: 0pt;counter-reset: d2 0; }
 #l6> li:before {counter-increment: d2; content: counter(d2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l7 {padding-left: 0pt;counter-reset: d2 0; }
 #l7> li:before {counter-increment: d2; content: counter(d2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l8 {padding-left: 0pt;counter-reset: f1 0; }
 #l8> li:before {counter-increment: f1; content: counter(f1, decimal)". "; color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 #l9 {padding-left: 0pt;counter-reset: f2 0; }
 #l9> li:before {counter-increment: f2; content: counter(f2, decimal)". "; color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 #l10 {padding-left: 0pt;counter-reset: g1 1; }
 #l10> li:before {counter-increment: g1; content: counter(g1, decimal)". "; color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 #l11 {padding-left: 0pt;counter-reset: h1 0; }
 #l11> li:before {counter-increment: h1; content: counter(h1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l12 {padding-left: 0pt;counter-reset: i1 19; }
 #l12> li:before {counter-increment: i1; content: "("counter(i1, lower-latin)") "; color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l13 {padding-left: 0pt;counter-reset: i1 20; }
 #l13> li:before {counter-increment: i1; content: "("counter(i1, lower-latin)") "; color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l14 {padding-left: 0pt;counter-reset: i2 0; }
 #l14> li:before {counter-increment: i2; content: counter(i2, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l15 {padding-left: 0pt;counter-reset: j1 2; }
 #l15> li:before {counter-increment: j1; content: counter(j1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l16 {padding-left: 0pt;counter-reset: k1 0; }
 #l16> li:before {counter-increment: k1; content: counter(k1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l17 {padding-left: 0pt;counter-reset: l1 0; }
 #l17> li:before {counter-increment: l1; content: counter(l1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l18 {padding-left: 0pt;counter-reset: l1 1; }
 #l18> li:before {counter-increment: l1; content: counter(l1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l19 {padding-left: 0pt;counter-reset: l2 0; }
 #l19> li:before {counter-increment: l2; content: counter(l2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><p class="s1" style="padding-left: 27pt;text-indent: 0pt;text-align: center;">Evaluating Scenario-based Decision-making for Interactive Autonomous Driving Using Rational Criteria: A Survey</p><p class="s2" style="padding-left: 1pt;text-indent: 0pt;line-height: 21pt;text-align: left;">arXiv:2501.01886v1  [cs.RO]  3 Jan 2025</p><p style="text-indent: 0pt;text-align: left;"/><p class="s3" style="padding-top: 11pt;padding-left: 147pt;text-indent: -131pt;text-align: left;">Zhen Tian<span class="s4">†</span>, Zhihao Lin<span class="s4">†</span>, Dezong Zhao<span class="s5">⋆</span>, <i>Senior Member, IEEE</i>, Wenjing Zhao, David Flynn, <i>Member, IEEE</i>, Shuja Ansari and Chongfeng Wei, <i>Member, IEEE</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: justify;">Abstract<span class="h2">—Autonomous vehicles (AVs) can significantly pro- mote the advances in road transport mobility in terms of safety, reliability, and decarbonization. However, ensuring safety and efficiency in interactive during within dynamic and diverse environments is still a primary barrier to large-scale AV adoption. In recent years, deep reinforcement learning (DRL) has emerged as an advanced AI-based approach, enabling AVs to learn decision-making strategies adaptively from data and interactions. DRL strategies are better suited than traditional rule-based meth- ods for handling complex, dynamic, and unpredictable driving environments due to their adaptivity. However, varying driving scenarios present distinct challenges, such as avoiding obstacles on highways and reaching specific exits at intersections, requir- ing different scenario-specific decision-making algorithms. Many DRL algorithms have been proposed in interactive decision- making. However, a rationale review of these DRL algorithms across various scenarios is lacking. Therefore, a comprehensive evaluation is essential to assess these algorithms from multiple perspectives, including those of vehicle users and vehicle manu- facturers. This survey reviews the application of DRL algorithms in autonomous driving across typical scenarios, summarizing road features and recent advancements. The scenarios include highways, on-ramp merging, roundabouts, and unsignalized in- tersections. Furthermore, DRL-based algorithms are evaluated based on five rationale criteria: driving safety, driving efficiency, training efficiency, unselfishness, and interpretability (DDTUI). Each criterion of DDTUI is specifically analyzed in relation to the reviewed algorithms. Finally, the challenges for future DRL- based decision-making algorithms are summarized.</span></p><p class="s7" style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: justify;">Index Terms<span class="h2">—Interactive autonomous driving, decision mak- ing, deep reinforcement learning, typical scenarios, rationale evaluation.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l1"><li style="padding-top: 7pt;padding-left: 59pt;text-indent: 26pt;text-align: left;"><p style="display: inline;"><a name="bookmark0">INTRODUCTION</a></p><h1 style="text-indent: 0pt;line-height: 27pt;text-align: left;">A</h1><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 22pt;text-align: right;"><a href="#bookmark46" class="a">UTONOMOUS  vehicles  (AVs)  face  significant  chal- lenges in making reliable decisions when interacting with human-driven vehicles (HDVs). This challenge is primarily due to the difficulty of accurately predicting the intentions of HDVs. Road traffic crashes cause significant fatalities and serious injuries, reflecting the global issue of millions of lives lost annually </a><a href="#bookmark47" class="a">[1]. Since 2021, over 900 Tesla crashes involving driver-assistance  systems  have  been  reported  </a><a href="#bookmark48" class="a">[2].  Despite unresolved safety issues, the number of AVs is projected to surpass 50 million by 2024 </a>[3]. These statistics underscore the critical need for improving safety in autonomous driving.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 5pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">The authors are with the James Watt School of Engineering, University of Glasgow, Glasgow, G12 8QQ, U.K.</p><p class="s9" style="padding-left: 13pt;text-indent: 0pt;line-height: 10pt;text-align: left;">⋆ <a href="mailto:dezong.zhao@glasgow.ac.uk" class="s10" target="_blank">Corresponding author: dezong.zhao@glasgow.ac.uk.</a></p><p class="s11" style="padding-left: 13pt;text-indent: 0pt;line-height: 8pt;text-align: left;">† <span class="s8">Equal contribution</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark49" class="a">With a safe decision-making system, AVs have the potential to significantly decrease road crashes caused by human er- rors such as fatigue, distraction, and delayed reactions </a><a href="#bookmark50" class="a">[4]. Moreover, AVs are capable of making optimal decisions faster than human drivers, thereby enhancing traffic efficiency </a><a href="#bookmark51" class="a">[5]. There are several typical driving scenarios, such as highways, roundabouts, on-ramping merging, and unsignalized intersec- tions, each characterized by distinct road features and scenario- specific requirements. Autonomous driving in such scenarios is depicted in Fig. 1. For example, on-ramp merging involves completing lane changes well in advance of any obstructed roadway, while navigating a roundabout requires seamlessly exiting at the intended point. Achieving these scenario-based requirements relies heavily on precise and timely operational decision-making in real time. Operational decision support for  AV  driving  includes  perception,  planning,  and  control modules. The perception module consists of onboard sensors that continuously perceive the surrounding environment. The perceived data is processed through perception algorithms, such as YOLO methods </a><a href="#bookmark52" class="a">[6], </a>[7]. The planning module handles driving  tasks  based  on  scenario  recognition.  Subsequently, the motion planner generates discrete decisions and converts them into feasible trajectories. These feasible trajectories are then transmitted to the control module to generate control commands,  which  are  sent  to  the  vehicle’s  actuators.  The actuators, including the steering wheel and pedals, receive and execute the control commands to drive the vehicle.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark53" class="a">The interactions between AVs and HDVs are complex and therefore continuous decision-making is required, such as lane changes or braking </a>[8]. The model-based, simple guidance, and learning-based methods are commonly used in interactive driving with HDVs.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark54" class="a">There are mainly four types of model-based approaches. The first model-based approach aims to predict the intentions or trajectories of HDVs, but heavily relies on rule-based classification. For example, </a><a href="#bookmark55" class="a">[9] predicts the trajectories of HDVs within a fixed time window. However, the time required for a lane-changing maneuver may exceed this fixed time win- dow. The second model-based approach is to make decisions using robust control methods, such as the min-max model predictive control </a><a href="#bookmark56" class="a">[10]. However, robust control methods make excessively cautious decisions based on a worst-case scenario assumption </a>[11].  These  methods  are  not  suitable  for  most real traffic environments because worst-case scenarios  are rare in real-world settings. Furthermore, decisions made for</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="540" height="280" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_001.png"/></span></p><p class="s12" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Fig. 1. Autonomous driving in different scenarios.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="348" height="257" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_002.jpg"/></span></p><h3 style="padding-top: 4pt;padding-left: 62pt;text-indent: 0pt;text-align: center;">a</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 174pt;text-indent: -16pt;text-align: left;">Conventional Mode:90 (36%)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 3pt;padding-left: 77pt;text-indent: -7pt;text-align: left;">Moving:108 (43%)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="text-indent: 0pt;text-align: right;">Major: 6</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 2pt;text-indent: 0pt;text-align: right;">Moderate: 40</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 4pt;text-indent: 0pt;text-align: left;">(2%)</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 4pt;text-indent: 0pt;text-align: left;">(16%)</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 106pt;text-indent: -3pt;text-align: left;">HDV hit ADS: 252 (79%)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 174pt;text-indent: -16pt;text-align: left;">Autonomous Mode:162 (64%)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 69pt;text-indent: -7pt;text-align: left;">Stopping:144 (57%)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="text-indent: 0pt;text-align: right;">Minor: 206</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 15pt;text-indent: 0pt;text-align: left;">(82%)</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;text-indent: 0pt;text-align: center;">b</h3><h4 style="padding-top: 3pt;text-indent: 0pt;text-align: right;">Major: 2</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">(3%)</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 106pt;text-indent: -3pt;text-align: left;">ADS hit HDV: 67 (21%)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 45pt;text-indent: -16pt;text-align: left;">Conventional Mode:48 (72%)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 2pt;padding-left: 54pt;text-indent: -16pt;text-align: left;">Autonomous Mode:19 (28%)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 59pt;text-indent: -7pt;text-align: left;">Moving:51 (76%)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 63pt;text-indent: -7pt;text-align: left;">Stopping:16 (24%)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="text-indent: 0pt;text-align: right;">Moderate:20</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 2pt;text-indent: 0pt;text-align: right;">Minor:45</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">(30%)</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">(67%)</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Fig. 2. Rear-end accident conditions between ADS and HDV: (a) Rear-end accidents that HDV hit an ADS from behind with a sample of 252; (b) Rear-end accidents that ADS hit an HDV from behind with a sample of 67 [16].</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark57" class="a">worst-case scenarios negatively impact driving quality, such as resulting in slower driving  speeds.  On  the  other  hand, the game theory, the third model-based approach, has gained popularity recently. Game theory includes cooperative and non-cooperative games, both relying on equilibrium models. However, these models fail to capture the complexities of real- world driving, which are characterized by uncertainties and do not adhere to a regular equilibrium framework. Therefore, model-based methods are unable to handle interactive driv- ing with HDVs effectively. Additionally, the fourth model- based approach, including collision-avoidance methods </a><a href="#bookmark58" class="a">[12] and Voronoi diagram-based methods </a><a href="#bookmark59" class="a">[13], is unable to safely respond to movable objects. Real-world collisions between HDVs and vehicles equipped with advanced driving system (ADS) assistance are summarized in </a>[14]. As illustrated in Fig. 2, <span class="s13">79 % </span>of accidents involve HDVs hitting AVs, and <span class="s13">21 % </span>of</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">that involve AVs hitting HDVs. Therefore, achieving collision- free interactions with HDVs are still to be addressed.</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark60" class="a">Compared to the aforementioned methods, simple guidance methods, such as risk-quantified fields, are widely used be- cause they do not need to predict HDVs’ intentions or make excessively cautious decisions </a><a href="#bookmark61" class="a">[15]. The artificial potential field (APF) is a typical example, which can guide the AV to the target lane without collisions by utilizing attractive and repulsive force fields </a><a href="#bookmark62" class="a">[16]. However, APF assumes that all areas around the vehicle have the same level of risk because it calculates risks toward the central point. This assumption differs from reality, where the front of a car faces more danger than other parts. Additionally, APF is difficult to generalize across different scenarios without prior knowledge of the entire environment [17].</a></p><p style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">To promote collision-free interactions, a large number of</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 32pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="265" height="313" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_003.png"/></span></p><p class="s12" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Fig. 3. DRL-based autonomous driving system</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark63" class="a">interactions are needed to exclude risky actions, taking into account the uncertainties in decision-making and the varying driving conditions of HDVs. Learning-based methods facilitate the exploration of control strategies by allowing full interaction with the mixed-traffic environment. These methods enable AVs to learn and adapt to complex driving scenarios through iter- ative interactions and feedback. Machine learning (ML) </a><a href="#bookmark64" class="a">[18], </a><a href="#bookmark65" class="a">[19] focuses on developing algorithms to make decisions based on data, including supervised, unsupervised, and reinforcement learning. Supervised learning trains models on labeled data, supporting tasks like classification </a><a href="#bookmark66" class="a">[20], </a><a href="#bookmark67" class="a">[21]. However, super- vised learning is less suited for implementation in real driving environments, as labeling complex driving scenarios exhaus- tively is challenging and impractical. Unsupervised learning methods are particularly suitable for interactive driving as they do not require labeled data, allowing agents to learn decision- making strategies independently. Unsupervised machine learn- ing has demonstrated robust performance across a range of driving scenarios </a><a href="#bookmark68" class="a">[22]. However, unsupervised learning often struggles with generalization in highly dynamic environments. Reinforcement learning (RL) is a powerful technique for mak- ing optimal decisions in dynamic environments </a><a href="#bookmark69" class="a">[23], </a>[24]. RL involves an agent that interacts with its environment and learns safe control strategies through a reward-based framework. The adaptability of RL makes it ideal for interactive driving, where the environment is constantly changing, and the AV must adjust its behavior accordingly.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark70" class="a">Deep reinforcement learning (DRL) is an advanced form of RL that combines the principles of deep learning </a><a href="#bookmark71" class="a">[25], </a><a href="#bookmark72" class="a">[26] with RL. By utilizing deep neural networks to approximate complex value functions, DRL enables agents to learn directly from perceptual inputs, such as sensory data. This capability allows DRL to handle more complex and real-time decision- making tasks compared to traditional RL. For example, </a>[27] demonstrates the application of DRL in collision-free path</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">planning against surrounding obstacles.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The DRL-based autonomous driving system is illustrated in Fig. 3. The agent interacts with the environment through actuators, observations, and rewards. The agent comprises a decision network that receives information from observations of the environment and uses rewards to assess its actions. These observations are provided by the observer, which in- terprets the state of the AV and its environment. Based on the observations, the agent generates control commands and then sends the commands to actuators. Following the actuation of these control commands, the renewed environment information and AV state are updated. Simultaneously, a reward function evaluates the agent’s actions based on predefined metrics such as safety, efficiency, or compliance to driving norms. This reward function assigns positive or negative rewards depending on how well the AV’s actions align with the desired outcomes. These rewards are then fed back to the agent, guiding the learning towards the optimal driving behavior.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark73" class="a">DRL has been proven effective in handling emergency situations, which are critical for real-world driving scenarios. For example, </a><a href="#bookmark76" class="a">[28] proposes a DRL-powered driving system designed to avoid collisions in emergencies. This system learns to react swiftly and safely  to  sudden  changes,  improving the robustness of decision-making in real-world conditions. Recently, several studies have been demonstrated in various scenarios </a>[29]–[38]. However, different scenarios present dis- tinct driving requirements, necessitating tailored algorithms. On highways, the decision-making of AVs primarily focuses on avoiding collisions with HDVs while maintaining a high average speed. In contrast, ramps introduce additional chal- lenges, such as blocked areas that are not present on highways, as illustrated in Fig. 4. Furthermore, it is essential to assess DRL-based algorithms based on demands from various social perspectives, including vehicle users, vehicle manufacturers, and public traffic systems. Research on DRL-based algorithms, categorized by driving scenarios and evaluated based on their adaptability to real-world demands, is crucial for identifying valuable research directions.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark77" class="a">This survey aims to review DRL-based algorithms for autonomous interactive driving, classified by scenarios and evaluated for adaptation to real-world conditions. Four typical scenarios are included: highways, on-ramping merging, round- abouts, and unsignalized intersections. Five key evaluation factors are used, including driving safety </a><a href="#bookmark78" class="a">[39], </a><a href="#bookmark79" class="a">[40], driving efficiency </a><a href="#bookmark80" class="a">[41], </a><a href="#bookmark75" class="a">[42], training efficiency </a><a href="#bookmark81" class="a">[35], </a>[43], unselfish-</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark83" class="a">ness </a><a href="#bookmark84" class="a">[44]–[46], and interpretability </a><a href="#bookmark85" class="a">[47], </a>[48] (DDTUI). DRL- based decision-making approaches are reviewed for the four typical scenarios and evaluated using the criteria of DDTUI. The evaluation is consistent across all papers by examining the inclusion of evaluation factors in the designed algorithms and their corresponding verifications. For example, if a paper discusses safety but doesn’t include verifications like a lower number of collisions or consistently maintaining safe distances <span class="s14">d</span><span class="s15">s</span>, it would not be considered to include the safety factor. The reminder of this survey is structured as follows: Section II describes the features and driving tasks of the four typical scenarios. Section III explains the rationale for selecting the five evaluation factors. Sections IV-VII evaluated DRL-based</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 49pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="272" height="144" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_004.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="272" height="273" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_005.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="272" height="132" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_006.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_007.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="38" height="27" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_008.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="334" height="381" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_009.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="26" height="16" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_010.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="92" height="17" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_011.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="58" height="58" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_012.png"/></span></p><p class="s16" style="padding-left: 42pt;text-indent: 0pt;text-align: left;">4</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 42pt;text-indent: 0pt;text-align: left;">4</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 41pt;text-indent: 0pt;text-align: left;">4</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 146pt;text-indent: 0pt;text-align: left;">(c)                                                                                                      <span class="s18"> (d)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 351pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="26" height="26" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_013.png"/></span>	<span><img width="105" height="16" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_014.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Fig. 4. Example scenarios of autonomous driving: (a) highway; (b) on-ramp merging; (c) roundabout with 12 ports (8 entrances: EM1–EM4, EB1–EB4; 4 exits: O1–O4) and a central planted island; (d) unsignalized intersection.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">algorithms for highways, on-ramping merging, roundabouts, and unsignalized intersections using the criteria of DDTUI, respectively. Finally, Section VIII provides conclusions and discussions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 57pt;text-indent: -15pt;text-align: left;"><p style="display: inline;"><a name="bookmark1">R</a><span class="s8">OAD </span>F<span class="s8">EATURES AND </span>D<span class="s8">RIVING </span>T<span class="s8">ASKS</span></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">This section provides the road features and driving tasks for AVs in the scenarios of highways, on-ramping merging, roundabouts, and unsignalized intersections.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l2"><li style="padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark2">Highways</a></p><ol id="l3"><li style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark3">Road Features and Driving Tasks: </a><span class="p">Highways are fun- damental components of road networks, designed to enable vehicle movement over long distances with minimal interrup- tion. The design of highways focuses on safety, efficiency, and environmental impact. Safety features include wide lanes and clear signage to reduce collision risks.High efficiency is achieved by optimizing lane layouts to keep vehicles driving smoothly and reduce bottlenecks. The impact of highways on</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">natural landscapes is reduced through careful route planning. The <i>Interstate Highway System </i><a href="#bookmark86" class="a">in the  United  States  is  a vast network of highways designed to support long-distance travel and economic connectivity across states </a>[49]. Similarly, Germany’s <i>Autobahn</i><a href="#bookmark87" class="a">, known for its sections without speed limits, exemplifies the balance between high-speed travel and safety on highways [50].</a></p></li><li style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: left;"><p class="s19" style="display: inline;"><a name="bookmark4">An Example of a Highway: </a><span class="p">Fig. 4(a) presents a scenario involving a three-lane highway. The AV drives in main lane</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">3 and interacts with  HDVs  in  all  three  lanes.  There  are no disturbances or uncertainties other than the surrounding HDVs. Therefore, the issue of driving safety primarily relates to collisions with surrounding HDVs. In the car-following phase, the AV can follow the HDV ahead by adjusting its acceleration. However, cautious following can lead to a loss of driving efficiency. To maintain high driving efficiency, the AV may change lanes when the space ahead is limited. However, collisions with HDVs in the target lane could occur during the lane-changing. Therefore, the driving task on highways can be summarized as balancing collision avoidance with surrounding</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">HDVs while maintaining a consistently high speed.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s19" style="display: inline;"><a name="bookmark5">On-ramping Merging</a></p><ol id="l4"><li style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark6">Road Features and Driving Tasks: </a><span class="p">Ramps, including on-ramps or off-ramps, are essential components of highway systems. Due to the symmetry between on-ramping and off- ramping processes, this survey considers only on-ramping merging. Ramps enable the smooth and safe transition of vehicles between different roadways, typically connecting sur- face streets with highways. Ramps provide access to highways without disrupting traffic flow on the main highway lanes.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Ramp design focuses on safety, efficiency, and space utiliza- tion. Safety is crucial, as ramps must accommodate vehicles accelerating or decelerating while merging onto or diverging from the highway. On-ramps enhance traffic flow by reducing disruptions to mainline traffic and providing sufficient space for safe merging. Additionally, urban space constraints often require innovative ramp designs, such as cloverleaf inter- changes, to connect multiple roadways effectively.</p></li><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark7">Comparison with Highways: </a><span class="p">Highways and ramps serve different functions, which are summarized below.</span></p><ul id="l5"><li style="padding-top: 3pt;padding-left: 25pt;text-indent: -10pt;text-align: justify;"><p style="display: inline;">Functionality: Highways are designed for high-speed, long-distance travel with minimal interruption, while ramps are the transition between different road types.</p></li><li style="padding-left: 25pt;text-indent: -10pt;text-align: justify;"><p style="display: inline;">Design: Highways are characterized by long, straight stretches with multiple lanes, designed to maintain high speeds and efficient traffic flow. In contrast, ramps often involve curves and elevation changes, designed to accom- modate vehicles as they speed up or slow down.</p></li><li style="padding-left: 25pt;text-indent: -10pt;text-align: justify;"><p style="display: inline;">Speed: Highways support higher speeds, with vehicles typically traveling at constant high speeds over long distances. Ramps involve acceleration or deceleration, requiring careful design to manage the speed differential between the ramp lane and the main lane.</p></li></ul><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">For example, the <i>Cloverleaf Interchange </i><a href="#bookmark88" class="a">is a common de- sign that efficiently manages space while connecting highways with multiple surface streets </a>[51]. Another example is the <i>High Occupancy Vehicle (HOV) lane ramps</i><a href="#bookmark89" class="a">, which are designed to control the flow of carpool vehicles onto highways, providing direct and less congested access points [52].</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Consider a three-lane ramp scenario in Fig. 4(b), which includes two main lanes and one ramp lane. The AV interacts with both dynamic and static objects. The dynamic objects are surrounding HDVs, each with unique driving intentions, speeds, and acceleration patterns. The static object represents an obstruction within the ramp lane, rendering the lane impass- able and blocking access. As a result, the AV must change into the main lane before the ramp ends, considering the HDVs and the feasibility in lane-changing.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Waiting for enough space to change lanes and driving slowly to avoid blocked roads lead to safer driving. However, this cautious driving can significantly reduce driving efficiency and lower road capacity on the ramp. Consequently, it is challenging to navigate the ramp, avoid collisions with both surrounding HDVs and the blocked road ahead, while still maintaining a high driving speed.</p></li></ol></li><li style="padding-top: 2pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark8">Roundabouts</a></p><ol id="l6"><li style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark9">Road Features and Driving Tasks: </a><span class="p">Roundabouts are de- signed to improve traffic flow and enhance safety by reducing the likelihood of severe accidents. One example of a typical roundabout is </span>Folon’s obelisk in Pietrasanta <a href="#bookmark90" class="a">in Italy, which features a central island and circular roads around it </a><span class="p">[53]. Another example is the </span>Place Charles de Gaulle <a href="#bookmark91" class="a">in Paris, France, where twelve major avenues converge around the Arc de Triomphe [54].</a><a name="bookmark10">&zwnj;</a></p></li><li style="padding-left: 29pt;text-indent: -13pt;line-height: 11pt;text-align: justify;"><p class="s19" style="display: inline;">An Example of a Roundabout: <span class="p">An example of a round-</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">about is presented in Fig. 4(c). The AV starts from the EB4 port and has three possible exit choices: O1, O2, and O3. When the target exit is O1, the AV can simply follow the outer lane. For the target exit O2, there are two possible routes. One route is staying in the outer lane, which is generally safer. The other route is merging into the inner lane and exiting near O2. This second route is more efficient, as the inner lane offers a shorter curve length for the same round angle. However, rear vehicles driving in the inner lane bring potential collision risks. For the target exit O3, the AV must find the right moment to merge into the inner lane. After traveling in the inner lane for a period, the AV is expected to change lanes near the exit. The main challenge is to safely interact with other HDVs when approaching each of these three exits.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark11">Unsignalized Intersections</a></p><ol id="l7"><li style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark12">Road Features and Driving Tasks: </a><span class="p">Unsignalized inter- sections are critical components of road networks where two or roads meet or cross. They are designed to manage traffic flow from different directions, enabling vehicles to navigate safely through crossing points. Unsignalized intersection can control and organize traffic movements, reduce congestion, and enhance safety for all vehicles. One example is the </span><a href="#bookmark92" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt;">Diverging Diamond Interchange (DDI) </a><a href="#bookmark92" class="a">[55].</a><a name="bookmark13">&zwnj;</a></p></li><li style="padding-left: 29pt;text-indent: -13pt;line-height: 11pt;text-align: justify;"><p class="s19" style="display: inline;">An Example of an Unsignalized Intersection:  <span class="p">Fig. 4(d)</span></p></li></ol></li></ol><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">shows a three-lane unsignalized intersection designed for moderate to heavy traffic flow. The intersection accommodates vehicles from all four directions, with dedicated lanes for specific traffic movements. Each approach to the intersection includes three lanes, and the areas surrounding the intersection are grassland. At the center, where all four roads meet, there is an ample space for vehicles to make turns from any direction. This central area is essential for preventing bottlenecks and ensuring smooth traffic flow.</p></li><li style="padding-top: 7pt;padding-left: 49pt;text-indent: -19pt;text-align: left;"><p style="display: inline;"><a name="bookmark14">R</a><span class="s8">ATIONALE OF THE </span>E<span class="s8">VALUATION </span>F<span class="s8">ACTORS</span></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">In the context of adapting decision-making algorithms to real-world driving, five key evaluation factors have been selected: driving safety and efficiency, training efficiency, unselfishness, and interpretability. As depicted in Fig. 5, driving safety and efficiency form the foundation of any autonomous driving system. Training efficiency enables faster convergence of algorithms. Unselfishness enhances interaction with surrounding traffic, promoting cooperation with HDVs. Meanwhile, interpretability fosters public trust and addresses algorithmic errors, ensuring that decision-making is transpar- ent and understandable. The detailed rationale behind selecting these factors is discussed below.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="637" height="355" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_015.png"/></span></p><p class="s20" style="text-indent: 0pt;text-align: right;">Driving Safety</p><p class="s21" style="padding-top: 4pt;padding-left: 185pt;text-indent: 0pt;text-align: left;">Aggressive</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Reliable Autonomous Driving</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-left: 37pt;text-indent: 3pt;line-height: 111%;text-align: left;">Driving Efficiency</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l8"><li style="padding-left: 9pt;text-indent: -6pt;line-height: 6pt;text-align: left;"><p class="s21" style="display: inline;">Public Trustworthyness</p><p class="s21" style="padding-top: 1pt;padding-left: 60pt;text-indent: 68pt;line-height: 11pt;text-align: left;">Normal <span style=" color: #F00;">Unselfishness</span>: Smart Interactive</p><p class="s23" style="padding-left: 25pt;text-indent: 0pt;line-height: 9pt;text-align: left;">②     <span class="s24"> ③                                                       </span><span class="s21">Driving</span></p><p class="s24" style="padding-left: 32pt;text-indent: 0pt;text-align: left;">①</p><ol id="l9"><li style="padding-top: 4pt;padding-left: 82pt;text-indent: -6pt;text-align: left;"><p class="s21" style="display: inline;">Faster Convergence</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s21" style="padding-top: 3pt;padding-left: 11pt;text-indent: 5pt;line-height: 110%;text-align: left;">Public Feedback</p><p class="s20" style="text-indent: 0pt;text-align: right;">Interpretability</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 13pt;text-indent: -6pt;text-align: left;"><p class="s21" style="display: inline;">Debugging Simplicity</p></li></ol></li></ol><p class="s20" style="padding-left: 46pt;text-indent: 2pt;line-height: 6pt;text-align: left;">Training</p><p class="s20" style="padding-left: 46pt;text-indent: 0pt;text-align: left;">Efficiency</p><ol id="l10"><li style="padding-top: 2pt;padding-left: 14pt;text-indent: -6pt;text-align: left;"><p class="s21" style="display: inline;">Higher Training Rewards</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 14pt;text-indent: -6pt;text-align: left;"><p class="s21" style="display: inline;">Better Performance</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="text-indent: 0pt;text-align: right;">Trustworthy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="text-indent: 0pt;text-align: center;">High-quality</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 59pt;text-indent: 0pt;text-align: left;">Road Test</p><p class="s22" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: left;">Transparent</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 59pt;text-indent: 0pt;text-align: left;">Fast-update</p><p class="s12" style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Fig. 5. The importance and necessaries of achieving DDTUI in real-world autonomous driving.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l11"><li style="padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s19" style="display: inline;"><a name="bookmark15">Driving Safety</a></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark93" class="a">Driving safety is a fundamental requirement for autonomous vehicles. Frequent collisions cause substantial economic losses and pose  severe  safety  risks  </a><a href="#bookmark94" class="a">[56],  </a><a href="#bookmark95" class="a">[57].  Therefore,  driving safety is primarily evaluated based on the frequency of colli- sions with other vehicles </a><a href="#bookmark96" class="a">[58], </a><a href="#bookmark97" class="a">[59]. Minimizing collisions is a direct measure of the vehicle’s compliance to safety standards. Collision avoidance commonly relies on flexible reactions to hazardous areas. Once a hazard is detected, the system assesses the risk by analyzing the relative speed, distance, and trajectory of surrounding objects </a><a href="#bookmark98" class="a">[60]. Additionally, some autonomous driving systems evaluate possible decisions to avoid collisions while maintaining high efficiency </a><a href="#bookmark99" class="a">[61], </a><a href="#bookmark100" class="a">[62]. Furthermore, other autonomous driving systems use rule-based commands to adjust the AV’s behavior when unsafe conditions emerge [63]. For instance, AVs will be asked to stop when they encounter an interaction and spot-lines simultaneously [63].</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 6pt;padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s19" style="display: inline;"><a name="bookmark16">Driving Efficiency</a></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Driving efficiency refers to an AV’s ability to maintain a high average speed while adapting to varying traffic condi- tions. However, the implications of driving efficiency extend far beyond speed, affecting road capacity, user experience, and energy consumption.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark102" class="a">On road capacity, efficient driving allows vehicles to travel at optimal speeds, minimizing delays and reducing traffic congestion. For example, HDVs tend to drive faster on fa- miliar roads, contributing to higher road capacity and traf- fic flow </a>[64]–[66]. Similarly, AVs promote smoother traffic flow when they operate efficiently. Therefore, efficient driving allows more vehicles to travel smoothly without congestion. On user experience, an efficient journey means shorter travel</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark104" class="a">time and a smoother ride, significantly improving overall satisfaction </a><a href="#bookmark105" class="a">[67]–[69]. Besides, improving driving efficiency is crucial for reducing the energy consumption </a><a href="#bookmark106" class="a">[70], [71].</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark17">Training Efficiency</a></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark75" class="a">Training efficiency of algorithms directly impact the time and resources required to bring a fully functional AV system to reality. One primary benefit of improved training efficiency is the reduced training time. The acceleration allows developers to focus more on system fine-tuning and extensive testing. Sev- eral studies have reduced training time by adding extra training mechanisms or adjusting the structures of networks </a><a href="#bookmark107" class="a">[35], </a><a href="#bookmark108" class="a">[72]– </a>[74]. Another important benefit is the reduction in  device wear and tear. Fast and efficient training reduces the required computational resources. By improving training efficiency, the workload of computing equipment is minimized, resulting in less frequent maintenance and replacement.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark18">Unselfishness</a></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">In the context of autonomous driving, unselfishness refers to an AV’s ability to consider and accommodate the intentions of other HDVs on the road. Unselfishness evaluates how well an AV can cooperate with surrounding vehicles by predicting their intentions and adjusting its behavior accordingly. Human drivers often prioritize factors such as safety, efficiency, and comfort, and these intentions vary widely depending on the specific situations.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark109" class="a">Accurately classifying these driving intentions  is  essen- tial for effective interactions with surrounding HDVs. Exist- ing methods for recognizing driving intentions and enabling interaction-aware driving have been reviewed in </a>[75]. These methods categorize driving intentions across various scenarios,</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark110" class="a">including car following and lane changing </a><a href="#bookmark111" class="a">[76], </a><a href="#bookmark112" class="a">[77]. While many papers have focused on the self-driving characteristics of the ego vehicle </a><a href="#bookmark113" class="a">[78], </a>[79], the importance of unselfish behavior is becoming increasingly recognized.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">An unselfish AV that effectively anticipates and responds to the intentions of other vehicles contributes to a smoother and more harmonious traffic flow. By avoiding overly aggressive or excessively cautious driving behaviors, the AV can help minimize disruptions and conflicts with other vehicles. This cooperative approach enhances the safety and efficiency of all vehicles in the road network and improves the driving experience for everyone involved.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark19">Algorithm Interpretability</a></p></li></ol><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark84" class="a">Algorithm interpretability has gained significant importance due to DRL models are required to make logical decisions. A logical structure makes the black-box of learning more transparent </a><a href="#bookmark85" class="a">[47], </a><a href="#bookmark114" class="a">[48]. In DRL-based autonomous driving sys- tems, improving interpretability is crucial for system’s safety and transparency. To address the challenges in interpretability, various approaches have been adopted, including policy visual- ization to showcase DRL behaviors </a><a href="#bookmark115" class="a">[80], </a><a href="#bookmark117" class="a">[81], and surrogate models for approximate human-understandable explanations </a>[82]–[85]. Furthermore, specific rule-based methods, algorith- mic structure-adapted methods, and human-grounded methods</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">derived from common driving practices, ensuring a <span class="s14">d</span><span class="s15">s</span><span class="s25"> </span>with other vehicles. The dynamically-learned safety module uses driving data to learn safety  patterns.  By  integrating  both the handcrafted and dynamically-learned safety modules, the driving safety is improved.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark128" class="a">Moreover, deep deterministic policy gradients (DDPG) have been used to improve driving efficiency by overtaking sur- rounding vehicles in [96]. The overtaking-oriented training is achieved by adding a high reward for overtaking maneuvers. The reward function for overtaking is formulated as [96]:</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_016.png"/></span></p><p class="s13" style="padding-top: 6pt;padding-left: 5pt;text-indent: 27pt;text-align: left;"><i>R</i><span class="s26">overtaking</span><span class="s27"> </span>= <i>R</i><span class="s26">lane</span><span class="s27"> keeping </span>+ 100 <span class="s28">× </span>(<i>n </i><span class="s28">− </span><span class="p">RacePos</span>)    <span class="p">(1)</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_017.png"/></span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">where <span class="s14">R</span><span class="s26">lane</span><span class="s27"> keeping </span>is the reward for lane-keeping, <span class="s14">n </span>is the total number of vehicles in a given episode, and <span class="s14">RacePos </span>reflects the number of vehicles in front of the AV. Therefore, the larger the <span class="s14">RacePos</span>, the smaller the <span class="s14">R</span><span class="s26">overtaking</span>. Although safety rewards are applied, the collision rate increases with the frequency of overtaking.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark129" class="a">Furthermore, non-linear model predictive control (NMPC) has been integrated with DDQN to maintain safe highway driv- ing in [97]. NMPC inherently incorporates vehicle dynamics as constraints into its optimization, ensuring that the control inputs from the DRL agent remain within safe and feasible bounds [97]:</a></p><p class="s13" style="padding-top: 5pt;padding-left: 72pt;text-indent: 0pt;text-align: left;">   <span class="s29">T</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">have been proposed to assess interpretability.</p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;">Specific rules have been developed to assess interpretability</p><p class="s13" style="padding-left: 12pt;text-indent: 0pt;line-height: 10pt;text-align: left;">min</p><p class="s25" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">x<span class="s30">(</span>t<span class="s30">)</span>,u<span class="s30">(</span>t<span class="s30">)  </span><span class="s31">0</span></p><p class="s14" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">e<span class="s13">(</span>t<span class="s13">)</span><span class="s32">⊤</span>Qe<span class="s13">(</span>t<span class="s13">) + </span>rδ<span class="s33">2</span><span class="s13">(</span>t<span class="s13">) + </span>ru<span class="s33">2</span><span class="s13">(</span>t<span class="s13">)</span><span class="p">d</span>t</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">[86]. One such rule, known as <span class="s14">FAST </span>, evaluates interpretabil- ity via four criteria: <span class="s14">F </span>for fairness, <span class="s14">A </span>for accountability, <span class="s14">S </span>for</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">s.t.</p><p class="s14" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">x<span class="s13">˙ (</span>t<span class="s13">) = </span>f <span class="s13">(</span>x<span class="s13">(</span>t<span class="s13">)</span>, u<span class="s13">(</span>t<span class="s13">))</span>,</p><p class="s14" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: left;">e<span class="s15">y</span><span class="s34">min </span><span class="s35"> </span><span class="s28">≤ </span>e<span class="s15">y</span><span class="s25"> </span><span class="s13">(</span>t<span class="s13">) </span><span class="s28">≤ </span>e<span class="s15">y</span><span class="s34">max</span><span class="s35"> </span>,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">(2)</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">sustainability, and <span class="s14">T </span><a href="#bookmark119" class="a">for transparency </a>[87]. Fairness requires</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">models to be formalized using basic explanation labels and</p><p class="s37" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><span class="s36">e</span>ψ<span class="s35">min  </span><span class="s38">≤</span><span class="s28"> </span><span class="s14">e</span>ψ</p><ol id="l12"><li style="padding-top: 1pt;padding-left: 12pt;text-indent: -14pt;text-align: left;"><p class="s28" style="display: inline;">≤ <span class="s14">e</span><span class="s15">ψ</span></p><p class="s35" style="padding-top: 2pt;text-indent: 0pt;text-align: left;">max <span class="s36">,</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">functionality evaluation. Accountability refers to answerability and auditability, ensuring that the system has been clearly de- fined. Sustainability ensures safe operation without inequality or discrimination, while transparency ensures that the model’s internal rule settings are accessible and understandable.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark120" class="a">Some methods focuses on assessing interpretability by ad- justing DRL algorithmic structures. Researchers achieve this by developing standardized benchmarks that use interpretabil- ity metrics </a><a href="#bookmark121" class="a">[88], </a><a href="#bookmark122" class="a">[89] or by troubleshooting  explanations </a><a href="#bookmark123" class="a">[90], </a><a href="#bookmark124" class="a">[91] to identify instances where these explanations fall short. In addition, some studies concentrate on altering neural network architectures to enhance interpretability </a><a href="#bookmark125" class="a">[92], [93].</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark126" class="a">Furthermore, human-grounded methods focus on how easily people can understand the model’s key computational sections </a>[94]. DRL-based algorithms incorporating traffic-related mod- els enable people to better understand their structures through traffic knowledge or mathematical formulation, thereby im- proving interpretability.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 59pt;text-indent: -27pt;text-align: left;"><p style="display: inline;"><a name="bookmark20">D</a><span class="s8">EEP </span>R<span class="s8">EINFORCEMENT </span>L<span class="s8">EARNING</span>-<span class="s8">BASED DECISION</span>-<span class="s8">MAKING ON </span>H<span class="s8">IGHWAYS</span></p><ol id="l13"><ol id="l14"><li style="padding-top: 3pt;padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark21">Single-factor Methods for Highway Driving</a></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark127" class="a">Many works consider only one of five key factors. A Double Deep Q-Network (DDQN) is integrated with handcrafted safety and dynamically-learned safety modules in </a>[95]. The handcrafted  safety  module  relies  on  heuristic  safety  rules</p><p class="s14" style="padding-left: 71pt;text-indent: 0pt;line-height: 12pt;text-align: left;">δ<span class="s26">min</span><span class="s27"> </span><span class="s28">≤ </span>δ<span class="s13">(</span>t<span class="s13">) </span><span class="s28">≤ </span>δ<span class="s26">max</span>,</p><p class="s14" style="padding-left: 71pt;text-indent: 0pt;line-height: 15pt;text-align: left;">u<span class="s26">min </span><span class="s27"> </span><span class="s28">≤ </span>u<span class="s39">1</span><span class="s13">(</span>t<span class="s13">) </span><span class="s28">≤ </span>u<span class="s26">max</span></p><p class="s14" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><span class="p">where </span>T <span class="p">is the prediction horizon, </span>e<span class="s13">(</span>t<span class="s13">) </span><span class="p">is the error vector to be regulated to zero, and </span>Q <span class="s13">= </span><span class="p">diag</span><span class="s13">(</span>q<span class="s39">1</span>, q<span class="s39">2</span><span class="s13">) </span><span class="p">is a diagonal matrix of tracking weights. The control effort weight is denoted by</span></p><p class="s14" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">r<span class="p">. The steering angle is represented by </span>δ<span class="s13">(</span>t<span class="s13">)</span><span class="p">, and the control input is </span>u<span class="s13">(</span>t<span class="s13">)</span><span class="p">. The state vector is </span>x<span class="s13">(</span>t<span class="s13">)</span><span class="p">, and </span>f <span class="p">represents the system dynamics. </span>e<span class="s15">y</span><span class="s25"> </span><span class="s13">(</span>t<span class="s13">) </span><span class="p">and </span>e<span class="s15">ψ</span><span class="s25"> </span><span class="s13">(</span>t<span class="s13">) </span><span class="p">are the lateral position error and heading angle error, respectively. The  variables </span><span class="s36">e</span><span class="s37">y</span><span class="s35">min </span><span class="s40">,</span><span class="p"> </span>e<span class="s37">y</span><span class="s35">max </span><span class="s40">,</span><span class="p"> </span>e<span class="s37">ψ</span><span class="s35">min </span><span class="s40">,</span><span class="p"> </span>e<span class="s37">ψ</span><span class="s35">max </span><span class="s40">,</span><span class="p"> </span>δ<span class="s41">min</span><span class="s40">,</span><span class="p"> </span>δ<span class="s41">max</span><span class="s40">,</span><span class="p"> </span>u<span class="s41">min</span><span class="s40">,</span><span class="p"> and </span>u<span class="s41">max</span><span class="s27"> </span><span class="s40">are</span><a href="#bookmark132" class="a"> the minimum and maximum admissible values for the lateral position error, heading angle error, steering angle, and control input, respectively. NMPC improves the interpretability of safe control by providing a clear mathematical formulation that integrates the system’s constraints with the agent’s decision- making [98]–[100].</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark133" class="a">Additionally, a policy gradient (PG) method has been used with hard constraints to ensure safe highway driving in </a><a href="#bookmark134" class="a">[101]. These hard constraints prevent the AV from approaching risky boundaries, such as track edges. For example, the AV’s longi- tudinal and lateral positions are restricted from approaching the track boundaries. Cooperative lane-changing has been achieved in </a><a href="#bookmark135" class="a">[102], enhancing the unselfishness. Interpretability has been improved by combining DRL with imitation learning (IL) in </a>[103]. IL uses expert demonstrations to make the learning more interpretable. Training efficiency in highway</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark136" class="a">driving is also enhanced by integrating a spatial attention module and attention mechanism into the  deep Q-network in [104].</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark22">Dual-factor Methods for Highway Driving</a></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark137" class="a">Additionally, two of the five considered factors are inte- grated in some recent studies. The Intelligent Driver Model (IDM) </a><a href="#bookmark138" class="a">[105] has been incorporated into the DDQN for high- way driving in [106]. The IDM prevents collisions during car-following and therefore, the integration of DDQN with IDM enhances both the driving safety and interpretability in highway driving. The IDM is formulated as [106]:</a></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">efficiency and unselfishness are considered in highway driving. Driving efficiency is achieved by a reward based on <span class="s14">v</span><span class="s39">0</span>, <span class="s14">v</span><span class="s26">max</span>, and <span class="s14">v</span><span class="s26">min</span><a href="#bookmark146" class="a">. Unselfishness is achieved by penalizing unnecessary lane changes to reduce disturbances to HDVs. Driving safety and training efficiency have been addressed in </a>[114]. Safety is maintained by ensuring a <span class="s14">d</span><span class="s15">s</span><span class="s25"> </span>between vehicles using rule- based constraints, while training efficiency is improved by incorporating a multi-head attention mechanism.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark23">Three-factor Methods for Highway Driving</a></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: left;">Furthermore, three of the five considered factors are com- bined in a few recent papers. Driving safety, interpretability,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s27" style="padding-left: 49pt;text-indent: 0pt;text-align: left;"><span class="s42">U</span>IDM <span class="s43">=</span><span class="s13"> </span><span class="s14">U</span>max</p><p class="s13" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 13pt;text-align: center;">  <i>v</i><span class="s45">FV   </span><span class="s27"> </span><span class="s31">4</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="20" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_018.png"/></span></p><p class="s28" style="text-indent: 0pt;line-height: 9pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">1</p><p class="s14" style="padding-left: 18pt;text-indent: 0pt;line-height: 10pt;text-align: center;">v<span class="s15">e</span></p><p class="s13" style="padding-top: 2pt;padding-left: 10pt;text-indent: 0pt;line-height: 11pt;text-align: left;">  <i>g</i><span class="s46">∗</span><span class="s47">   </span><span class="s31">2</span><span class="s48"> </span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_019.png"/></span></p><p class="s28" style="text-indent: 0pt;line-height: 17pt;text-align: left;">—    <span class="s44">g</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: right;">(3)</p><p style="padding-left: 9pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark147" class="a">and driving efficiency have been improved in </a>[115]. Driving safety and interpretability are enhanced by using a collision penalty and the IDM. Driving efficiency is ensured by a reward</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">where <span class="s14">U</span><span class="s26">max </span><span class="s27"> </span>is the maximum acceleration of the AV, <span class="s14">v</span><span class="s15">e</span><span class="s25"> </span>is the expected velocity, and <span class="s14">g </span>is the gap between the AV and the</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">based on the velocity difference between <span class="s14">v</span><span class="s26">max </span><span class="s27"> </span>and <span class="s14">v</span><span class="s39">0</span>. The reward at time step <span class="s14">t </span><a href="#bookmark147" class="a">is formulated as [115]:</a></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">HDV. The desired gap <span class="s14">g</span><span class="s49">∗</span><span class="s47"> </span><a href="#bookmark138" class="a">between the AV and the front HDV is formulated as [106]:</a></p><p class="s28" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><span class="s14">R</span><span class="s15">t</span><span class="s25"> </span><span class="s13">= </span>−<span class="p">Collision </span>−</p><p class="s25" style="padding-top: 4pt;text-indent: 0pt;line-height: 3pt;text-align: right;">t</p><p class="s13" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">0<i>.</i>1</p><p class="s13" style="padding-top: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s28">× </span>(<i>v</i><span class="s50">t</span><span class="s25">    </span><span class="s28">− </span><i>v</i><span class="s50">t</span><span class="s25"> </span>) <span class="s28">− </span>0<i>.</i>4 <span class="s28">× </span>(<i>L </i><span class="s28">− </span>1)<span class="s33">2</span></p><p class="s27" style="padding-left: 18pt;text-indent: 0pt;line-height: 4pt;text-align: left;">max         <span class="s30">0</span></p><p class="s25" style="padding-top: 4pt;padding-left: 10pt;text-indent: 0pt;line-height: 3pt;text-align: left;">t</p><p class="s14" style="padding-top: 5pt;padding-left: 71pt;text-indent: 0pt;line-height: 12pt;text-align: left;">g<span class="s32">∗</span><span class="s47"> </span><span class="s13">= </span>d<span class="s15">s</span><span class="s25"> </span><span class="s13">+ </span>v<span class="s26">AV</span>T<span class="s15">e</span><span class="s25"> </span><span class="s28">−</span></p><p class="s14" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">v<span class="s26">AV</span><span class="s13">∆</span>v</p><p style="text-indent: 0pt;text-align: left;"><span><img width="50" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_020.png"/></span></p><p class="s28" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">√<span class="s51">          </span></p><p style="padding-top: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">(4)</p><p style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">where Collision, <span class="s14">v</span><span class="s52">max</span>, and <span class="s14">v</span><span class="s31">0</span><span class="s30"> </span>are the occurrence of a collision,</p><p style="padding-left: 9pt;text-indent: 0pt;line-height: 6pt;text-align: left;">maximum velocity, and current velocity at time <span class="s14">t</span>, respectively.</p><p class="s14" style="padding-left: 153pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s13">2  </span>U<span class="s26">max</span>b</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where <span class="s14">T</span><span class="s15">e</span><span class="s25"> </span>is the expected time gap, <span class="s13">∆</span><span class="s14">v </span>is the velocity difference between the AV and the front vehicle (FV), and <span class="s14">b </span>is the comfortable deceleration.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark139" class="a">The reward function of DDQN has been adapted to improve driving safety and efficiency in </a><a href="#bookmark140" class="a">[107]. Specifically, a penalty is applied when the vehicle goes off-road or the time-to-collision (TTC) falls below a threshold </a><a href="#bookmark139" class="a">[108] . The reward for driving efficiency is formulated as [107]:</a></p><p class="s14" style="padding-left: 27pt;text-indent: 0pt;line-height: 9pt;text-align: center;">v<span class="s39">0</span></p><p class="s14" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">L <span class="p">represents the relative position the target lane, where </span>L <span class="s13">= 1 </span><span class="p">indicates that the vehicle has successfully reached the target lane. A collision results in a negative reward, and a larger difference between </span>v<span class="s39">0</span><span class="s30"> </span><span class="p">and </span>v<span class="s26">max</span><span class="s27"> </span><span class="p">also leads to a negative reward. Additionally, if the vehicle does not drive in the target lane, a penalty is applied.</span></p><p style="padding-left: 5pt;text-indent: 9pt;line-height: 12pt;text-align: justify;"><a href="#bookmark148" class="a">A multi reward-based DQN has been proposed to achieve safe, efficient, and unselfish driving in </a>[116]. Three rewards are combined: speed reward, limited lane-changing reward, and  overtaking  reward.  The  speed  reward  is  a  normalized</p><p style="text-indent: 0pt;text-align: left;"><span><img width="24" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_021.png"/></span></p><p class="s14" style="text-indent: 0pt;line-height: 1pt;text-align: right;">R <span class="s13">=</span></p><p class="s42" style="text-indent: 0pt;line-height: 10pt;text-align: right;">v<span class="s27">max</span></p><p style="text-indent: 0pt;line-height: 4pt;text-align: right;">(5)</p><p style="padding-left: 9pt;text-indent: 0pt;text-align: left;">reward based on the current speed relative to the minimum</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">where <span class="s14">v</span><span class="s26">max </span><span class="s27"> </span>is the maximum velocity, and <span class="s14">v</span><span class="s39">0</span><span class="s30"> </span>is the current velocity. This reward function helps maintain a relatively high</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark148" class="a">and maximum speed limits [116]:</a></p><p class="s14" style="padding-top: 3pt;padding-left: 112pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s13">(</span>v<span class="s39">0</span><span class="s30"> </span><span class="s28">− </span>v<span class="s26">min</span><span class="s13">) </span><span class="s28">∗ </span>r<span class="s15">v</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 2pt;text-align: left;">(7)</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">driving velocity, thus increasing driving efficiency. Moreover,</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">driving safety and altruism have been achieved using a level-</p><p style="text-indent: 0pt;text-align: left;"><span><img width="86" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_022.png"/></span></p><p class="s14" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">R<span class="s15">v</span><span class="s25"> </span><span class="s13">=</span></p><p class="s42" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">v<span class="s27">max</span></p><p class="s53" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">—<span class="s28"> </span><span class="s14">v</span><span class="s27">min</span></p><p class="s14" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">k <a href="#bookmark141" class="a">game-based DQN in </a><span class="p">[109]. The level-</span>k <a href="#bookmark142" class="a">game models the reasoning interaction between AVs and HDVs, promoting unselfish decision-making. A crash penalty is implemented in the DQN to prevent frequent collisions between AVs and HDVs. Additionally, unselfishness and training efficiency have been considered in </a><span class="p">[110]. Unselfishness is achieved through a</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">where <span class="s14">R</span><span class="s15">v</span><span class="s25"> </span>represents the reward for speed, encouraging higher</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">speeds within safe limits. <span class="s14">v</span><span class="s26">min</span><span class="s27"> </span>is the minimum speed of the agent vehicle, and <span class="s14">r</span><span class="s15">v</span><span class="s25"> </span>is the base reward for speed. The limited lane-changing reward function is designed to minimize the number of lane changes, promoting safer driving and reducing the disturbance to surrounding vehicles:</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">cooperative multi-goal credit function-based policy gradient</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">(PG). This adapted PG accounts for the goals of all vehicles,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">R<span class="s15">l</span><span class="s25"> </span><span class="s13">=</span></p><p class="s14" style="text-indent: 0pt;line-height: 18pt;text-align: left;"><span class="s54">(</span><span class="s28">−</span>r<span class="s15">l</span>,  <span class="p">if the agent vehicle changes lanes;</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">(8)</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">optimizing overall performance during training. Training ef- ficiency is improved by a multi-agent reinforcement learning (MARL) curriculum, which reduces the number of trainable parameters and lowers computational costs.</p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Unselfishness  and  driving  efficiency  on  highways  are</p><p class="s13" style="padding-left: 62pt;text-indent: 0pt;line-height: 10pt;text-align: left;">0<i>,      </i><span class="p">otherwise.</span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">where <span class="s28">−</span><span class="s14">r</span><span class="s15">l</span><span class="s25"> </span>is the penalty value for a lane change. The overtaking reward function encourages the agent vehicle to overtake more vehicles, improving driving efficiency:</p><p class="s54" style="padding-left: 34pt;text-indent: 0pt;line-height: 12pt;text-align: left;">(<i>r , </i><span class="p">if the agent vehicle overtakes another vehicle;</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark143" class="a">achieved in </a>[111]. Unselfishness is promoted through MARL</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">by considering each vehicle’s state. Driving efficiency is en- hanced by a reward function that selects actions to increase the</p><p class="s14" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">R<span class="s15">o</span><span class="s25"> </span><span class="s13">=</span></p><p class="s25" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">o</p><p class="s13" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">0<i>,    </i><span class="p">otherwise.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">(9)</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark144" class="a">average velocity of all vehicles. Driving safety and driving ef- ficiency have been achieved using multi-objective approximate policy iteration (MO-API) in </a>[112]. Driving safety is ensured by monitoring collisions, while driving efficiency has been assessed by comparing the <span class="s14">v</span><span class="s39">0</span><span class="s30"> </span>with the <span class="s14">v</span><span class="s15">e</span><a href="#bookmark145" class="a">. In </a>[113], driving</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">where <span class="s14">r</span><span class="s15">o</span><span class="s25"> </span>is the reward value for overtaking.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark149" class="a">Interpretability, driving safety, and driving efficiency have been achieved in </a>[117]. Safety and efficiency are enhanced by penalizing frequent lane changes and tracking the desired velocity <span class="s14">v</span><span class="s15">d</span>, respectively. Interpretability is achieved through</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark149" class="a">a car-following process using a proportional-derivative (PD) controller with transparent mathematical formulation [117]:</a></p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;"><a href="#bookmark153" class="a">Training efficiency is achieved through the potential-based reward shaping function. The total reward function and reward shaping function are given by [121]:</a></p><p class="s42" style="text-indent: 0pt;line-height: 3pt;text-align: right;">d<span class="s27">des</span><span class="s25">,i </span><span class="s43">=</span><span class="s13"> </span><span class="s14">α</span><span class="s55">j</span></p><p class="s25" style="padding-left: 4pt;text-indent: 0pt;line-height: 3pt;text-align: left;">j<span class="s30">+1</span></p><p class="s56" style="padding-left: 139pt;text-indent: 0pt;line-height: 2pt;text-align: left;">i<span class="s25"> </span><span class="s14">v</span>l                                    <span class="s25"> </span><span class="p">(10)</span></p><p class="s14" style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;line-height: 12pt;text-align: center;">a<span class="s26">cf</span><span class="s25">,i </span><span class="s13">= </span>K<span class="s15">p</span><span class="s13">(</span>x<span class="s57">j</span><span class="s30">+1 </span><span class="s28">− </span>x<span class="s57">j</span><span class="s25"> </span><span class="s13">) + </span>K<span class="s15">d</span><span class="s13">(</span>v<span class="s57">j</span><span class="s30">+1 </span><span class="s28">− </span>v<span class="s57">j</span><span class="s25"> </span><span class="s13">)        </span><span class="p">(11)</span></p><p class="s25" style="padding-left: 53pt;text-indent: 0pt;line-height: 4pt;text-align: center;">l          i             l          i</p><p class="s14" style="padding-left: 47pt;text-indent: 0pt;line-height: 11pt;text-align: center;">R<span class="s32">′</span><span class="s47"> </span><span class="s13">= </span>R<span class="s13">(</span>s, a, s<span class="s32">′</span><span class="s13">) + </span>βF <span class="s13">(</span>s, a, s<span class="s32">′</span><span class="s13">)               </span><span class="p">(16)</span></p><p class="s14" style="padding-top: 4pt;padding-left: 54pt;text-indent: 0pt;line-height: 12pt;text-align: center;">F <span class="s13">(</span>s, a, s<span class="s32">′</span><span class="s13">) = </span>γϕ<span class="s13">(</span>s<span class="s32">′</span><span class="s13">) </span><span class="s28">− </span>ϕ<span class="s13">(</span>s<span class="s13">)                 </span><span class="p">(17)</span></p><p class="s25" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;line-height: 7pt;text-align: left;">l</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="padding-left: 5pt;text-indent: 0pt;line-height: 82%;text-align: justify;"><span class="p">where </span>d<span class="s26">des</span><span class="s25">,i </span><span class="p">is the desired following distance for the </span>i<span class="p">-th vehicle, </span>α<span class="s57">j</span><span class="s25"> </span><span class="p">is a sensitivity parameter with random values from </span><span class="s28">N </span><span class="s13">(1</span>.<span class="s13">3</span>, <span class="s13">0</span>.<span class="s13">02)</span><span class="p">, </span>v<span class="s57">j</span><span class="s30">+1 </span><span class="p">is the speed of the leading vehicle in the</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">where <span class="s14">R</span><span class="s49">′</span></p><p class="s14" style="padding-top: 6pt;padding-left: 1pt;text-indent: 10pt;line-height: 16pt;text-align: left;">F <span class="s13">(</span>s, a, s<span class="s32">′</span>, t, t<span class="s32">′</span><span class="s13">) = </span>γϕ<span class="s13">(</span>s<span class="s32">′</span>, t<span class="s32">′</span><span class="s13">) </span><span class="s28">− </span>ϕ<span class="s13">(</span>s, t<span class="s13">)           </span><span class="p">(18) is the new reward criterion, </span>R<span class="s13">(</span>s, a, s<span class="s49">′</span><span class="s13">) </span><span class="p">is the original</span></p><p class="s14" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s13">(</span>j <span class="s13">+ 1)</span><span class="p">-th lane, </span>a<span class="s26">cf</span><span class="s25">,i </span><span class="p">is the acceleration command, </span>K<span class="s15">p</span><span class="s25"> </span><span class="p">and </span>K<span class="s15">d</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">are the proportional and derivative gains, and <span class="s14">x</span><span class="s57">j</span><span class="s30">+1 </span>and <span class="s14">x</span><span class="s57">j</span><span class="s25"> </span>are</p><p class="s14" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="p">reward function, </span>β <span class="p">is a weighting factor, </span>F <span class="s13">(</span>s, a, s<span class="s49">′</span><span class="s13">) </span><span class="p">is the</span></p><p class="s47" style="text-indent: 0pt;line-height: 3pt;text-align: right;">′</p><p style="padding-left: 197pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="s25">l            i         </span>potential-based reward shaping function, <span class="s14">s </span>and <span class="s14">s</span></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">are  the</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">the positions of the leading and <span class="s14">i</span>-th vehicles, respectively.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark150" class="a">In </a><a href="#bookmark151" class="a">[118], driving safety, efficiency, and interpretability have also been combined. Safety and efficiency are achieved by penalizing collisions and rewarding high average velocity. Interpretability is enhanced by using the risk potential field (RPF), which models and visualizes risks around surrounding vehicles. In </a>[119], driving safety and interpretability have been achieved in adaptive cruise control (ACC), which maintains <span class="s14">d</span><span class="s15">s</span><span class="s25"> </span><a href="#bookmark152" class="a">between vehicles and provides interpretable mathematical formulations. Driving efficiency is achieved by rewarding each high-speed state. Finally, driving safety,  driving  efficiency, and training efficiency have been achieved in </a>[120]. Safety is ensured through a collision penalty, and efficiency is rewarded based on the velocity difference between <span class="s14">v</span><span class="s39">0</span><span class="s30"> </span>and <span class="s14">v</span><span class="s26">min</span>. Training efficiency is improved by using a long short-term memory (LSTM) network-assisted DDQN.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: left;"><p class="s19" style="display: inline;"><a name="bookmark24">Four-factor Methods for Highway Driving</a></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark153" class="a">Moreover, four of the five considered factors have been included in some studies. Driving safety, driving efficiency, training efficiency, and interpretability have been considered in </a>[121]. Driving safety and driving efficiency are achieved by a reward function that maintains a <span class="s14">d</span><span class="s15">s</span><span class="s25"> </span>from the leading vehicle while tracking the <span class="s14">v</span><span class="s15">d</span><a href="#bookmark153" class="a">. Interpretability is ensured through safety-based driving rules [121]:</a></p><p class="s14" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;"><span class="p">current and next state, respectively, </span>a <span class="p">is  the  action  taken, and </span>γ <span class="p">is the discount factor. </span>ϕ<span class="s13">(</span>s<span class="s13">) </span><span class="p">is the potential function mapping the state to a real number,  and  </span>t <span class="p">and  </span>t<span class="s49">′ </span><span class="s47"> </span><span class="p">are  the time corresponding to </span>s <span class="p">and </span>s<span class="s49">′</span><span class="p">, respectively. (16) combines the original reward function with an additional shaping term.</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">(17) defines the shaping function as the difference between the discounted potential of the next state and the current state. (18) extends (17) by including time as a parameter and therefore allows for dynamic potential functions.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark154" class="a">Driving safety, driving efficiency, unselfishness, and training efficiency on highways have been addressed in </a><a href="#bookmark155" class="a">[122]. Driving safety and efficiency are considered in the reward function of the DQN. Unselfishness is achieved through a joint policy update, accounting for the profits of multiple vehicles. Training efficiency is enhanced by reusing the experiences of single agents within a MARL framework. In </a><a href="#bookmark156" class="a">[123], driving safety, efficiency, unselfishness, and training efficiency on highways have been explored. Safety and efficiency are ensured by assessing the remaining reaction time during emergencies and selecting the proper lane-changing point, respectively. Un- selfishness is achieved using MARL for cooperative highway driving, while training efficiency is improved with a dynamic coordinate graph (DCG) that enhances cooperative efficiency. In </a>[124], safety, efficiency, unselfishness, and training effi- ciency have been considered. Safety is ensured by applying</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: justify;">penalties both for collisions and for deviating from the road.</p><p class="s14" style="padding-top: 6pt;padding-left: 61pt;text-indent: 0pt;text-align: left;">t<span class="s15">f</span><span class="s34">min </span><span class="s35"> </span><span class="s13">= inf</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-left: 61pt;text-indent: 0pt;text-align: left;">t<span class="s15">b</span><span class="s34">min </span><span class="s35"> </span><span class="s13">= inf</span></p><p class="s13" style="text-indent: 0pt;line-height: 4pt;text-align: left;">(</p><p class="s14" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">t <span class="s13">: </span>t &gt;</p><p class="s13" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">(</p><p class="s14" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">t <span class="s13">: </span>t &gt;</p><p class="s14" style="padding-left: 18pt;text-indent: -16pt;line-height: 13pt;text-align: left;"><span class="s58">2(</span>v <span class="s28">− </span>v<span class="s37">f</span><span class="s35">target </span><span class="s58">)</span><span class="s13"> </span><span class="s59">1</span></p><p style="padding-left: 1pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="69" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_023.png"/></span></p><p class="s36" style="padding-left: 18pt;text-indent: 0pt;line-height: 11pt;text-align: left;">a<span class="s37">d</span><span class="s35">max</span></p><p class="s14" style="padding-left: 17pt;text-indent: -15pt;line-height: 19pt;text-align: left;"><span class="s58">2(</span>v<span class="s37">b</span><span class="s35">target </span><span class="s38">−</span><span class="s28"> </span>v<span class="s13">) </span><span class="s59">1</span></p><p style="padding-left: 1pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="68" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_024.png"/></span></p><p class="s36" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">a<span class="s37">d</span><span class="s35">max</span></p><p style="padding-top: 6pt;text-indent: 0pt;text-align: right;">(12)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;text-indent: 0pt;text-align: right;">(13)</p><p style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">Efficiency is achieved by rewarding each state that overtakes other vehicles. Unselfishness is promoted through MARL to coordinate driving. Training efficiency is enhanced by employ- ing a parameter-sharing mechanism, which stores experience of each agent to reinforce common scenario understanding.</p><p class="s35" style="text-indent: 0pt;line-height: 5pt;text-align: left;">min</p><p style="text-indent: 0pt;text-align: left;"/><p class="s42" style="padding-top: 4pt;padding-left: 46pt;text-indent: 0pt;text-align: left;">d<span class="s27">target</span></p><p class="s13" style="padding-top: 4pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">= min</p><p style="text-indent: 0pt;text-align: left;"><span><img width="59" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_025.png"/></span></p><p class="s14" style="text-indent: 0pt;line-height: 14pt;text-align: center;"><span class="s60">(</span><span class="s13">(</span>v <span class="s28">− </span>v<span class="s15">f</span><span class="s25"> </span><span class="s13">)</span>t<span class="s15">f</span><span class="s25"> </span><span class="s61">,</span></p><p class="s13" style="padding-left: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">2</p><p style="text-indent: 0pt;text-align: left;"><span><img width="56" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_026.png"/></span></p><p class="s14" style="padding-left: 19pt;text-indent: -18pt;line-height: 12pt;text-align: left;"><span class="s13">(</span>v<span class="s15">b</span><span class="s25"> </span><span class="s28">− </span>v<span class="s13">)</span>t<span class="s15">b</span><span class="s25"> </span><span class="s60">1</span></p><p class="s13" style="text-indent: 0pt;line-height: 11pt;text-align: center;">2</p><p style="padding-top: 4pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">(14)</p><p style="padding-left: 9pt;text-indent: 9pt;line-height: 12pt;text-align: justify;"><a href="#bookmark157" class="a">In </a>[125], safety, efficiency, unselfishness, and interpretabil- ity have been considered. Safety, efficiency, and unselfish- ness  are  improved  through  rewards  for  collisions,  velocity</p><p class="s14" style="padding-left: 29pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><span class="s58">∆</span>d<span class="s41">target</span><span class="s27"> </span><span class="s58">=</span><span class="s13"> min </span><span class="s54">{</span><span class="s38">|</span>x<span class="s41">AV</span><span class="s27"> </span><span class="s38">−</span><span class="s28"> </span>x<span class="s37">f</span><span class="s35">target </span><span class="s38">|</span>, <span class="s28">|</span>x<span class="s41">AV</span><span class="s27"> </span><span class="s38">−</span><span class="s28"> </span>x<span class="s37">b</span><span class="s35">target </span><span class="s38">|</span><span class="s54">�       </span><span class="s13"> </span><span class="s40">(15)</span></p><p style="padding-left: 9pt;text-indent: 0pt;text-align: left;">ratio between <span class="s14">v</span><span class="s39">0</span>, <span class="s14">v</span></p><p class="s27" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">max</p><p style="text-indent: 0pt;text-align: left;">, and <span class="s14">v</span></p><p class="s27" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">min</p><p style="text-indent: 0pt;text-align: left;">, and limiting unnecessary</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">where <span class="s14">t</span><span class="s15">f</span><span class="s34">min</span><span class="s35"> </span>and <span class="s14">t</span><span class="s15">b</span><span class="s34">min</span><span class="s35"> </span>are the minimum safe time intervals between the AV and the vehicles in front and behind in the target lane, respectively. <span class="s14">v </span>is the speed of the AV; <span class="s14">v</span><span class="s15">f</span><span class="s34">target</span><span class="s35"> </span>and <span class="s14">v</span><span class="s15">b</span><span class="s34">target</span><span class="s35"> </span>are the speeds of the front and behind vehicles in the target lane, respectively. <span class="s14">a</span><span class="s15">d</span><span class="s34">max  </span><span class="s35"> </span>is the maximum deceleration.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark158" class="a">lane changes, respectively. Interpretability is enhanced by integrating an autonomous emergency braking system, pro- moting safer decision-making. In </a>[126], safety, efficiency, interpretability, and training efficiency have been addressed. Safety and efficiency are enhanced by adding a safety layer</p><p class="s42" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">d<span class="s27">target</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s35" style="text-indent: 0pt;line-height: 6pt;text-align: left;">min</p><p style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">is the minimum distance between the AV and the FV</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">and incorporating the ratio between longitudinal speed, <span class="s14">v</span><span class="s26">max</span>,</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">in the target lane, and <span class="s13">∆</span><span class="s14">d</span><span class="s26">target</span><span class="s27">  </span>is the actual distance between</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">the AV and the nearest vehicle in the target lane. <span class="s14">x</span><span class="s26">AV</span>, <span class="s14">x</span><span class="s15">f</span><span class="s34">target</span><span class="s35"> </span>, and <span class="s14">x</span><span class="s15">b</span><span class="s34">target</span><span class="s35"> </span>represent the horizontal coordinates of the AV, the front target vehicle, and the vehicle behind in the target lane, respectively. By implementing these safety rules, the decision- making of the AV becomes more transparent and interpretable.</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">and <span class="s14">v</span><span class="s26">min</span>. Interpretability is improved by using a support vector</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">machine (SVM), which provides interpretable safe decision boundaries. Training efficiency is boosted through an external space attention mechanism that pays attention to the crucial areas of surrounding environment.</p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark159" class="a">In </a>[127], safety, efficiency, unselfishness, a</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark160" class="a">nd training efficiency have been tackled. Safety, efficiency, and unselfishness are achieved through rewards for colli- sions, velocity ratios, and MARL, while training efficiency is improved using a distributional DQN with multi-type input data. Finally, in </a>[128], safety, efficiency, unselfishness, and interpretability have been considered. Safety, efficiency, and unselfishness are enhanced through rewards for collisions, target velocity differences, and unnecessary lane changes, respectively. Interpretability is achieved through rule-based constraints, such as preventing lane changes with short lateral distances to lead vehicles.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s19" style="display: inline;"><a name="bookmark25">Five-factor Methods for Highway Driving</a></p></li></ol></ol><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark161" class="a">Additionally, all the five factors are addressed in some stud- ies, such as </a>[129]. Driving safety and efficiency, and unselfish- ness are achieved by reducing collisions, increasing speed, and minimizing lane-change frequency through rewards. Training efficiency is improved through a convolutional neural network- based LSTM. Interpretability is enhanced by using spatio- temporal image representations for HDVs, which increase the interpretability of the inputs. The DRL-based decision making in highway driving based on DDTUI is summarized in Table I.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 32pt;text-indent: 1pt;text-align: left;"><p style="display: inline;"><a name="bookmark26">D</a><span class="s8">EEP </span>R<span class="s8">EINFORCEMENT </span>L<span class="s8">EARNING</span>-<span class="s8">BASED DECISION</span>-<span class="s8">MAKING IN </span>O<span class="s8">N</span>-<span class="s8">RAMPING </span>M<span class="s8">ERGING</span></p><p class="s19" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark27">A. Single-factor Methods for On-ramping Merging</a></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark162" class="a">Driving efficiency has been considered using Q-learning in [130]. The remaining time of AV on the ramp lane is reduced by optimizing the reward function, thus promoting fast lane-changing to the main lane. The reward function is formulated as [130]:</a></p><p class="s13" style="padding-top: 6pt;padding-left: 61pt;text-indent: 0pt;text-align: left;"><i>r</i><span class="s15">t</span><span class="s25"> </span>= <i>µv</i>¯<span class="s15">t</span><span class="s25"> </span>+ <i>ωq</i>¯<span class="s15">t</span><i>,   µ &gt; </i>0<i>,   ω &lt; </i>0             <span class="p">(19)</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Interpretability is achieved by using DDPG to tune a traditional controller’s parameters, keeping the traditional controller as the main system to ensure transparency. Driving efficiency is enhanced by reducing the error state, which reflects the gap between actual and critical traffic density. A smaller error state leads to higher traffic flow and average speed.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l15"><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark29">Three-factor Methods for On-ramping Merging</a></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark169" class="a">Driving efficiency, interpretability, and training efficiency have been addressed in </a><a href="#bookmark170" class="a">[137]. Driving efficiency is achieved through a reward using the difference between the start and end time of each trip. Training efficiency is improved by a teacher- student model to train the decision-making system, where the traditional control method acts as the teacher guiding the DQN student. Similarly, driving efficiency, interpretability, and un- selfishness have been improved in </a><a href="#bookmark168" class="a">[138]. Driving efficiency is achieved by a reward that compares the average speed between two consecutive time. Unselfishness and interpretability are achieved by combining ramp metering (RM) with Q-learning. RM optimizes average vehicle speed and is algorithmically transparent. Driving safety, efficiency, and unselfishness have been addressed in </a>[136]. Driving safety is achieved through a penalty for small relative distances, driving efficiency is en- hanced by minimizing the relative distance while maintaining at least the safe distance, and unselfishness is achieved using MARL to optimize general driving performance.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark30">Four-factor Methods for On-ramping Merging</a></p></li></ol><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark171" class="a">Driving efficiency, training efficiency, unselfishness, and interpretability have been improved in </a><a href="#bookmark172" class="a">[139], where driving and training efficiency is enhanced by DDPG-assisted RM and variable speed limit (VSL) control. Interpretability and unselfishness are improved through RM and VSL, which are algorithmically transparent. Driving safety, efficiency, training efficiency, and interpretability have been achieved in [140],</a></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">where  <span class="s14">r</span><span class="s15">t </span><span class="s25"> </span>represents  the  reward  after  taking  action  <span class="s14">a</span><span class="s15">t</span>;</p><p class="s14" style="padding-left: 3pt;text-indent: 0pt;line-height: 9pt;text-align: left;">v<span class="s13">¯</span><span class="s15">t</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">where safety and interpretability are enhanced by combining</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">denotes the average speed in the merging area during time</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">step <span class="s14">t</span>; <span class="s14">q</span><span class="s13">¯</span><span class="s15">t</span><span class="s25"> </span>indicates the average queue length at the on-ramp during time steps <span class="s14">t </span>and <span class="s14">t </span><span class="s13">+ 1</span>; <span class="s14">µ </span>is a positive weight assigned to the speed reward, and <span class="s14">ω </span><a href="#bookmark163" class="a">is a negative weight for the queue length reward. These rewards help balance the trade- off between enhancing vehicle mobility on the mainline and reducing delays at the on-ramp. Driving efficiency has also been improved in </a>[131] by reducing the total travel time reward (<span class="s14">R</span><span class="s26">TTT</span><a href="#bookmark164" class="a">), represented by the summation of the total number of vehicles at each time step. Driving safety has been achieved through a safety factor in </a><a href="#bookmark165" class="a">[132]. The safety factor is a negative reward when the relative distances between AV and HDV are small. Driving safety has been achieved in </a>[133], by giving rewards for each state having a <span class="s14">d</span><span class="s15">s</span><span class="s25"> </span>and penalties for collisions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s19" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark28">B. Dual-factor Methods for On-ramping Merging</a></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark166" class="a">Driving efficiency and unselfishness have been considered in </a><a href="#bookmark167" class="a">[134]. Driving efficiency is achieved by using the average velocity of AVs as part of the reward, and unselfishness is achieved using MARL to maximize general profits. Inter- pretability and driving efficiency have been addressed in [135].</a></p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark173" class="a">APF, which quantifies and visualizes risk areas and  pro- vides interpretable input. Driving and training efficiency are achieved by combining MPC with DDQN, which outperforms single MPC or DDQN methods. Similarly, driving safety, efficiency, training efficiency, and unselfishness have been ad- dressed in </a><a href="#bookmark174" class="a">[141], where safety and efficiency are promoted by penalties for collisions and stop maneuvers. Training efficiency is improved by integrating the driver’s intention model (DIM) with DDPG, while unselfishness is achieved by considering HDVs’ various cooperation intentions. In </a>[142], driving safety, efficiency, training efficiency, and interpretability have been achieved by applying the safety, efficiency rewards, and IDM respectively, with independant PPO (IPPO) used for improved training efficiency compared to baseline algorithms.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s19" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark31">E. Five-factor Methods for On-ramping Merging</a></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark175" class="a">In </a>[143], driving safety and efficiency have been achieved through collision and stable speed assessment rewards, re- spectively. Training efficiency is improved by using a safety supervisor, filtering detectable collision cases. Interpretabil- ity  is  enhanced  through  rule-based  safety  constraints,  and</p><p style="padding-top: 2pt;padding-left: 104pt;text-indent: 0pt;text-align: center;">T<span class="s8">ABLE </span>I</p><p style="padding-left: 104pt;text-indent: 0pt;text-align: center;">E<span class="s8">VALUATION OF THE </span>DRL-<span class="s8">BASED DECISION MAKING IN HIGHWAY DRIVING</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.964pt" cellspacing="0"><tr style="height:16pt"><td style="width:52pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Reference</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;text-indent: 0pt;text-align: center;">Safety</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 27pt;text-indent: 0pt;text-align: left;">Efficiency</p></td><td style="width:93pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Training Efficiency</p></td><td style="width:93pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">Unselfishness</p></td><td style="width:87pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Interpretability</p></td></tr><tr style="height:16pt"><td style="width:52pt;border-top-style:solid;border-top-width:1pt"><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark127" class="s63">[95]</a></p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">Safety modules</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark128" class="s63">[96]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Overtaking reward</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark129" class="s63">[97]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">NMPC constraints</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" class="s63">[101]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Hard constraints</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark134" class="s63">[102]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Local interactions</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark135" class="s63">[103]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Imitation learning</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark136" class="s63">[104]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Attention module</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark138" class="s63">[106]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">IDM integration</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">IDM integration</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark139" class="s63">[107]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">TTC threshold</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Velocity reward</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark141" class="s63">[109]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Crash penalty</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">Level-k game</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark142" class="s63">[110]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">MARL curriculum</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Cooperative function</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark143" class="s63">[111]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Average velocity</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">MARL</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark144" class="s63">[112]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Collision monitoring</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Velocity comparison</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark145" class="s63">[113]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Velocity reward</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Lane change penalty</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark146" class="s63">[114]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">Rule-based</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Attention mechanism</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark147" class="s63">[115]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">IDM &amp; collision</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Velocity difference</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">IDM integration</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark148" class="s63">[116]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Speed-limit reward</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Overtaking reward</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Lane-change limit</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark149" class="s63">[117]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Lane change penalty</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Velocity tracking</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">PD controller</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark150" class="s63">[118]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Reward function</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Reward function</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Risk potential field</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark151" class="s63">[119]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">Adaptive cruise</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">High-speed reward</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">ACC formulations</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark152" class="s63">[120]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Collision penalty</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Velocity difference</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">LSTM-DDQN</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark153" class="s63">[121]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Safety rules</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Reward function</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">Reward shaping</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">Safety rules</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark154" class="s63">[122]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Reward function</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Reward function</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">MARL reuse</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Joint policy</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark155" class="s63">[123]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Reaction time</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Lane-changing point</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">DCG efficiency</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">MARL</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark156" class="s63">[124]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Collision penalties</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Overtaking reward</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Parameter sharing</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">MARL</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark157" class="s63">[125]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Collision rewards</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Velocity ratio</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Lane change limit</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Emergency braking</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark158" class="s63">[126]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">Safety layer</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Velocity ratio</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Attention mechanism</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">SVM boundaries</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark159" class="s63">[127]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Collision rewards</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Velocity ratio</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Distributional DQN</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">MARL</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td></tr><tr style="height:16pt"><td style="width:52pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark160" class="s63">[128]</a></p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Collision rewards</p></td><td style="width:92pt"><p class="s62" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Velocity difference</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">-</p></td><td style="width:93pt"><p class="s62" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Lane change penalty</p></td><td style="width:87pt"><p class="s62" style="padding-top: 1pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">Rule-based</p></td></tr><tr style="height:15pt"><td style="width:52pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark161" class="s63">[129]</a></p></td><td style="width:92pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Collision reduction</p></td><td style="width:92pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Speed increase</p></td><td style="width:93pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">CNN-LSTM</p></td><td style="width:93pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Lane change limit</p></td><td style="width:87pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Representations</p></td></tr></table><p class="s8" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">’-’ indicates that the corresponding factor was not explicitly addressed in the study.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark176" class="a">unselfishness is achieved using MARL to maximize general profits. Similarly, all factors have been addressed in </a><a href="#bookmark177" class="a">[144], where driving safety and efficiency are achieved via collision rewards and a velocity ratio, respectively. Training efficiency is enhanced by adversarial constraints, while  unselfishness and interpretability is enhanced through a transparent Nash- based game that considers HDV’s profits. Finally, in </a>[145], driving safety and interpretability have been achieved using the deceleration rate to avoid a crash (DRAC), which has a detailed mathematical formulation and is transparent. Driving efficiency is improved by using (5) as an efficiency reward. Unselfishness is addressed by considering the cooperation intentions of other vehicles, and training efficiency is improved using multi-state representations to enhance the agent’s learn-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">ing capabilities. The DRL-based decision making on on-ramp merging based on DDTUI is summarized in Table II.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 50pt;text-indent: -18pt;text-align: left;"><p style="display: inline;"><a name="bookmark32">D</a><span class="s8">EEP </span>R<span class="s8">EINFORCEMENT </span>L<span class="s8">EARNING</span>-<span class="s8">BASED DECISION</span>-<span class="s8">MAKING AT </span>R<span class="s8">OUNDABOUTS</span></p><ol id="l16"><li style="padding-top: 4pt;padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s19" style="display: inline;"><a name="bookmark33">Single-factor Methods for Roundabout Driving</a></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark178" class="a">Driving efficiency in roundabout driving has been im- proved using soft actor-critic (SAC) with higher peak rewards in </a><a href="#bookmark179" class="a">[146]. Training efficiency has been achieved through action repeat and asynchronous advantage in </a>[147]. Action repeat improves efficiency by allowing the agent to repeat the same action for several time steps, decreasing the frequency of making new decisions. Asynchronous advantage enables each</p><p style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">E<span class="s8">VALUATION OF THE </span>DRL-<span class="s8">BASED DECISION MAKING IN ON</span>-<span class="s8">RAMPING MERGING</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:6.865pt" cellspacing="0"><tr style="height:27pt"><td style="width:50pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">Ref.</p></td><td style="width:102pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Safety</p></td><td style="width:106pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Efficiency</p></td><td style="width:81pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 3pt;padding-left: 6pt;padding-right: 39pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Training Efficiency</p></td><td style="width:95pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Unselfishness</p></td><td style="width:78pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">Interpretability</p></td></tr><tr style="height:14pt"><td style="width:50pt;border-top-style:solid;border-top-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark162" class="s63">[130]</a></p></td><td style="width:102pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:106pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Reward</p></td><td style="width:81pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:95pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:78pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:102pt"/><td style="width:106pt"><p class="s62" style="padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: left;">function</p></td><td style="width:81pt"/><td style="width:95pt"/><td style="width:78pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark163" class="s63">[131]</a></p></td><td style="width:102pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:106pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Travel time re-</p></td><td style="width:81pt"><p class="s62" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:95pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:78pt"><p class="s62" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:102pt"/><td style="width:106pt"><p class="s62" style="padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: left;">ward</p></td><td style="width:81pt"/><td style="width:95pt"/><td style="width:78pt"/></tr><tr style="height:16pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark164" class="s63">[132]</a></p></td><td style="width:102pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Safety factor</p></td><td style="width:106pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:81pt"><p class="s62" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:95pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:78pt"><p class="s62" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark165" class="s63">[133]</a></p></td><td style="width:102pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Collision-free</p></td><td style="width:106pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:81pt"><p class="s62" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:95pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:78pt"><p class="s62" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:102pt"><p class="s62" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">driving</p></td><td style="width:106pt"/><td style="width:81pt"/><td style="width:95pt"/><td style="width:78pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark166" class="s63">[134]</a></p></td><td style="width:102pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:106pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Average veloc-</p></td><td style="width:81pt"><p class="s62" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:95pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">MARL</p></td><td style="width:78pt"><p class="s62" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:102pt"/><td style="width:106pt"><p class="s62" style="padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: left;">ity reward</p></td><td style="width:81pt"/><td style="width:95pt"/><td style="width:78pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark167" class="s63">[135]</a></p></td><td style="width:102pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:106pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Error  state  re-</p></td><td style="width:81pt"><p class="s62" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:95pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:78pt"><p class="s62" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">Traditional</p></td></tr><tr style="height:10pt"><td style="width:50pt"/><td style="width:102pt"/><td style="width:106pt"><p class="s62" style="padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: left;">duction</p></td><td style="width:81pt"/><td style="width:95pt"/><td style="width:78pt"><p class="s62" style="padding-left: 19pt;text-indent: 0pt;line-height: 9pt;text-align: left;">controller</p></td></tr><tr style="height:31pt"><td style="width:50pt"><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark168" class="s63">[136]</a></p></td><td style="width:102pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s62" style="padding-left: 24pt;text-indent: 0pt;text-align: left;">Distance penalty</p></td><td style="width:106pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s62" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">Distance minimization</p></td><td style="width:81pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s62" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:95pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s62" style="padding-left: 18pt;text-indent: 0pt;text-align: left;">MARL</p></td><td style="width:78pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s62" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:18pt"><td style="width:50pt"><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark169" class="s63">[137]</a></p></td><td style="width:102pt"><p class="s62" style="padding-top: 6pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:106pt"><p class="s62" style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Trip  time  dif-</p></td><td style="width:81pt"><p class="s62" style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Teacher-</p></td><td style="width:95pt"><p class="s62" style="padding-top: 6pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:78pt"><p class="s62" style="padding-top: 6pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">Traditional</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:102pt"/><td style="width:106pt"><p class="s62" style="padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: left;">ference</p></td><td style="width:81pt"><p class="s62" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">student model</p></td><td style="width:95pt"/><td style="width:78pt"><p class="s62" style="padding-left: 19pt;text-indent: 0pt;line-height: 9pt;text-align: left;">control</p></td></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark170" class="s63">[138]</a></p></td><td style="width:102pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:106pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Speed compari-</p></td><td style="width:81pt"><p class="s62" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:95pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Ramp metering</p></td><td style="width:78pt"><p class="s62" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">Ramp metering</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:102pt"/><td style="width:106pt"><p class="s62" style="padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: left;">son</p></td><td style="width:81pt"/><td style="width:95pt"/><td style="width:78pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark171" class="s63">[139]</a></p></td><td style="width:102pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:106pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">DDPG-assisted</p></td><td style="width:81pt"><p class="s62" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">DDPG</p></td><td style="width:95pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">RM and VSL</p></td><td style="width:78pt"><p class="s62" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">RM and VSL</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:102pt"/><td style="width:106pt"><p class="s62" style="padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: left;">RM</p></td><td style="width:81pt"/><td style="width:95pt"/><td style="width:78pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark172" class="s63">[140]</a></p></td><td style="width:102pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">APF</p></td><td style="width:106pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">MPC         with</p></td><td style="width:81pt"><p class="s62" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">MPC         with</p></td><td style="width:95pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:78pt"><p class="s62" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">APF</p></td></tr><tr style="height:15pt"><td style="width:50pt"/><td style="width:102pt"/><td style="width:106pt"><p class="s62" style="padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: left;">DDQN</p></td><td style="width:81pt"><p class="s62" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">DDQN</p></td><td style="width:95pt"/><td style="width:78pt"/></tr></table><p style="padding-left: 12pt;text-indent: 0pt;text-align: left;"><a href="#bookmark173" class="s64">[141]</a></p><p class="s12" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Collision penalty</p><p class="s12" style="padding-left: 12pt;text-indent: 0pt;text-align: left;">Stop maneuver</p><p class="s12" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">DIM with DDPG</p><p class="s12" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">HDV</p><p class="s12" style="padding-left: 43pt;text-indent: 0pt;line-height: 9pt;text-align: center;">-</p><p class="s12" style="padding-left: 176pt;text-indent: 0pt;line-height: 9pt;text-align: left;">penalty                                                                        intentions</p><p class="s12" style="padding-top: 6pt;padding-left: 176pt;text-indent: -163pt;line-height: 10pt;text-align: left;">[142]                     Safety reward                   Efficiency reward</p><p class="s12" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">IPPO                                 -                                        IDM</p><p style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: left;"><a href="#bookmark175" class="s64">[143]</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 12pt;text-indent: 0pt;text-align: left;">Crash evaluation</p><p class="s12" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Stable speed                     Safety</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 12pt;text-indent: 0pt;text-align: left;">MARL</p><p class="s12" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Rule-based</p><p class="s12" style="padding-left: 176pt;text-indent: 0pt;line-height: 9pt;text-align: left;">assessment                        supervisor                                                                   constraints</p><p class="s12" style="padding-top: 6pt;padding-left: 81pt;text-indent: -68pt;line-height: 10pt;text-align: left;">[144]                     Collision rewards</p><p class="s12" style="padding-top: 6pt;padding-left: 107pt;text-indent: -94pt;line-height: 10pt;text-align: left;">Velocity ratio                   Adversarial constraints</p><p class="s12" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Nash-based game</p><p class="s12" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;line-height: 10pt;text-align: left;">transparent game process</p><p class="s12" style="padding-top: 5pt;padding-left: 176pt;text-indent: -163pt;line-height: 10pt;text-align: left;">[145]                     DRAC                              Velocity    ratio reward</p><p class="s12" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Multi-state rep.                Vehicle coop.                   DRAC</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 9pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="676" height="1" alt="image" src="2025-Evaluating Scenario-based Decision-making for nteractive Autonomous Driving Using Rational Criteria A Survey/Image_027.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark180" class="a">agent to share its interaction experience with others. Training efficiency has been further improved by embedding the opera- tional design domain (ODD) into DQN in </a>[148]. ODD guides the training to more targeted scenarios, reducing unnecessary exploration and accelerating convergence.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark34">Dual-factor Methods for Roundabout Driving</a></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: right;"><a href="#bookmark181" class="a">Driving  efficiency  and  training  efficiency  are  improved using the Conditional Representation Model (CRM) in </a><a href="#bookmark182" class="a">[149], which helps the agent better understand safety by defining each  state  as  safe  or  unsafe  state.  Training  efficiency  and interpretability have been improved by leveraging labeled data from domain experts as guidance in </a><a href="#bookmark183" class="a">[150]. Driving safety and driving efficiency have been enhanced in </a>[151] by incorporat- ing <span class="s14">v</span><span class="s15">d</span><span class="s25"> </span><a href="#bookmark184" class="a">and allowable relative distance into the reward function. Training efficiency and interpretability have been improved in </a>[152]. Training efficiency is achieved through optimization-</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark185" class="a">embedded DRL for adaptive decision-making, and inter- pretability is enhanced by transparent model-based optimiza- tion. Driving safety  and  unselfishness  have  been  achieved in </a><a href="#bookmark186" class="a">[153], with safety ensured by penalizing collisions and ramping off the road, and unselfishness promoted by using MARL to maximize collective benefits. Driving safety and interpretability have been achieved in </a>[154], with safety main- tained through penalties for collisions with HDVs and walls. Interpretability is supported by gradual training mode, similar to human learning, where the system starts with sparse traffic and progresses to dense traffic later.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 7pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark35">Three-factor Methods for Roundabout Driving</a></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark187" class="a">Driving safety, driving efficiency, and training efficiency have been improved in </a>[155]. Driving safety and driving efficiency are promoted through safety and efficiency rewards,</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark188" class="a">respectively. Training efficiency is enhanced via trust region policy optimization (TPRO), which converges faster than PPO and DDPG. In </a><a href="#bookmark189" class="a">[156], driving safety and driving efficiency have been achieved by rewards for non-collision lane-changing and the difference between initial and target velocities, respec- tively. Training efficiency is improved by embedding LSTM into the actor-critic network. Training efficiency, driving safety and efficiency have been enhanced in </a>[157]. Training effi- ciency is improved by normalizing the initial reward for faster convergence. Driving efficiency and driving safety benefit from multiple environments where agents are trained simultane- ously, achieving higher success rates and fewer crashes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 7pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark36">Four-factor Methods for Roundabout Driving</a></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark190" class="a">Driving safety, driving efficiency, training efficiency, and unselfishness have been  addressed  in  </a>[158],  where  safety is maintained using <span class="s14">d</span><span class="s15">s</span><a href="#bookmark95" class="a">, and driving efficiency is enhanced by the ratio of initial to target velocity. Training efficiency is improved through a synthetic representation  mechanism that enhances agents’ understanding, and unselfishness is promoted using MARL to maximize joint benefits. Driving safety, driving efficiency, interpretability, and training effi- ciency have been addressed in </a><a href="#bookmark191" class="a">[58], where safety is ensured via crash penalties and efficiency via high-speed rewards. Interpretability is maintained using the IDM for safe, trans- parent algorithmic-following. Training efficiency is improved through an interval prediction model to precompute feasible paths, reducing training computation. Driving safety, driving efficiency, training efficiency, and interpretability have been enhanced in </a>[159]. Safety and efficiency are promoted through penalties for collisions and vehicle-stop maneuvers, respec- tively. Training efficiency is increased by integrating DDPG, DQN, and NMPC. Interpretability is enhanced via the NMPC.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 7pt;padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark37">Five-factor Methods for Roundabout Driving</a></p></li></ol><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark131" class="a">All five factors have been considered in </a>[99], where driving safety and interpretability are ensured by a rule-based action inspector. Driving efficiency is enhanced via high-speed re- wards. Training efficiency is achieved through a Kolmogorov- Arnold network-enhanced DQN. Unselfishness is promoted through rule-based route planning that considers the varying distributions of HDVs on the roundabout. The DRL-based de- cision making in roundabouts based on DDTUI is summarized in Table III.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 15pt;text-indent: 14pt;text-align: left;"><p style="display: inline;"><a name="bookmark38">D</a><span class="s8">EEP </span>R<span class="s8">EINFORCEMENT </span>L<span class="s8">EARNING</span>-<span class="s8">BASED DECISION</span>-<span class="s8">MAKING AT </span>U<span class="s8">NSIGNALIZED </span>I<span class="s8">NTERSECTIONS</span></p><ol id="l17"><li style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><p class="s19" style="display: inline;"><a name="bookmark39">Single-factor Methods for Intersection Driving</a></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Traffic efficiency has been improved by using the difference between <span class="s14">v</span><span class="s39">0</span><span class="s30"> </span>and <span class="s14">v</span><span class="s15">d</span><span class="s25"> </span><a href="#bookmark192" class="a">as a reward in </a><a href="#bookmark193" class="a">[160]. Additionally, a penalty is applied when the velocity drops below a threshold, further boosting traffic efficiency. In </a>[161], driving efficiency has been achieved by applying a constant penalty as long as the AV has not reached the target exits.</p></li><li style="padding-top: 2pt;padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s19" style="display: inline;"><a name="bookmark40">Dual-factor Methods for Unsignalized Intersection Driving</a></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark194" class="a">Both driving and training efficiency have been improved in </a><a href="#bookmark195" class="a">[162], where the driving efficiency is enhanced by using total waiting time (TWT) as part of the reward. Training efficiency is increased by employing a background removal ResNet as the Q-network, resulting in lower TWT than base- line algorithms. In </a>[163], driving efficiency and interpretability have been enhanced. The driving efficiency is improved by using the difference between the <span class="s14">v</span><span class="s15">d</span><span class="s25"> </span>and <span class="s14">v</span><span class="s39">0</span><span class="s30"> </span><a href="#bookmark196" class="a">as part of the reward, while the interpretability is achieved through the use of IDM for safe and transparent vehicle following. Similarly, in </a><a href="#bookmark197" class="a">[164], both driving efficiency and interpretability have been improved. The former is enhanced by incorporating a velocity- based reward, and the latter is enhanced by applying a safety- based rule policy. In </a><a href="#bookmark198" class="a">[165], driving efficiency is increased by using the safe distance as a reward and the risky distance as a penalty, resulting in higher success rates. Interpretability is achieved using a model-based transparent method combined with twin delayed deep deterministic policy gradient (TD3). In </a>[166], driving efficiency is enhanced by the ratio of <span class="s14">v</span><span class="s39">0</span><span class="s30"> </span>to <span class="s14">v</span><span class="s26">max</span><span class="s27"> </span>as part of the reward, and interpretability is improved by gridding the coordination zone into different granularities, converting risky areas into a matrix format.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark199" class="a">Both driving efficiency and training efficiency have been improved in </a><a href="#bookmark200" class="a">[167]. Driving efficiency is achieved by rewarding goal attainment, and training efficiency is increased by using DQN with common and specific sub-tasks. The common sub- task enables knowledge sharing across tasks, while the specific sub-task helps the system better understand a task’s  main goal. Training efficiency and unselfishness have been im- proved in </a><a href="#bookmark201" class="a">[168] through an incentive communication-assisted MARL. Agents create custom messages to influence other agents’ policies, improving coordination and achieving glob- ally optimal decisions. The unselfishness is realized by using MARL to maximize overall profits. In </a><a href="#bookmark202" class="a">[169], driving efficiency and training efficiency have been improved by the adaptive dual-objective transit signal priority (D2-TSP) algorithm with DDQN. D2-TSP optimizes bus speed, saving time for both passengers and those waiting at downstream stops. Similarly, in </a>[170], cooperative intersection management-enhanced DQN boosts both driving and training efficiency by leveraging connectivity between vehicles.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><p class="s19" style="display: inline;"><a name="bookmark41">Three-factor Methods for Unsignalized Intersection Driv- ing</a></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark203" class="a">In </a><a href="#bookmark204" class="a">[171], driving efficiency, unselfishness, and training effi- ciency have been addressed. Driving efficiency is enhanced by penalizing each low-speed state, while unselfishness is achieved through MARL for maximizing overall profits. Train- ing efficiency is improved with multi-agent DQN, which offers faster convergence than baseline algorithms. In </a>[172], driving efficiency, training efficiency, and interpretability have been integrated. Driving efficiency is increased by rewarding each high-velocity state, and training efficiency is achieved by com- bining deep Q-learning with transfer learning. Interpretability is improved by using the IDM for safe vehicle following.</p><p style="padding-top: 2pt;padding-left: 104pt;text-indent: 0pt;text-align: center;">T<span class="s8">ABLE </span>III</p><p style="padding-left: 104pt;text-indent: 0pt;text-align: center;">E<span class="s8">VALUATION OF THE </span>DRL-<span class="s8">BASED DECISION MAKING AT ROUNDABOUTS</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:6.865pt" cellspacing="0"><tr style="height:26pt"><td style="width:50pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">Ref.</p></td><td style="width:100pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Safety</p></td><td style="width:57pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Efficiency</p></td><td style="width:38pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"/><td style="width:94pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;padding-right: 39pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Training Efficiency</p></td><td style="width:91pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Unselfishness</p></td><td style="width:82pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Interpretability</p></td></tr><tr style="height:14pt"><td style="width:50pt;border-top-style:solid;border-top-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark178" class="s63">[146]</a></p></td><td style="width:100pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:57pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">SAC</p></td><td style="width:38pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">with</p></td><td style="width:94pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:91pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:10pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:57pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">higher</p></td><td style="width:38pt"><p class="s62" style="padding-left: 1pt;text-indent: 0pt;line-height: 9pt;text-align: left;">peak</p></td><td style="width:94pt"/><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:57pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">rewards</p></td><td style="width:38pt"/><td style="width:94pt"/><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark179" class="s63">[147]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:57pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Action   repeat,</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:10pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:57pt"/><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">asynchronous</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:57pt"/><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">advantage</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark180" class="s63">[148]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:57pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">ODD-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:10pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:57pt"/><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">embedded</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:57pt"/><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">DQN</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:16pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark181" class="s63">[149]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:57pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">CRM</p></td><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">CRM</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark182" class="s63">[150]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:57pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Expert</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Expert</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:57pt"/><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">guidance</p></td><td style="width:91pt"/><td style="width:82pt"><p class="s62" style="padding-left: 22pt;text-indent: 0pt;line-height: 9pt;text-align: left;">guidance</p></td></tr><tr style="height:26pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark183" class="s63">[151]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 2pt;padding-left: 24pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Allowable rela- tive distance</p></td><td style="width:57pt"><p class="s65" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">v<span class="s66">d</span></p></td><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark184" class="s63">[152]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:57pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Optimization-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Model-based</p></td></tr><tr style="height:10pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:57pt"/><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">embedded</p></td><td style="width:91pt"/><td style="width:82pt"><p class="s62" style="padding-left: 22pt;text-indent: 0pt;line-height: 9pt;text-align: left;">optimization</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:57pt"/><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">DRL</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark185" class="s63">[153]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Collision</p></td><td style="width:57pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">MARL</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"><p class="s62" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">penalties</p></td><td style="width:57pt"/><td style="width:38pt"/><td style="width:94pt"/><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark186" class="s63">[154]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Collision</p></td><td style="width:57pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:38pt"/><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Gradual   train-</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"><p class="s62" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">penalties</p></td><td style="width:57pt"/><td style="width:38pt"/><td style="width:94pt"/><td style="width:91pt"/><td style="width:82pt"><p class="s62" style="padding-left: 22pt;text-indent: 0pt;line-height: 9pt;text-align: left;">ing</p></td></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark187" class="s63">[155]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Safety rewards</p></td><td style="width:95pt" colspan="2"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Efficiency</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">TPRO</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:95pt" colspan="2"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">rewards</p></td><td style="width:94pt"/><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark188" class="s63">[156]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Non-collision</p></td><td style="width:95pt" colspan="2"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Velocity differ-</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">LSTM-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:10pt"><td style="width:50pt"/><td style="width:100pt"><p class="s62" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">rewards</p></td><td style="width:95pt" colspan="2"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">ence rewards</p></td><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">embedded</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:95pt" colspan="2"/><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">actor-critic</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark189" class="s63">[157]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Fewer crashes</p></td><td style="width:95pt" colspan="2"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Higher success</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Reward</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:95pt" colspan="2"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">rates</p></td><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">normalization</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark190" class="s63">[158]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Safety distance</p></td><td style="width:95pt" colspan="2"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Velocity ratio</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Synthetic   rep-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">MARL</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:95pt" colspan="2"/><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">resentation</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark95" class="s63">[58]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Crash penalties</p></td><td style="width:95pt" colspan="2"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">High-speed  re-</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Interval predic-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">IDM</p></td></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:95pt" colspan="2"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">wards</p></td><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">tion</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark191" class="s63">[159]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Collision</p></td><td style="width:95pt" colspan="2"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Vehicle-stop</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">DDPG,   DQN,</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">NMPC</p></td></tr><tr style="height:10pt"><td style="width:50pt"/><td style="width:100pt"><p class="s62" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">penalties</p></td><td style="width:95pt" colspan="2"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">penalties</p></td><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">NMPC integra-</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"/><td style="width:100pt"/><td style="width:95pt" colspan="2"/><td style="width:94pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">tion</p></td><td style="width:91pt"/><td style="width:82pt"/></tr><tr style="height:13pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark131" class="s63">[99]</a></p></td><td style="width:100pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Rule-based  in-</p></td><td style="width:95pt" colspan="2"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">High-speed  re-</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">KAN-DQN</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Rule-based</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Rule-based  in-</p></td></tr><tr style="height:13pt"><td style="width:50pt;border-bottom-style:solid;border-bottom-width:1pt"/><td style="width:100pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">spector</p></td><td style="width:95pt;border-bottom-style:solid;border-bottom-width:1pt" colspan="2"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">wards</p></td><td style="width:94pt;border-bottom-style:solid;border-bottom-width:1pt"/><td style="width:91pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-left: 18pt;text-indent: 0pt;line-height: 9pt;text-align: left;">planning</p></td><td style="width:82pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-left: 22pt;text-indent: 0pt;line-height: 9pt;text-align: left;">spector</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark205" class="a">In </a><a href="#bookmark206" class="a">[173], driving safety, driving efficiency, and training efficiency have been incorporated. Driving safety is promoted through collision penalties, and driving efficiency is enhanced by rewarding velocities higher than a baseline. Training effi- ciency is improved by using a spatial and temporal attention module with SAC. In </a><a href="#bookmark210" class="a">[174], all three aspects have also been addressed. Driving safety and efficiency are improved by rewarding goal attainment and penalizing collisions. Training efficiency is enhanced using a randomized prior function (RPF) for each ensemble member, leading to a better Bayesian posterior [178].</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: left;"><p class="s19" style="display: inline;"><a name="bookmark42">Four-factor Methods for Unsignalized Intersection Driving</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark207" class="a">In </a><a href="#bookmark208" class="a">[175], driving safety, driving efficiency, training effi- ciency, and unselfishness have been incorporated. Driving safety is enhanced through autonomous intersection manage- ment (AIM), and driving efficiency is improved by applying a constant penalty until the AV reaches the exits. Training efficiency is improved by embedding AIM and LSTM into the learning, and unselfishness is achieved through MARL. In </a>[176], driving safety, driving efficiency, training efficiency, and interpretability have been integrated. Driving safety  is promoted through collision penalties, and driving efficiency is enhanced by rewarding goal attainment. Training efficiency is  improved  through  the  Mix-Attention  Network,  synthetic</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:6.865pt" cellspacing="0"><tr style="height:27pt"><td style="width:50pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 3pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">Ref.</p></td><td style="width:88pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Safety</p></td><td style="width:107pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 3pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">Efficiency</p></td><td style="width:94pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 3pt;padding-left: 18pt;padding-right: 39pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Training Efficiency</p></td><td style="width:91pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 3pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Unselfishness</p></td><td style="width:82pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 3pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Interpretability</p></td></tr><tr style="height:26pt"><td style="width:50pt;border-top-style:solid;border-top-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark192" class="s63">[160]</a></p></td><td style="width:88pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 30pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Velocity differ- ence reward</p></td><td style="width:94pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:91pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:15pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark193" class="s63">[161]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">Time penalty</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:25pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark194" class="s63">[162]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Total    waiting time</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Background re- moval ResNet</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:25pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark195" class="s63">[163]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Velocity differ- ence reward</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">IDM</p></td></tr><tr style="height:25pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark196" class="s63">[164]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 22pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Velocity-based reward</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;padding-right: 13pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Safety-based rule policy</p></td></tr><tr style="height:25pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark197" class="s63">[165]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Safe    distance reward</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">MPC with TD3</p></td></tr><tr style="height:25pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark198" class="s63">[166]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Velocity    ratio reward</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;padding-right: 2pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Gridded   coor- dination zone</p></td></tr><tr style="height:35pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark199" class="s63">[167]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 37pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Goal attainment reward</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN with sub- tasks</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:25pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark200" class="s63">[168]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Incentive  com- munication</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">MARL</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:15pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark201" class="s63">[169]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">D2-TSP</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">DDQN</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:25pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark202" class="s63">[170]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 21pt;text-indent: 0pt;line-height: 10pt;text-align: left;">CIM-enhanced DQN</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;padding-right: 21pt;text-indent: 0pt;line-height: 10pt;text-align: left;">CIM-enhanced DQN</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:25pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark203" class="s63">[171]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 35pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Low-speed penalty</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;padding-right: 33pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi-agent DQN</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">MARL</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:35pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark204" class="s63">[172]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 25pt;text-indent: 0pt;line-height: 10pt;text-align: left;">High-velocity reward</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQL         with transfer learning</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">IDM</p></td></tr><tr style="height:25pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark205" class="s63">[173]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;padding-right: 30pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Collision penalties</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 25pt;text-indent: 0pt;line-height: 10pt;text-align: left;">High-velocity reward</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SAC   with   at- tention</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:35pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark206" class="s63">[174]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;padding-right: 30pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Collision penalties</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 37pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Goal attainment reward</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">RPF</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:25pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark207" class="s63">[175]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">AIM</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Constant   time penalty</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;padding-right: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">AIM           and LSTM</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">MARL</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">-</p></td></tr><tr style="height:35pt"><td style="width:50pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark208" class="s63">[176]</a></p></td><td style="width:88pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;padding-right: 30pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Collision penalties</p></td><td style="width:107pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 37pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Goal attainment reward</p></td><td style="width:94pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;padding-right: 23pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Mix-Attention Network</p></td><td style="width:91pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">-</p></td><td style="width:82pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">IDM</p></td></tr><tr style="height:25pt"><td style="width:50pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark209" class="s63">[177]</a></p></td><td style="width:88pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;padding-right: 30pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Collision penalties</p></td><td style="width:107pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 30pt;padding-right: 27pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Low-velocity penalty</p></td><td style="width:94pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">VD-MADQL</p></td><td style="width:91pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">MARL</p></td><td style="width:82pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">IDM</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">representation mechanism, and replay memory mechanism. The interpretability is ensured by using the IDM.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s19" style="display: inline;"><a name="bookmark43">Five-factor Methods for Intersection Driving</a></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark209" class="a">In </a>[177], driving safety has been promoted through collision penalties, while driving efficiency is enhanced by penalizing each state with velocity lower than the <span class="s14">v</span><span class="s26">min</span>. Training efficiency is improved using value decomposition-based multi-agent deep Q-learning. Unselfishness is achieved by employing MARL to minimize joint profits, and interpretability is ensured through the IDM. The DRL-based decision making on unsignalized intersections based on DDTUI is summarized in Table IV.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 77pt;text-indent: -27pt;text-align: left;"><p style="display: inline;"><a name="bookmark44">C</a><span class="s8">ONCLUSION AND </span>D<span class="s8">ISCUSSION</span></p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">This survey presents a comprehensive overview of the current state of the art in DRL-based decision-making for autonomous vehicles. By discussing recent research efforts in this field, this survey highlights the diverse algorithms developed to address decision-making tasks across various sce- narios, including highways, on-ramping merging, roundabouts, and unsignalized intersections. Our analysis goes beyond simply presenting these algorithms by uncovering valuable insights, identifying key gaps in the current research, and high- lighting emerging trends in DRL-based decision making for autonomous driving. While driving efficiency and safety are</p><p style="padding-top: 2pt;padding-left: 82pt;text-indent: 0pt;text-align: left;">O<span class="s8">CCURRENCE AND </span>R<span class="s8">ATIO OF </span>E<span class="s8">VALUATION </span>F<span class="s8">ACTORS </span>A<span class="s8">CROSS </span>D<span class="s8">IFFERENT </span>S<span class="s8">CENARIOS</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.214pt" cellspacing="0"><tr style="height:16pt"><td style="width:71pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">Scenario</p></td><td style="width:83pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Safety</p></td><td style="width:80pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Efficiency</p></td><td style="width:90pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Training Efficiency</p></td><td style="width:68pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Unselfishness</p></td><td style="width:85pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Interpretability</p></td></tr><tr style="height:16pt"><td style="width:71pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">Highway</p></td><td style="width:83pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">23 (76.7%)</p></td><td style="width:80pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">20 (66.7%)</p></td><td style="width:90pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">11 (36.7%)</p></td><td style="width:68pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">13 (43.3%)</p></td><td style="width:85pt;border-top-style:solid;border-top-width:1pt"><p class="s62" style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">11 (36.7%)</p></td></tr><tr style="height:15pt"><td style="width:71pt"><p class="s62" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">Ramp</p></td><td style="width:83pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">9 (56.25%)</p></td><td style="width:80pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">14 (87.5%)</p></td><td style="width:90pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">8 (50%)</p></td><td style="width:68pt"><p class="s62" style="padding-top: 1pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">8 (50%)</p></td><td style="width:85pt"><p class="s62" style="padding-top: 1pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">9 (56.25%)</p></td></tr><tr style="height:15pt"><td style="width:71pt"><p class="s62" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">Roundabout</p></td><td style="width:83pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">10 (62.5%)</p></td><td style="width:80pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">11 (68.75%)</p></td><td style="width:90pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">12 (75%)</p></td><td style="width:68pt"><p class="s62" style="padding-top: 1pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">3 (18.75%)</p></td><td style="width:85pt"><p class="s62" style="padding-top: 1pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">6 (37.5%)</p></td></tr><tr style="height:15pt"><td style="width:71pt"><p class="s62" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">Intersection</p></td><td style="width:83pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">5 (27.7%)</p></td><td style="width:80pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">17 (94.4%)</p></td><td style="width:90pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">12 (66.7%)</p></td><td style="width:68pt"><p class="s62" style="padding-top: 1pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">4 (22.2%)</p></td><td style="width:85pt"><p class="s62" style="padding-top: 1pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">6 (33.3%)</p></td></tr><tr style="height:15pt"><td style="width:71pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">Total</p></td><td style="width:83pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">47 (58%)</p></td><td style="width:80pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">63 (77.8%)</p></td><td style="width:90pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">44 (54.3%)</p></td><td style="width:68pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">29 (35.8%)</p></td><td style="width:85pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s62" style="padding-top: 1pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">32 (39.5%)</p></td></tr></table><p class="s8" style="padding-top: 1pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">The numbers in parentheses indicate the percentage of the total studies for each factor.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">addressed across most studies, there is a growing trend towards addressing multiple DDTUI factors concurrently. Emerging approaches, such as MARL and the integration of traditional control methods with DRL, show promise in tackling complex challenges with increased unselfishness and interpretability in autonomous driving.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Based on existing studies, Table V summarizes the distribu- tion of evaluation factors considered in four typical scenarios. Most studies, i.e., 18 studies that account for 94.4% of ex- isting studies, prioritize efficiency at intersections to optimize travel time in complex and interaction-heavy environments. Efficiency is also emphasized at ramps in 14 studies (i.e., 87.5%) to reduce congestion and streamline traffic flow. Safety is particularly emphasized on highways in 23 studies (i.e., 76.7%), which addresses the importance of accident prevention in high-speed settings. In contrast, intersections address safety less often. Training efficiency is significant at roundabouts in 12 studies (i.e., 75%) and unsignalized intersections in 12 studies (i.e., 66.7%). This reflects a need for effective train- ing methods to ensure smooth vehicle maneuvering in these challenging contexts. Interpretability is particularly valued at ramps in 9 studies (i.e., 56.25%) and on highways in 11 studies (i.e., 36.7%), respectively. This emphasizes understandable decision-making in these areas. Unselfishness receives less emphasis overall, although highways and ramps give it much attention. Future challenges are summarized as</p><ol id="l18"><ol id="l19"><li style="padding-top: 1pt;padding-left: 32pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">Achieving a balance between all five DDTUI factors in a single framework: This survey reveals that while many studies addressed multiple DDTUI factors, very few managed to incorporate all five factors simultaneously. For instance, only 3 out of 16 studies in roundabout sce- narios and 1 out of 19 studies in intersection scenarios addressed all five factors. This highlights the complexity of developing a unified framework that can effectively balance DDTUI. Future research should focus on devel- oping integrated frameworks that can holistically address five DDTUI factors concurrently.</p></li><li style="padding-left: 32pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">Improving the interpretability of DRL models without sacrificing performance: While some studies have made strides in improving interpretability, such as using IDM for interpretable car-following, many high-performing DRL models remain black boxes. Out of the reviewed papers, less than 40% explicitly addressed interpretabil- ity. Furthermore, most papers considering interpretabil- ity use only one method. In the future, multiple in- terpretability methods can be applied to enhance inter-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">pretability, such as using the APF and IDM concurrently.</p></li><li style="padding-left: 28pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">Enhancing the unselfishness of AVs in complex, multi- agent environments: While approximately 50% of stud- ies use MARL to promote unselfishness, the complexity of real-world traffic scenarios presents uncertainties of driving behaviors. Future research should explore more sophisticated MARL techniques based on real-world experience. For example, combining game theory with driving style classification based on real-world datasets can better model the behaviors of HDVs.</p></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: center;"><a name="bookmark45">R</a><span class="s8">EFERENCES</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 26pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a href="https://www.gov.uk/government/collections/road-accidents-and-safety-statistics" class="s10" target="_blank" name="bookmark46">[1]  Department  for  Transport,  “Road   accidents   and   safety statistics,” https://www.gov.uk/government/collections/ </a>road-accidents-and-safety-statistics, 2023, accessed: 2024-04-28.</p><p class="s8" style="padding-left: 26pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark47">[2] D. Omeiza, H. Webb, M. Jirotka, and L. Kunze, “Explanations in autonomous driving: A survey,” </a><i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 23, no. 8, pp. 10 142–10 162, 2021.</p><p class="s8" style="padding-left: 26pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark48">[3] H. A. Ignatious, M. Khan </a><i>et al.</i>, “An overview of sensors in autonomous vehicles,” <i>Procedia Computer Science</i>, vol. 198, pp. 736–741, 2022.</p><p class="s8" style="padding-left: 11pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark49">[4]  “Self-driving cars: A survey,” </a><i>Expert Systems with Applications</i>, vol.</p><p class="s8" style="padding-left: 26pt;text-indent: 0pt;line-height: 9pt;text-align: left;">165, p. 113816, 2021.</p><p class="s8" style="padding-left: 26pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark50">[5] J. Pe´rez, V. Milane´s </a><i>et al.</i>, “Autonomous driving manoeuvres in urban road traffic environment: a study on roundabouts,” <i>IFAC Proceedings Volumes</i>, vol. 44, no. 1, pp. 13 795–13 800, 2011.</p><p class="s8" style="padding-left: 26pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark51">[6] Z. Lin, Q. Zhang, Z. Tian, P. Yu, and J. Lan, “Dpl-slam: Enhancing dynamic point-line slam through dense semantic methods,” </a><i>IEEE Sensors Journal</i>, 2024.<a name="bookmark52">&zwnj;</a></p><p class="s8" style="padding-left: 26pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[7] Z. Lin, Q. Zhang, Z. Tian, P. Yu, Z. Ye, H. Zhuang, and J. Lan, “Slam2: Simultaneous localization and multimode mapping for indoor dynamic environments,” <i>Pattern Recognition</i>, p. 111054, 2024.</p><p class="s8" style="padding-left: 26pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a href="https://www.who.int/news-room/fact-sheets/detail/road-traffic-injuries" class="s10" target="_blank" name="bookmark53">[8] World Health Organization, “Road traffic injuries,” https://www.who. </a>int/news-room/fact-sheets/detail/road-traffic-injuries, 2023, accessed: 2024-04-28.<a name="bookmark54">&zwnj;</a></p><p class="s8" style="padding-left: 26pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[9] H. Vijayakumar, D. Zhao <i>et al.</i>, “A holistic safe planner for automated driving considering interaction with human drivers,” <i>IEEE Transaction on Intelligent Vehicle</i>, vol. 9, no. 1, pp. 2061–2076, 2023.</p><p class="s8" style="padding-left: 7pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark55">[10]  J. Lo¨fberg, </a><i>Minimax approaches to robust model predictive control</i>.</p><p class="s8" style="padding-left: 26pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Linko¨ping University Electronic Press, 2003, vol. 812.</p><p class="s8" style="padding-left: 26pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark56">[11] S. V. Rakovic´, “Model predictive control: classical, robust, and stochas- tic [bookshelf],” </a><i>IEEE Control Systems Magazine</i>, vol. 36, no. 6, pp. 102–105, 2016.<a name="bookmark57">&zwnj;</a></p><p class="s8" style="padding-left: 26pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark58">[12] Z. Lin, Z. Tian </a><i>et al.</i>, “Enhanced visual slam for collision-free driving with lightweight autonomous cars,” <i>Sensors</i>, vol. 24, no. 19, p. 6258, 2024.</p><p class="s8" style="padding-left: 26pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[13] P. Bhattacharya and M. L. Gavrilova, “Voronoi diagram in optimal path planning,” in <i>Proceedings of the International Symposium on Voronoi Diagrams in Science and Engineering</i>, 2007, pp. 38–47.</p><p class="s8" style="padding-left: 26pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark59">[14] M. Abdel-Aty and S. Ding, “A matched case-control analysis of autonomous vs human-driven vehicle accidents,” </a><i>Nature Communica- tions</i>, vol. 15, no. 1, p. 4931, 2024.</p><p class="s8" style="padding-left: 26pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark60">[15] H. H. Triharminto, O. Wahyunggoro </a><i>et al.</i>, “A novel of repulsive func- tion on artificial potential field for robot path planning,” <i>International Journal of Electrical and Computer Engineering</i>, vol. 6, no. 6, p. 3262, 2016.</p><p class="s8" style="padding-top: 3pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark61">[16] Q. Yao, Z. Zheng </a><i>et al.</i>, “Path planning method with improved artificial potential field—a reinforcement learning perspective,” <i>IEEE Access</i>, vol. 8, pp. 135 513–135 523, 2020.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark62">[17] H. H. Triharminto, O. Wahyunggoro </a><i>et al.</i>, “Local information using stereo camera in artificial potential field based path planning,” <i>IAENG International Journal of Computer Science</i>, vol. 44, no. 3, pp. 316–326, 2017.<a name="bookmark63">&zwnj;</a></p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark64">[18] B. Mahesh, “Machine learning algorithms-a review,” </a><i>International Journal of Science and Research (IJSR).[Internet]</i>, vol. 9, no. 1, pp. 381–386, 2020.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[19] M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspec- tives, and prospects,” <i>Science</i>, vol. 349, no. 6245, pp. 255–260, 2015.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark65">[20] I. Muhammad and Z. Yan, “Supervised machine learning approaches: A survey.” </a><i>ICTACT Journal on Soft Computing</i>, vol. 5, no. 3, 2015.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;text-align: justify;"><a name="bookmark66">[21] M. Castelli, L. Vanneschi </a><i>et al.</i>, “Supervised learning: classification,” <i>por Ranganathan, S., M. Grisbskov, K. Nakai y C. Scho¨nbach</i>, vol. 1, pp. 342–349, 2018.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark67">[22] R. Tian, S. Li </a><i>et al.</i>, “Adaptive game-theoretic decision making for autonomous vehicle control at roundabouts,” in <i>Proceedings of the IEEE Conference on Decision and Control</i>. IEEE, 2018, pp. 321– 326.<a name="bookmark68">&zwnj;</a></p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[23] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement learning: A survey,” <i>Journal of Artificial Intelligence Research</i>, vol. 4, pp. 237–285, 1996.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark69">[24] J. Lu, L.  Han </a><i>et al.</i>, “Event-triggered deep reinforcement  learning using parallel control: A case study in autonomous driving,” <i>IEEE Transactions on Intelligent Vehicles</i>, vol. 8, no. 4, pp. 2821–2831, 2023.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark70">[25] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” </a><i>nature</i>, vol. 521, no. 7553, pp. 436–444, 2015.</p><p class="s8" style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark71">[26]  I. Goodfellow, </a><i>Deep learning</i>.   MIT Press, 2016, vol. 196.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark72">[27] K. Yeom, “Deep reinforcement learning based autonomous driving with collision free for mobile robots,” </a><i>International Journal of Mechanical Engineering and Robotics Research</i>, vol. 11, no. 5, pp. 338–344, 2022.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark73">[28] A. J. M. Muzahid, S. F. Kamarulzaman </a><i>et al.</i>, “Deep reinforcement learning-based driving strategy for avoidance of chain collisions and its safety efficiency analysis in autonomous vehicles,” <i>IEEE Access</i>, vol. 10, pp. 43 303–43 319, 2022.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark74">[29] C. Xu, W. Zhao </a><i>et al.</i>, “A Nash Q-learning based motion decision algorithm with considering interaction to traffic participants,” <i>IEEE Transactions on Vehicular Technology</i>, vol. 69, no. 11, pp. 12 621–</p><p class="s8" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">12 634, 2020.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[30] K. Min, H. Kim <i>et al.</i>, “Deep Q learning based high level driving policy determination,” in <i>Proceedings of the IEEE Intelligent Vehicles Symposium</i>, 2018, pp. 226–231.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[31] S. Gu, T. Lillicrap <i>et al.</i>, “Continuous deep Q-learning with model- based acceleration,” in <i>Proceedings of the International Conference on Machine Learning</i>, 2016, pp. 2829–2838.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[32] H. Wei, X. Liu <i>et al.</i>, “Mixed-autonomy traffic control with proximal policy optimization,” in <i>Proceedings of the IEEE Vehicular Networking Conference</i>, 2019, pp. 1–8.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[33] F. Ye, X. Cheng <i>et al.</i>, “Automated lane change strategy using proximal policy optimization-based deep reinforcement learning,” in <i>Proceedings of the IEEE Intelligent Vehicles Symposium</i>, 2020, pp. 1746–1752.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[34] G. Dulac-Arnold, R. Evans <i>et al.</i>, “Deep reinforcement learning in large discrete action spaces. arxiv 2015,” <i>arXiv preprint arXiv:1512.07679</i>.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark75">[35] Z. Tian, D. Zhao </a><i>et al.</i>, “Efficient and balanced exploration-driven decision making for autonomous racing using local information,” <i>IEEE Transactions on Intelligent Vehicles</i>, 2024.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[36] Z. Tian, D. Zhao, Z. Lin, D. Flynn, W. Zhao, and D. Tian, “Balanced reward-inspired reinforcement learning for autonomous vehicle racing,” in <i>6th Annual Learning for Dynamics &amp; Control Conference</i>. PMLR, 2024, pp. 628–640.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[37] G. Basile, A. Petrillo, and S. Santini, “DDPG based end-to-end driving enhanced with safe anomaly detection functionality for autonomous vehicles,” in <i>Proceedings of the IEEE International Conference on Metrology for Extended Reality, Artificial Intelligence and Neural Engineering</i>, 2022, pp. 248–253.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark76">[38] M. A. Hebaish, A. Hussein </a><i>et al.</i>, “Towards safe and efficient modular path planning using twin delayed DDPG,” in <i>Proceedings of the IEEE Vehicular Technology Conference</i>, 2022, pp. 1–7.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark77">[39] L. Abualigah, S. Ekinci </a><i>et al.</i>, “Modified elite opposition-based ar- tificial hummingbird algorithm for designing fopid controlled cruise control system.” <i>Intelligent Automation &amp; Soft Computing</i>, vol. 38, no. 2, 2023.<a name="bookmark78">&zwnj;</a></p><p class="s8" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">[40]  S. Tang, Z. Zhang, Y. Zhang, J. Zhou, Y. Guo, S. Liu, S. Guo, Y.-F. Li,</p><p class="s8" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">L. Ma, Y. Xue <i>et al.</i>, “A survey on automated driving system testing:</p><p class="s8" style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Landscapes and trends,” <i>ACM Transactions on Software Engineering and Methodology</i>, vol. 32, no. 5, pp. 1–62, 2023.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark79">[41] K. Yang, X. Tang </a><i>et al.</i>, “Towards robust decision-making for au- tonomous driving on highway,” <i>IEEE Transactions on Vehicular Tech- nology</i>, vol. 72, no. 9, pp. 11 251–11 263, 2023.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark80">[42] J. Wu, Z. Song </a><i>et al.</i>, “Deep reinforcement learning-based energy- efficient decision-making for autonomous electric vehicle in dynamic traffic environments,” <i>IEEE Transactions on Transportation Electrifi- cation</i>, vol. 10, no. 1, pp. 875–887, 2023.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark81">[43] Y. Fu, C. Li </a><i>et al.</i>, “An incentive mechanism of incorporating su- pervision game for federated learning in autonomous driving,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 24, no. 12, pp. 14 800–14 812, 2023.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark82">[44] W. Yue, X. Wu, C. Li, N. Cheng, P. Duan, and Z. Han, “Navigating the impact of connected and automated vehicles on mixed traffic efficiency: A driving behavior perspective,” </a><i>IEEE Internet of Things Journal</i>, 2024.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[45] B. Toghi, R. Valiente, D. Sadigh, R. Pedarsani, and Y. P. Fallah, “Social coordination and altruism in autonomous driving,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 23, no. 12, pp. 24 791–</p><p class="s8" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">24 804, 2022.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark83">[46] Y. Liu, X. Zhao, Y. Tian, and J. Sun, “Sociality probe: Game- theoretic inverse reinforcement learning for modeling and quantifying social patterns in driving interaction,” </a><i>IEEE Transactions on Intelligent Transportation Systems</i>, 2024.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark84">[47] X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo, M. Wu, and X. Yi, “A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability,” </a><i>Computer Science Review</i>, vol. 37, p. 100270, 2020.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark85">[48] F.-L. Fan, J. Xiong, M. Li, and G. Wang, “On interpretability of artificial neural networks: A survey,” </a><i>IEEE Transactions on Radiation and Plasma Medical Sciences</i>, vol. 5, no. 6, pp. 741–760, 2021.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark86">[49] D. Karas, “Highway to inequity: the disparate impact of the interstate highway system on poor and minority communities in american cities,” </a><i>New Visions for Public Affairs</i>, vol. 7, no. April, pp. 9–21, 2015.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark87">[50] M. Gross, “Speed tourism: The german autobahn as a tourist destination and location of “unruly rules”,” </a><i>Tourist Studies</i>, vol. 20, no. 3, pp. 298– 313, 2020.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark88">[51] J. P. Leisch, “Freeway and interchange design: A historical perspec- tive,” </a><i>Transportation Research Record</i>, pp. 60–60, 1993.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark89">[52] G. Davis, M. Contreras-Sweet </a><i>et al.</i>, “Ramp meter design manual,” <i>Traffic Operation Program, Department of California Highway Patrol</i>, 2000.<a name="bookmark90">&zwnj;</a></p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[53] A. Pratelli and R. R. Souleyrette, “Visibility, perception and roundabout safety,” <i>WIT Transactions on the Built Environment</i>, vol. 107, pp. 577– 588, 2009.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark91">[54] M. Naderi, M. Papageorgiou </a><i>et al.</i>, “Automated vehicle driving on large lane-free roundabouts,” in <i>Proceeding of the IEEE International Conference on Intelligent Transportation Systems</i>, 2022, pp. 1528– 1535.<a name="bookmark92">&zwnj;</a></p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[55] J. G. Bared, P. K. Edara <i>et al.</i>, “Design and operational performance of double crossover intersection and diverging diamond interchange,” <i>Transportation Research Record</i>, vol. 1912, no. 1, pp. 31–38, 2005.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark93">[56] J. York and T. Maze, “Economic evaluation of truck collision warning systems,” </a><i>Transportation Research Circular</i>, vol. 475, pp. 46–50, 1997.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark94">[57] I. C. Burnett, </a><i>Traffic Collisions in North Carolina: Weather, Human Factors, and Economic Analysis, 2013 to 2019</i>. North Carolina State University, 2023.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark95">[58] J. Wang, J. Wu, X. Zheng, D. Ni, and K. Li, “Driving safety field theory modeling and its application in pre-collision warning system,” </a><i>Transportation Research Part C: Emerging Technologies</i>, vol. 72, pp. 306–324, 2016.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark96">[59] R. Nahata, D. Omeiza, R. Howard, and L. Kunze, “Assessing and explaining collision risk in dynamic environments for autonomous driving safety,” in </a><i>Proceeding of the IEEE International Intelligent Transportation Systems Conference</i>.   IEEE, 2021, pp. 223–230.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark97">[60] M. Wang, L. Zhang, Z. Zhang, and Z. Wang, “A hybrid trajectory planning strategy for intelligent vehicles in on-road dynamic scenarios,” </a><i>IEEE Transactions on Vehicular Technology</i>, vol. 72, no. 3, pp. 2832– 2847, 2023.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark98">[61] A. Botros and S. L. Smith, “Spatio-temporal lattice planning using optimal motion primitives,” </a><i>IEEE Transactions on Intelligent Trans- portation Systems</i>, 2023.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark99">[62] W. Chen, Y. Chen </a><i>et al.</i>, “Motion planning using feasible and smooth tree for autonomous driving,” <i>IEEE Transactions on Vehicular Tech- nology</i>, vol. 73, no. 5, pp. 6270–6282, 2024.</p><p class="s8" style="padding-top: 3pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark100">[63] F. Bouchard, S. Sedwards, and K. Czarnecki, “A rule-based behaviour planner for autonomous driving,” in </a><i>Proceeding of the International Joint Conference on Rules and Reasoning</i>. Springer, 2022, pp. 263– 279.<a name="bookmark101">&zwnj;</a></p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[64] R. Gajjar and D. Mohandas, “Critical assessment of road capacities on urban roads–a mumbai case-study,” <i>Transportation Research Procedia</i>, vol. 17, pp. 685–692, 2016.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[65] M. A. S. Kamal, T. Hayakawa, and J.-i. Imura, “Road-speed profile for enhanced perception of traffic conditions in a partially connected vehicle environment,”  <i>IEEE Transactions  on  Vehicular Technology</i>, vol. 67, no. 8, pp. 6824–6837, 2018.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark102">[66] M. A. S. Kamal, S. Taguchi, and T. Yoshimura, “Efficient driving on multilane roads under a connected vehicle environment,” </a><i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 17, no. 9, pp. 2541–2551, 2016.<a name="bookmark103">&zwnj;</a></p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[67] D. A. Hensher, “Valuation of travel time savings,” in <i>A handbook of transport economics</i>.   Edward Elgar Publishing, 2011.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[68] F. Steck, V. Kolarova, F. Bahamonde-Birke, S. Trommer, and B. Lenz, “How autonomous driving may affect the value of travel time savings for commuting,” <i>Transportation Research Record</i>, vol. 2672, no. 46, pp. 11–20, 2018.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark104">[69] C. Zhai, F. Luo, Y. Liu, and Z. Chen, “Ecological cooperative look- ahead control for automated vehicles travelling on freeways with varying slopes,” </a><i>IEEE Transactions on Vehicular Technology</i>, vol. 68, no. 2, pp. 1208–1221, 2018.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark105">[70] S. A. Birrell, M. Fowkes </a><i>et al.</i>, “Effect of using an in-vehicle smart driving aid on real-world driver performance,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 15, no. 4, pp. 1801–1810, 2014.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark106">[71] C. Sun, J. Guanetti </a><i>et al.</i>, “Optimal eco-driving control of connected and autonomous vehicles through signalized intersections,” <i>IEEE In- ternet of Things Journal</i>, vol. 7, no. 5, pp. 3759–3773, 2020.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark107">[72] A. Srinivas, T.-Y. Lin, N. Parmar, J. Shlens, P. Abbeel, and A. Vaswani, “Bottleneck transformers for visual recognition,” in </a><i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2021, pp. 16 519–16 529.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[73] R. Xu, J. Joshi <i>et al.</i>, “Nn-emd: Efficiently training neural networks using encrypted multi-sourced datasets,” <i>IEEE Transactions on De- pendable and Secure Computing</i>, vol. 19, no. 4, pp. 2807–2820, 2021.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark108">[74] H. Touvron, P. Bojanowski </a><i>et al.</i>, “Resmlp: Feedforward networks for image classification with data-efficient training,” <i>IEEE transactions on Pattern Analysis and Machine Intelligence</i>, vol. 45, no. 4, pp. 5314– 5321, 2022.<a name="bookmark109">&zwnj;</a></p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[75] C. M. Martinez, M. Heucke, F.-Y. Wang, B. Gao, and D. Cao, “Driving style recognition for intelligent vehicle control and advanced driver assistance: A survey,” <i>IEEE Transaction on Intelligent Transportation System</i>, vol. 19, no. 3, pp. 666–676, 2017.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark110">[76] X. Li, W. Wang, and M. Roetting, “Estimating driver’s lane-change in- tent considering driving style and contextual traffic,” </a><i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 20, no. 9, pp. 3258–3271, 2018.<a name="bookmark111">&zwnj;</a></p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[77] D. Xu, H. Zhao, F. Guillemard, S. Geronimi, and F. Aioun, “Aware of scene vehicles—probabilistic modeling of car-following behaviors in real-world traffic,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 20, no. 6, pp. 2136–2148, 2018.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark112">[78] P. Jardin, I. Moisidis, S. S. Zetina,  and  S.  Rinderknecht,  “Rule- based driving style  classification  using  acceleration  data  profiles,” in </a><i>Proceeding of  the IEEE  International Conference on  Intelligent Transportation Systems</i>, 2020, pp. 1–6.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark113">[79] R. Vogel, F. Schmidsberger, A. Ku¨hn, K. A. Schneider </a><i>et al.</i>, “You can’t drive my car-a method to fingerprint individual driving styles in a sim- racing setting,” in <i>Proceeding of the IEEE International Conference on Electrical, Computer and Energy Technologies</i>, 2022, pp. 1–9.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark114">[80] F. Lateef, M. Kas </a><i>et al.</i>, “Saliency heat-map as visual attention for autonomous driving using generative adversarial network (GAN),” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 23, no. 6, pp. 5360–5373, 2022.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark115">[81] N. Ding, C. Zhang </a><i>et al.</i>, “Saliendet: A saliency-based feature enhance- ment algorithm for object detection for autonomous driving,” <i>IEEE Transactions on Intelligent Vehicles</i>, vol. 9, no. 1, pp. 2624–2635, 2023.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark116">[82] Z. Cui, M. Li </a><i>et al.</i>, “An interpretation framework for autonomous vehicles decision-making via shap and rf,” in <i>Proceeding of the CAA International Conference on Vehicular Control and Intelligence</i>, 2022, pp. 1–7.</p><p class="s8" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[83] B. Gyevnar,  C.  Wang  <i>et  al.</i>,  “Causal  explanations  for  se- quential decision-making in multi-agent systems,” <i>arXiv preprint arXiv:2302.10809</i>, 2023.</p><p class="s8" style="padding-top: 3pt;padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[84] P. M. Dassanayake, A. Anjum <i>et al.</i>, “A deep learning based explainable control system for reconfigurable networks of edge devices,” <i>IEEE Transactions on Network Science and Engineering</i>, vol. 9, no. 1, pp. 7–19, 2021.</p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark117">[85] M. Zemni, M. Chen </a><i>et al.</i>, “Octet: Object-aware counterfactual expla- nations,” in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2023, pp. 15 062–15 071.</p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark118">[86] Z. Chen, F. Xiao, F. Guo, and J. Yan, “Interpretable machine learning for building energy management: A state-of-the-art review,” </a><i>Advances in Applied Energy</i>, vol. 9, p. 100123, 2023.</p><p class="s8" style="padding-left: 9pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark119">[87]  D.  Leslie,  “Understanding  artificial  intelligence  ethics  and  safety,”</a></p><p class="s67" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;">arXiv preprint arXiv:1906.05684<span class="s8">, 2019.</span></p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark120">[88] S. Hooker, D. Erhan, P.-J. Kindermans, and B. Kim, “A benchmark for interpretability methods in deep neural networks,” </a><i>Advances in Neural Information Processing Systems</i>, vol. 32, 2019.</p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark121">[89] R. Tomsett, D. Harborne, S. Chakraborty, P. Gurram, and A. Preece, “Sanity checks for saliency metrics,” in </a><i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, vol. 34, no. 04, 2020, pp. 6021–</p><p class="s8" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark122">6029.</a></p><p class="s8" style="padding-left: 9pt;text-indent: 0pt;line-height: 9pt;text-align: left;">[90]  J.  Adebayo,  J.  Gilmer,  M.  Muelly,  I.  Goodfellow,  M.  Hardt,  and</p><p class="s8" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;">B.  Kim,  “Sanity  checks  for  saliency  maps,”  <i>Advances  in  neural information processing systems</i>, vol. 31, 2018.</p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark123">[91] A. Ghorbani, A. Abid, and J. Zou, “Interpretation of neural networks is fragile,” in </a><i>Proceedings of the AAAI Conference on Artificial Intel- ligence</i>, vol. 33, no. 01, 2019, pp. 3681–3688.</p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark124">[92] A. A. Ismail, M. Gunady, L. Pessoa, H. Corrada Bravo, and S. Feizi, “Input-cell attention reduces vanishing saliency of recurrent neural net- works,” </a><i>Advances in Neural Information Processing Systems</i>, vol. 32, 2019.<a name="bookmark125">&zwnj;</a></p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[93] M. Wu, M. Hughes, S. Parbhoo, M. Zazzi, V. Roth, and F. Doshi-Velez, “Beyond sparsity: Tree regularization of deep models for interpretabil- ity,” in <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, vol. 32, no. 1, 2018.</p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark126">[94] T. Speith and M. Langer, “A new perspective on evaluation methods for explainable artificial intelligence (xai),” in </a><i>Proceeding of  the IEEE International Requirements Engineering Conference Workshops</i>. IEEE, 2023, pp. 325–331.</p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark127">[95] A. Baheri, S. Nageshrao </a><i>et al.</i>, “Deep reinforcement learning with enhanced safety for autonomous highway driving,” in <i>Proceeding of the IEEE Intelligent Vehicles Symposium</i>, 2020, pp. 1550–1555.</p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark128">[96] M. Kaushik, V. Prasad </a><i>et al.</i>, “Overtaking maneuvers in simulated highway driving using deep reinforcement learning,” in <i>2018 IEEE Intelligent Vehicles Symposium</i>.   IEEE, 2018, pp. 1885–1890.</p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark129">[97] N. Albarella, D. G. Lui </a><i>et al.</i>, “A hybrid deep reinforcement learning and optimal control architecture for autonomous highway driving,” <i>Energies</i>, vol. 16, no. 8, p. 3490, 2023.</p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark130">[98] H. Meng, H. Bin </a><i>et al.</i>, “Optimizing distributed energy system with an enhanced reinforcement learning–model predictive control algorithm,”  <i>Available at SSRN 4876862</i>.<a name="bookmark131">&zwnj;</a></p><p class="s8" style="padding-left: 28pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[99] Z. Lin, Z. Tian <i>et al.</i>, “A conflicts-free, speed-lossless KAN-based reinforcement learning decision system for interactive driving in round- abouts,” <i>arXiv preprint arXiv:2408.08242</i>, 2024.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark132">[100] J. Go´mez-Romero, “Explaining deep reinforcement learning-based methods for control of building hvac systems,” </a><i>Methods</i>, vol. 15, p. 52.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark133">[101] S. Shalev-Shwartz, S. Shammah </a><i>et al.</i>, “Safe, multi-agent, re- inforcement learning for autonomous driving,” <i>arXiv preprint arXiv:1610.03295</i>, 2016.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark134">[102] C. Yu, X. Wang, J. Hao, and Z. Feng, “Reinforcement learning for cooperative overtaking,” in </a><i>Proceedings of the International Conference on Autonomous Agents and Multiagent Systems</i>, 2019, pp. 341–349.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark135">[103] M. B. Ozcelik, B. Agin </a><i>et al.</i>, “Decision making for autonomous driv- ing in a virtual highway environment based on generative adversarial imitation learning,”  in <i>Proceeding of  the Innovations in Intelligent Systems and Applications Conference</i>, 2023, pp. 1–6.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark136">[104] S. Zhang, Y. Wu </a><i>et al.</i>, “Spatial attention for autonomous decision- making in highway scene,” in <i>Proceeding of the Annual Conference of the Society of Instrument and Control Engineers of Japan</i>, 2020, pp. 1435–1440.<a name="bookmark137">&zwnj;</a></p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[105] M. Treiber, A. Hennecke, and D. Helbing, “Congested traffic states in empirical observations and microscopic simulations,” <i>Physical Review E</i>, vol. 62, no. 2, p. 1805, 2000.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark138">[106] S. Nageshrao, H. E. Tseng, and D. Filev, “Autonomous highway driving using deep reinforcement learning,” in </a><i>Proceedings of the IEEE International Conference on Systems, Man and Cybernetics</i>, 2019, pp. 2326–2331.</p><p class="s8" style="padding-top: 3pt;padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark139">[107] J. Zhao, T. Qu </a><i>et al.</i>, “A deep reinforcement learning approach for autonomous highway driving,” <i>IFAC-PapersOnLine</i>, vol. 53, no. 5, pp. 542–546, 2020.<a name="bookmark140">&zwnj;</a></p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark141">[108] W. Zhao, S. Gong, D. Zhao, F. Liu, N. Sze, M. Quddus, and H. Huang, “A spatial-state-based omni-directional collision warning system for intelligent vehicles,” </a><i>IEEE Transactions on Intelligent Transportation Systems</i>, 2024.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark142">[109] B. M. Albaba and Y. Yildiz, “Driver modeling through deep rein- forcement learning and behavioral game theory,” </a><i>IEEE Transactions on Control Systems Technology</i>, vol. 30, no. 2, pp. 885–892, 2021.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark143">[110] J. Yang, A. Nakhaei </a><i>et al.</i>, “Cm3: Cooperative multi-goal multi-stage multi-agent reinforcement learning,” <i>arXiv preprint arXiv:1809.05188</i>, 2018.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark144">[111] M. Schutera, N. Goby </a><i>et al.</i>, “Transfer learning versus multi-agent learning regarding distributed decision-making in highway traffic,”  <i>arXiv preprint arXiv:1810.08515</i>, 2018.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark145">[112] X. Xu, L. Zuo </a><i>et al.</i>, “A reinforcement learning approach to au- tonomous decision making of intelligent vehicles on highways,” <i>IEEE Transactions on Systems, Man, and  Cybernetics:  Systems</i>,  vol.  50, no. 10, pp. 3884–3897, 2018.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark146">[113] Z. Bai, W. Shangguan </a><i>et al.</i>, “Deep reinforcement learning based high- level driving behavior decision-making model in heterogeneous traffic,” in <i>Proceeding of the IEEE Chinese Control Conference</i>. IEEE, 2019, pp. 8600–8605.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark147">[114] T. Liu, Q. Liu </a><i>et al.</i>, “Combining deep reinforcement learning with rule-based constraints for safe highway driving,” in <i>Proceeding of the China Automation Congress</i>, 2022, pp. 2785–2790.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark148">[115] J. Liao, T. Liu </a><i>et al.</i>, “Decision-making strategy on highway for autonomous vehicles using deep reinforcement learning,” <i>IEEE Access</i>, vol. 8, pp. 177 804–177 814, 2020.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark149">[116] W. Yuan, M. Yang </a><i>et al.</i>, “Multi-reward architecture based reinforce- ment learning for highway driving policies,” in <i>Proceeding of the IEEE Intelligent Transportation Systems Conference</i>, 2019, pp. 3810–3815.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark150">[117] S. Aradi, T. Becsi </a><i>et al.</i>, “Policy gradient based reinforcement learning approach for autonomous highway driving,” in <i>Proceeding of the IEEE Conference on Control Technology and Applications</i>. IEEE, 2018, pp. 670–675.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark151">[118] H. Wang, S. Yuan </a><i>et al.</i>, “Tactical driving decisions of unmanned ground vehicles in complex highway environments: A deep reinforce- ment learning approach,” <i>Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering</i>, vol. 235, no. 4, pp. 1113–1127, 2021.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark152">[119] A. M. Naveen, R. Ravish </a><i>et al.</i>, “Distributional reinforcement learning for automated driving vehicle,” in <i>Proceeding of the IEEE Mysore Sub Section International Conference</i>, 2022, pp. 1–6.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark153">[120] J. Wang, T. Yang </a><i>et al.</i>, “Learning an efficient and safe policy for highway driving using supervised learning and reinforcement learning,” in <i>Proceeding of the International Conference on Real-time Computing and Robotics</i>, 2019, pp. 112–117.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark154">[121] K. Lv, X. Pei, C. Chen, and J. Xu, “A  safe  and  efficient  lane change decision-making strategy of autonomous driving based on deep reinforcement learning,” </a><i>Mathematics</i>, vol. 10, no. 9, p. 1551, 2022.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[122] R. Ra˘dulescu, M. Legrand <i>et al.</i>, “Deep multi-agent reinforcement learning in a homogeneous open population,” in <i>Artificial Intelligence: 30th Benelux Conference, BNAIC 2018,‘s-Hertogenbosch, The Nether- lands, November 8–9, 2018, Revised Selected Papers 30</i>.    Springer,</p><p class="s8" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark155">2019, pp. 90–105.</a></p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark156">[123] C. Yu, X. Wang </a><i>et al.</i>, “Distributed multiagent coordinated learning for autonomous driving in highways based on dynamic coordination graphs,” <i>IEEE Transactions on Intelligent  Transportation  Systems</i>, vol. 21, no. 2, pp. 735–748, 2020.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark157">[124] M. Kaushik, N. Singhania </a><i>et al.</i>, “Parameter sharing reinforcement learning architecture for multi agent driving,” in <i>Proceedings of the 2019 4th International Conference on Advances in Robotics</i>, 2019, pp. 1–7.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark158">[125] M. Molaie, A. Amirkhani </a><i>et al.</i>, “Auto-driving policies in highway based on distributional deep reinforcement learning,” in <i>Proceeding of the International Conference on Pattern Recognition and Image Analysis</i>, 2021, pp. 1–6.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark159">[126] G. Chen, Y. Zhang </a><i>et al.</i>, “Attention-based highway safety planner for autonomous driving via deep reinforcement learning,” <i>IEEE Transac- tions on Vehicular Technology</i>, vol. 73, no. 1, pp. 162–175, 2024.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark160">[127] K. Min, H. Kim </a><i>et al.</i>, “Deep distributional reinforcement learning based high-level driving policy determination,” <i>IEEE Transactions on Intelligent Vehicles</i>, vol. 4, no. 3, pp. 416–424, 2019.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[128] B. Gangopadhyay, H. Soora <i>et al.</i>, “Hierarchical program-triggered re- inforcement learning agents for automated driving,” <i>IEEE Transactions</i></p><p class="s67" style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;">on Intelligent Transportation Systems<span class="s8">, vol. 23, no. 8, pp. 10 902–10 911,</span></p><p class="s8" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark161">2022.</a></p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark162">[129] S. Cheng, B. Yang, Z. Wang, and K.  Nakano,  “Spatio-temporal image representation and deep-learning-based decision framework for automated vehicles,” </a><i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 23, no. 12, pp. 24 866–24 875, 2022.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark163">[130] B. Liu, Y. Tang </a><i>et al.</i>, “A deep reinforcement learning approach for ramp metering based on traffic video data,” <i>Journal of Advanced Transportation</i>, vol. 2021, no. 1, p. 6669028, 2021.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark164">[131] M. Yang, Z. Li </a><i>et al.</i>, “A deep reinforcement learning-based ramp metering control framework for improving traffic operation at freeway weaving sections,” in <i>Proceedings of the Transportation Research Board Annual Meeting, Washington, DC, USA</i>, 2019, pp. 13–17.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark165">[132] P. Wang and C.-Y. Chan, “Formulation of deep reinforcement learning architecture toward autonomous driving for on-ramp merge,” in </a><i>Pro- ceedings of the International Conference on Intelligent Transportation Systems</i>, 2017, pp. 1–6.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark166">[133] Y. Lin, J. McPhee </a><i>et al.</i>, “Anti-jerk on-ramp merging using deep reinforcement learning,” in <i>Proceedings of the Intelligent Vehicles Symposium</i>.   IEEE, 2020, pp. 7–14.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark167">[134] F. Deng, J. Jin </a><i>et al.</i>, “Advanced self-improving ramp metering algo- rithm based on multi-agent deep reinforcement learning,” in <i>Proceeding of the IEEE Intelligent Transportation Systems Conference</i>, 2019, pp. 3804–3809.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark168">[135] M. Cheng, C. Zhang </a><i>et al.</i>,  “Adaptive  coordinated  variable  speed limit between highway mainline and on-ramp with deep reinforcement learning,” <i>Journal of Advanced Transportation</i>, vol. 2022, no. 1, p. 2435643, 2022.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark169">[136] S. Zhou, W. Zhuang </a><i>et al.</i>, “Cooperative on-ramp merging control of connected and automated vehicles: Distributed multi-agent deep rein- forcement learning approach,” in <i>Proceeding of the IEEE International Conference on Intelligent Transportation Systems</i>, 2022, pp. 402–408.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark170">[137] Z. Hu and W. Ma, “Guided deep reinforcement learning for coordi- nated ramp metering and perimeter control in large scale networks,” </a><i>Transportation Research Part C: Emerging Technologies</i>, vol. 159, p. 104461, 2024.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark171">[138] D. Deng, B. Yu </a><i>et al.</i>, “Automated traffic state optimization in the weaving area of urban expressways by a reinforcement learning-based cooperative method of channelization and ramp metering,” <i>Journal of Advanced Transportation</i>, vol. 2023, no. 1, p. 4771946, 2023.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[139] C. Wang, Y. Xu <i>et al.</i>, “Integrated traffic control for freeway recurrent bottleneck based on deep reinforcement learning,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 23, no. 9, pp. 15 522–15 535,</p><p class="s8" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark172">2022.</a></p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark173">[140] X. Qi, L. Zhang </a><i>et al.</i>, “Learning-based mpc for autonomous motion planning at freeway off-ramp diverging,” <i>IEEE Transactions on Intel- ligent Vehicles</i>, pp. 1–11, 2024.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark174">[141] Z. e. a. Kherroubi, S. Aknine, and R. Bacha, “Novel decision-making strategy for connected and autonomous vehicles in highway on-ramp merging,” </a><i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 23, no. 8, pp. 12 490–12 502, 2022.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[142] X. Zhang, L. Wu <i>et al.</i>, “High-speed ramp merging behavior decision for autonomous vehicles based on multiagent reinforcement learning,” <i>IEEE Internet of Things Journal</i>, vol. 10, no. 24, pp. 22 664–22 672,</p><p class="s8" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark175">2023.</a></p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[143] D. Chen, M. R. Hajidavalloo <i>et al.</i>, “Deep multi-agent reinforcement learning for highway on-ramp merging in mixed traffic,” <i>IEEE Transac- tions on Intelligent Transportation Systems</i>, vol. 24, no. 11, pp. 11 623–</p><p class="s8" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark176">11 638, 2023.</a></p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark177">[144] X. He, B. Lou </a><i>et al.</i>, “Robust decision making for autonomous vehicles at highway on-ramps: A constrained adversarial reinforcement learning approach,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 24, no. 4, pp. 4103–4113, 2023.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark178">[145] M. Li, Z. Li </a><i>et al.</i>, “Enhancing cooperation of vehicle merging control in heavy traffic using communication-based soft actor-critic algorithm,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 24, no. 6, pp. 6491–6506, 2023.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark179">[146] J. Chen, B. Yuan </a><i>et al.</i>, “Model-free deep reinforcement learning for urban autonomous driving,” in <i>Proceeding of the IEEE Intelligent Transportation Systems Conference</i>.   IEEE, 2019, pp. 2765–2771.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark180">[147] G. Bacchiani, D. Molinari, and M. Patander, “Microscopic traffic simulation by cooperative multi-agent deep reinforcement learning,”  </a><i>arXiv preprint arXiv:1903.01365</i>, 2019.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[148] B. Montgomery, C. Muise <i>et al.</i>, “Hierarchical deep reinforcement learning with cross-attention and planning for autonomous roundabout navigation,” in <i>Proceeding of the Canadian Conference on Electrical and Computer Engineering</i>, 2024, pp. 417–423.</p><p class="s8" style="padding-top: 3pt;padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark181">[149] Y. Tian, M. Han, L. Zhang, W. Liu, J. Wang, and W. Pan, “Variational constrained reinforcement learning with application to planning at roundabout,” 2020.</a><a name="bookmark182">&zwnj;</a></p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark183">[150] W. Wang, L. Jiang </a><i>et al.</i>, “Imitation learning based decision-making for autonomous vehicle control at traffic roundabouts,” <i>Multimedia Tools and Applications</i>, vol. 81, no. 28, pp. 39 873–39 889, 2022.</p><p class="s8" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">[151]  L. Ferrarotti, M. Luca, G. Santin, G. Previati, G. Mastinu, M. Gobbi,</p><p class="s8" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark184">E. Campi, L. Uccello, A. Albanese, P. Zalaya </a><i>et al.</i>, “Autonomous and human-driven vehicles interacting in a roundabout: A quantitative and qualitative evaluation,” <i>IEEE Access</i>, 2024.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark185">[152] Y. Zhang, B. Gao </a><i>et al.</i>, “Adaptive decision-making for automated vehicles under roundabout scenarios using optimization embedded reinforcement learning,” <i>IEEE Transactions on Neural Networks and Learning Systems</i>, vol. 32, no. 12, pp. 5526–5538, 2021.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark186">[153] F. Konstantinidis, M. Sackmann, O. De Candido, U. Hofmann, J. Thi- elecke, and W. Utschick, “Parameter sharing reinforcement learning for modeling multi-agent driving behavior in roundabout scenarios,” in </a><i>Proceeding of the IEEE Intelligent Transportation Systems Conference</i>, 2021, pp. 1974–1981.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark187">[154] R. Nakaya, T. Harada </a><i>et al.</i>, “Emergence of cooperative automated driving control at roundabouts using deep reinforcement learning,” in <i>Proceeding of the Annual Conference of the Society of Instrument and Control Engineers</i>, 2023, pp. 97–102.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark188">[155] H. Yuan, P. Li </a><i>et al.</i>, “Safe, efficient, comfort, and energy-saving automated driving through roundabout based on deep reinforcement learning,” in <i>Proceeding of the IEEE International Conference on Intelligent Transportation Systems</i>.   IEEE, 2023, pp. 6074–6079.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark189">[156] W. Wang,  F.  Hui  </a><i>et  al.</i>, “Deep  reinforcement  learning  method  for trajectory planning of connected and autonomous vehicles in the roundabout lane-changing scenario,” in <i>Proceeding of the International Symposium on Computer Technology and Information Science</i>, 2024, pp. 168–173.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark190">[157] A. P. Capasso, G. Bacchiani </a><i>et al.</i>, “From simulation to real world maneuver execution using deep reinforcement learning,” in <i>Proceeding of the IEEE Intelligent Vehicles Symposium</i>. IEEE, 2020, pp. 1570– 1575.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark191">[158] ——, “Intelligent roundabout insertion using deep reinforcement learn- ing,” </a><i>arXiv preprint arXiv:2001.00786</i>, 2020.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark192">[159] S. Alighanbari and N. L. Azad, “Deep reinforcement learning with nmpc assistance nash switching for urban autonomous driving,” </a><i>IEEE Transactions on Intelligent Vehicles</i>, vol. 8, no. 3, pp. 2604–2615, 2023.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark193">[160] B. Peng, M. F. Keskin </a><i>et al.</i>, “Connected autonomous vehicles for im- proving mixed traffic efficiency in unsignalized intersections with deep reinforcement learning,” <i>Communications in Transportation Research</i>, vol. 1, p. 100017, 2021.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark194">[161] A. Pozzi, S. Bae, Y. Choi, F. Borrelli, D. M. Raimondo, and S. Moura, “Ecological velocity planning through signalized intersections: A deep reinforcement learning approach,” in </a><i>Proceeding of the IEEE Confer- ence on Decision and Control</i>.   IEEE, 2020, pp. 245–252.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark195">[162] K.-F. Chu, A. Y. S. Lam, and V. O. K. Li, “Traffic signal control using end-to-end off-policy deep reinforcement learning,” </a><i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 23, no. 7, pp. 7184–7195, 2022.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark196">[163]  D. Quang Tran and S.-H. Bae, “Proximal policy optimization through a deep  reinforcement  learning framework  for  multiple autonomous vehicles at a non-signalized intersection,” </a><i>Applied Sciences</i>, vol. 10, no. 16, p. 5722, 2020.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[164] Z. Bai, P. Hao, W. Shangguan, B. Cai, and M. J. Barth, “Hybrid reinforcement learning-based eco-driving strategy for connected and automated vehicles at signalized intersections,” <i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 23, no. 9, pp. 15 850–15 863,</p><p class="s8" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark197">2022.</a></p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark198">[165] R. Bautista-Montesano, R. Galluzzi </a><i>et al.</i>, “Autonomous navigation at unsignalized intersections: A coupled reinforcement learning and model predictive control approach,” <i>Transportation Research Part C: Emerging Technologies</i>, vol. 139, p. 103662, 2022.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark199">[166] D. Li, F. Zhu, T. Chen, Y. D. Wong, C. Zhu, and J. Wu, “Coor- plt: A hierarchical control model for coordinating adaptive platoons of connected and autonomous vehicles at signal-free intersections based on deep reinforcement learning,” </a><i>Transportation Research Part C: Emerging Technologies</i>, vol. 146, p. 103933, 2023.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark200">[167] S. Kai, B. Wang </a><i>et al.</i>, “A multi-task reinforcement learning approach for navigating unsignalized intersections,” in <i>Proceeding of the IEEE Intelligent Vehicles Symposium</i>, 2020, pp. 1583–1588.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[168] B. Zhou, Q. Zhou, S. Hu, D. Ma, S. Jin, and D.-H. Lee, “Cooperative traffic signal control using a distributed agent-based deep reinforce- ment learning with incentive communication,” <i>IEEE Transactions on</i></p><p class="s67" style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Intelligent Transportation Systems<span class="s8">, vol. 25, no. 8, pp. 10 147–10 160,</span></p><p class="s8" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark201">2024.</a></p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark202">[169] W. X. Hu, H. Ishihara, C. Chen, A. Shalaby, and B. Abdulhai, “Deep reinforcement learning two-way transit signal priority algorithm for optimizing headway adherence and speed,” </a><i>IEEE Transactions on Intelligent Transportation Systems</i>, vol. 24, no. 8, pp. 7920–7931, 2023.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark203">[170] A. Lombard, A. Noubli, A. Abbas-Turki, N. Gaud, and S. Galland, “Deep reinforcement learning approach for v2x managed intersections of connected vehicles,” </a><i>IEEE Transactions on Intelligent Transporta- tion Systems</i>, vol. 24, no. 7, pp. 7178–7189, 2023.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark204">[171] D. Li, J. Wu </a><i>et al.</i>, “Adaptive traffic signal control model on inter- sections based on deep reinforcement learning,” <i>Journal of Advanced Transportation</i>, vol. 2020, no. 1, p. 6505893, 2020.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark205">[172] H. Shu, T. Liu, X. Mu, and D. Cao, “Driving tasks transfer using deep reinforcement learning for decision-making of autonomous vehicles in unsignalized intersection,” </a><i>IEEE Transactions on Vehicular Technology</i>, vol. 71, no. 1, pp. 41–52, 2021.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark206">[173] H. Seong, C. Jung, S. Lee, and D. H. Shim, “Learning to drive at unsignalized intersections using attention-based deep reinforcement learning,” in </a><i>Proceeding of the IEEE International Intelligent Trans- portation Systems Conference</i>, 2021, pp. 559–566.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark207">[174] C.-J. Hoel, T. Tram, and J. Sjo¨berg, “Reinforcement learning with uncertainty estimation for tactical decision-making in intersections,” in </a><i>Proceeding of  the IEEE  International Conference on  Intelligent Transportation Systems</i>.   IEEE, 2020, pp. 1–7.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark208">[175] G.-P. Antonio and C. Maria-Dolores, “Multi-agent deep reinforcement learning to manage connected autonomous vehicles at tomorrow’s intersections,” </a><i>IEEE Transactions on Vehicular Technology</i>, vol. 71, no. 7, pp. 7033–7043, 2022.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark209">[176] W. Xiao, Y. Yang, X. Mu,  Y. Xie, X. Tang, D. Cao, and T. Liu, “Decision-making for autonomous vehicles in random task scenarios at unsignalized intersection using deep reinforcement learning,” </a><i>IEEE Transactions on Vehicular Technology</i>, vol. 73, no. 6, pp. 7812–7825, 2024.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark210">[177] Z. Guo, Y. Wu, L. Wang, and J. Zhang,  “Coordination  for  con- nected and automated vehicles at non-signalized intersections: A value decomposition-based multiagent deep reinforcement learning approach,” </a><i>IEEE Transactions on Vehicular Technology</i>, vol. 72, no. 3, pp. 3025–3034, 2023.</p><p class="s8" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[178] I. Osband, J. Aslanides, and A. Cassirer, “Randomized prior functions for deep reinforcement learning,” <i>Advances in Neural Information Processing Systems</i>, vol. 31, 2018.</p></body></html>
