<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Traffic Signal Control via Reinforcement Learning: A Review on Applications and Innovations</title><meta name="author" content="Panagiotis Michailidis, Iakovos Michailidis, Charalampos Rafail Lazaridis and Elias Kosmatopoulos"/><meta name="keywords" content="reinforcement learning; traffic management; traffic signal control; adaptive control; model-free control; intelligent transportation"/><meta name="description" content="Traffic signal control plays a pivotal role in intelligent transportation systems, directly affecting urban mobility, congestion mitigation, and environmental sustainability. As traffic networks become more dynamic and complex, traditional strategies such as fixed-time and actuated control increasingly fall short in addressing real-time variability. In response, adaptive signal control—powered predominantly by reinforcement learning—has emerged as a promising data-driven solution for optimizing signal operations in evolving traffic environments. The current review presents a comprehensive analysis of high-impact reinforcement-learning-based traffic signal control methods, evaluating their contributions across numerous key dimensions: methodology type, multi-agent architectures, reward design, performance evaluation, baseline comparison, network scale, practical applicability, and simulation platforms. Through a systematic examination of the most influential studies, the review identifies dominant trends, unresolved challenges, and strategic directions for future research. The findings underscore the transformative potential of RL in enabling intelligent, responsive, and sustainable traffic management systems, marking a significant shift toward next-generation urban mobility solutions."/><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s2 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 h1 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 18pt; }
 .h3, h3 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s3 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s4 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s5 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s6 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s7 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; }
 .p, p { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 .s8 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s9 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; }
 .s10 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s11 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 h2 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s12 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s13 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 h4 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s14 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s15 { color: #0774B7; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s16 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s18 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s19 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s20 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s21 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s22 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s23 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s24 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s25 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s26 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s27 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -1pt; }
 .s28 { color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -1pt; }
 .s29 { color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 4pt; }
 .s30 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s31 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -2pt; }
 .s32 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s33 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s34 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s35 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 4pt; }
 .s36 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 4pt; }
 .s37 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s38 { color: black; font-family:"B Yagut"; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 11pt; }
 .s39 { color: black; font-family:"B Yagut"; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s40 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 4pt; }
 .s41 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 5pt; }
 .s42 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 4pt; }
 .s43 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s44 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 5pt; }
 .s45 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 5pt; }
 .s46 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -1pt; }
 .s47 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s48 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -2pt; }
 .s49 { color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; vertical-align: -2pt; }
 .s50 { color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .s51 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -1pt; }
 .s52 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -3pt; }
 .s53 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s54 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s55 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s57 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s59 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s60 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s61 { color: #0774B7; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 0; }
 #l1> li:before {counter-increment: c1; content: counter(c1, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l2 {padding-left: 0pt;counter-reset: c2 0; }
 #l2> li:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l3 {padding-left: 0pt;counter-reset: d1 0; }
 #l3> li:before {counter-increment: d1; content: counter(d1, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l4 {padding-left: 0pt;counter-reset: c2 0; }
 #l4> li:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l5 {padding-left: 0pt;counter-reset: e1 0; }
 #l5> li:before {counter-increment: e1; content: counter(e1, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l6 {padding-left: 0pt; }
 #l6> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l7 {padding-left: 0pt; }
 #l7> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l8 {padding-left: 0pt; }
 #l8> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l9 {padding-left: 0pt;counter-reset: f1 0; }
 #l9> li:before {counter-increment: f1; content: counter(f1, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l10 {padding-left: 0pt;counter-reset: g1 2; }
 #l10> li:before {counter-increment: g1; content: counter(g1, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l11 {padding-left: 0pt;counter-reset: g2 0; }
 #l11> li:before {counter-increment: g2; content: counter(g1, decimal)"."counter(g2, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l12 {padding-left: 0pt;counter-reset: g3 0; }
 #l12> li:before {counter-increment: g3; content: counter(g1, decimal)"."counter(g2, decimal)"."counter(g3, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l13 {padding-left: 0pt; }
 #l13> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l14 {padding-left: 0pt; }
 #l14> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l15 {padding-left: 0pt; }
 #l15> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l16 {padding-left: 0pt;counter-reset: g2 0; }
 #l16> li:before {counter-increment: g2; content: counter(g1, decimal)"."counter(g2, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l17 {padding-left: 0pt; }
 #l17> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l18 {padding-left: 0pt; }
 #l18> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l19 {padding-left: 0pt;counter-reset: i1 0; }
 #l19> li:before {counter-increment: i1; content: counter(i1, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l20 {padding-left: 0pt;counter-reset: j1 5; }
 #l20> li:before {counter-increment: j1; content: counter(j1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l21 {padding-left: 0pt;counter-reset: j2 0; }
 #l21> li:before {counter-increment: j2; content: counter(j1, decimal)"."counter(j2, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l22 {padding-left: 0pt; }
 #l22> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l23 {padding-left: 0pt; }
 #l23> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l24 {padding-left: 0pt; }
 #l24> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l25 {padding-left: 0pt; }
 #l25> li:before {content: "• "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l26 {padding-left: 0pt;counter-reset: k1 5; }
 #l26> li:before {counter-increment: k1; content: counter(k1, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l27 {padding-left: 0pt;counter-reset: k2 0; }
 #l27> li:before {counter-increment: k2; content: counter(k1, decimal)"."counter(k2, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l28 {padding-left: 0pt; }
 #l28> li:before {content: "✓ "; color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l29 {padding-left: 0pt; }
 #l29> li:before {content: "■ "; color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l30 {padding-left: 0pt;counter-reset: n1 0; }
 #l30> li:before {counter-increment: n1; content: counter(n1, decimal)". "; color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="46" height="46" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_001.png"/></span>	<span><img width="144" height="22" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_002.png"/></span>	<span><img width="56" height="37" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_003.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="698" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_004.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Review</p><h1 style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 21pt;text-align: left;">Traffic Signal Control via Reinforcement Learning: A Review on Applications and Innovations</h1><h3 style="padding-top: 11pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Panagiotis Michailidis <span class="s3">1,2,</span>*<span><img width="12" height="12" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_005.png"/></span>, Iakovos Michailidis <span class="s3">1,2</span><span><img width="12" height="12" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_006.png"/></span>, Charalampos Rafail Lazaridis <span class="s3">1,2</span><span><img width="12" height="12" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_007.png"/></span></h3><h3 style="padding-left: 5pt;text-indent: 0pt;text-align: left;">and Elias Kosmatopoulos <span class="s3">1,2</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 136pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">1     <span class="s5"> </span><span class="s6">Center for Research and Technology Hellas, 57001 Thessaloniki, Greece</span></p><p class="s4" style="padding-left: 136pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">2     <span class="s5"> </span><span class="s6">Electrical and Computer Engineering, Democritus University of Thrace, 67100 Xanthi, Greece</span></p><p class="s7" style="padding-left: 135pt;text-indent: 0pt;text-align: justify;">*    <span class="s6">Correspondence: panosmih@iti.gr; Tel.: +30-2310-464160</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 136pt;text-indent: 0pt;line-height: 112%;text-align: justify;">Abstract: <span class="p">Traffic signal control plays a pivotal role in intelligent transportation systems, directly affecting urban mobility, congestion mitigation, and environmental sustainability. As traffic networks become more dynamic and complex, traditional strategies such as fixed-time and actuated control increasingly fall short in addressing real-time variability. In response, adaptive signal control—powered predominantly by reinforcement learning—has emerged as a promising data-driven solution for optimizing signal operations in evolving traffic environments. The current review presents a comprehensive analysis of high-impact reinforcement-learning-based traffic signal control methods, evaluating their contributions across numerous key dimensions: methodology type, multi-agent architectures, reward design, performance evaluation, baseline comparison, network scale, practical applicability, and simulation platforms. Through a systematic examination of the most influential studies, the review identifies dominant trends, unresolved challenges, and strategic directions for future research. The findings underscore the transformative potential of RL in enabling intelligent, responsive, and sustainable traffic management systems, marking a significant shift toward next-generation urban mobility solutions.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="68" height="23" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_008.png"/></span></p><h3 style="padding-left: 136pt;text-indent: 0pt;line-height: 112%;text-align: left;">Keywords: <span class="p">reinforcement learning; traffic management; traffic signal control; adaptive control; model-free control; intelligent transportation</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 136pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="525" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_009.png"/></span></p><p class="s8" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Academic Editor: Benedetto Barabino</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Received: 31 March 2025</p><p class="s8" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Revised: 1 May 2025</p><p class="s8" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Accepted: 2 May 2025</p><p class="s8" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Published: 6 May 2025</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s9" style="padding-left: 5pt;text-indent: 0pt;line-height: 137%;text-align: left;">Citation:  <span class="s8">Michailidis, P.; Michailidis, I.; Lazaridis, C.R.; Kosmatopoulos, E. Traffic Signal Control via Reinforcement Learning: A Review on Applications and Innovations.</span></p><p class="s10" style="padding-left: 5pt;text-indent: 0pt;line-height: 137%;text-align: left;">Infrastructures <b>2025</b><span class="s8">, </span>10<a href="https://doi.org/10.3390/infrastructures10050114" class="s11" target="_blank">, 114. https://doi.org/10.3390/ infrastructures10050114</a></p><p class="s9" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 137%;text-align: left;">Copyright: <a href="https://creativecommons.org/licenses/by/4.0/" class="s11" target="_blank">© 2025 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/ </a><span class="s8">licenses/by/4.0/).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l1"><li style="padding-left: 18pt;text-indent: -12pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark0">Introduction</a><a name="bookmark6">&zwnj;</a></h2><ol id="l2"><li style="padding-top: 3pt;padding-left: 23pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark1">Motivation</a></p><p class="s15" style="padding-top: 4pt;padding-left: 5pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark67" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The growing complexity of modern cyber-physical systems demands adaptive control mechanisms that ensure efficiency, resilience, and real-time responsiveness. Such mech- anisms are increasingly employed across advanced IoT ecosystems, including building automation [</a>1<a href="#bookmark68" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>5<a href="#bookmark69" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], manufacturing lines [</a>6<a href="#bookmark70" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>8<a href="#bookmark71" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and robotic procedures [</a>9<a href="#bookmark72" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>11<a href="#bookmark73" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], where systems often encounter unpredictable dynamics. In such contexts, traditional control approaches frequently fall short, necessitating more adaptable strategies capable of responding dy- namically to evolving operational conditions. Traffic networks constitute another complex cyber-physical IoT domain where real-time disruptions—ranging from congestion and accidents to fluctuating pedestrian activity—pose significant control challenges. Urbaniza- tion and rising vehicle volumes further strain conventional coordination methods, such as fixed-time (FT) and actuated signal control (AC), which lack the flexibility to accommodate such dynamic environments [</a>12<a href="#bookmark74" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>15<a href="#bookmark75" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. This growing demand for real-time, intelligent signal management has driven the adoption of adaptive traffic signal control frameworks aimed at mitigating economic losses, reducing emissions, and addressing broader inefficiencies in urban mobility [</a>16<a href="#bookmark77" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>19<span style=" color: #000;">].</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="699" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_010.png"/></span></p><p class="s12" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Infrastructures <b>2025</b><span class="s6">, </span>10<a href="https://doi.org/10.3390/infrastructures10050114" class="s13" target="_blank">, 114                                                                                                                              https://doi.org/10.3390/infrastructures10050114</a></p><p class="s15" style="padding-top: 2pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: right;"><a href="#bookmark76" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Traffic signal control (TSC) has attracted research attention since the mid-20th century, initially relying on fixed-time scheduling based on historical data [</a>17<a href="#bookmark78" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>20<a href="#bookmark79" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Later, actuated systems emerged, using sensor feedback to make localized adjustments. However, these conventional strategies often struggle under irregular traffic conditions such as unplanned congestion, events, or accidents [</a>21<a href="#bookmark80" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>23<a href="#bookmark81" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Advances in AI and machine learning have since paved the way for data-driven systems capable of real-time, self-adaptive control [</a>24<a href="#bookmark82" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>31<span style=" color: #000;">].</span></p><p class="s15" style="padding-left: 81pt;text-indent: 21pt;line-height: 112%;text-align: right;"><a href="#bookmark73" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Such adaptive traffic signal control (ATSC) frameworks may be broadly categorized as either model-based or model-free.   Model-based approaches employ mathematical models and optimization algorithms that integrate both real-time sensor input and his- torical trends [</a>12<a href="#bookmark78" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>20<a href="#bookmark83" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>32<a href="#bookmark84" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>34<a href="#bookmark85" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. In TSC contexts, such practices optimize signal timing across interconnected intersections based on predicted flow patterns [</a>35<a href="#bookmark86" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>37<a href="#bookmark87" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. In contrast, model- free methods utilize machine learning to derive optimal control strategies through inter- action, without relying on predefined system models [</a>38<a href="#bookmark88" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">].  Such systems learn control policies directly from traffic data and real-time environmental feedback [</a>39<a href="#bookmark89" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Reinforce- ment learning (RL) encompasses both model-free and model-based approaches, enabling agents to either learn optimal behaviors directly through interaction or by utilizing pre- dictive models of the environment to guide decision making. In the context of large-scale cyber-physical systems, model-free RL approaches have gained particular prominence due to their flexibility and ability to handle complex, dynamic urban environments. To this end, the applications of model-free RL range from smart building operation [</a>40<a href="#bookmark91" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>44<a href="#bookmark92" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] and industrial automation [</a>45<a href="#bookmark93" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>48<a href="#bookmark94" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] to robotic coordination [</a>49<a href="#bookmark95" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>52<a href="#bookmark75" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] and traffic network management [</a>16<a href="#bookmark96" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>53<a href="#bookmark98" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>55<a href="#bookmark73" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">].  Unlike traditional control frameworks, RL enables continuous adaptation, aiming to enhance traffic efficiency by reducing delays, minimizing stops, and improving overall flow. Early RL applications in traffic control typically focused on isolated intersections with single-agent designs. As the field matured, advanced techniques—such as deep reinforcement learning (DRL) and multi-agent RL (MARL) frameworks—were introduced to address the increasing coordination complexities across entire traffic net- works [</a>12<a href="#bookmark75" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>16<a href="#bookmark75" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">].  Over the past decade, such advanced RL-based methods have evolved substantially, incorporating richer state representations, refined reward functions, and col- laborative learning strategies for managing diverse, large-scale traffic systems [</a>16<a href="#bookmark96" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>53<a href="#bookmark97" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>54<a href="#bookmark99" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>56<span style=" color: #000;">]. In light of the growing need for intelligent traffic management, the current review systematically examines the most significant RL-based approaches developed between 2015 and 2025, with a specific focus on urban traffic signal optimization at intersections. It covers a wide range of aspects, including RL algorithms and control philosophies, agent structures, baseline control strategies, reward function architectures, performance indicator types, intersection classifications, simulation tools, and training methodologies. Through statistical analysis, the study identifies prevailing trends, uncovers key challenges, and</span></p><p style="padding-left: 82pt;text-indent: 0pt;text-align: left;">highlights future research directions to advance the field.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark2">Literature Analysis Approach</a></p><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">This review analyzes the key principles, RL frameworks, optimization strategies, and performance outcomes of RL-based traffic signal management approaches developed over the past decade. To ensure a structured and meaningful analysis, studies are categorized by RL methodology (value-based, policy-based, and actor-critic), training paradigms, inter- section control schemes, and coordination strategies in multi-agent systems. Additionally, aspects such as simulation environments, evaluation metrics, and real-world applicability are examined to offer a holistic perspective on RL-driven traffic signal optimization. In detail, the literature analysis approach may be described by the following steps:</p><ol id="l3"><li style="padding-top: 4pt;padding-left: 105pt;text-indent: -23pt;line-height: 112%;text-align: right;"><h3 style="display: inline;">Study Selection:   <span class="p">To ensure the scientific robustness of this review, a thorough se- lection methodology was applied, drawing exclusively from Scopus—and Web of</span></h3><p style="padding-top: 2pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;">Science—indexed peer-reviewed journals and conferences. An initial set of over 250 publications was screened based on abstracts, from which the most pertinent studies were shortlisted for full analysis. The selection process adhered to multiple quality criteria: <b>(a) </b>Citation Threshold: Only studies with at least 30 citations, excluding self-citations, were considered to guarantee recognized academic impact, verified through Scopus at the time of review. In addition, given their recent publication, the studies from 2023 and 2024 constituted an exception, with their threshold set at 10 citations. <b>(b) </b>Topical Focus: Only research directly addressing RL-driven traffic signal control was included, excluding unrelated works on traffic estimation, non- RL-based signal systems, or broader urban mobility management. <b>(c) </b>Peer-Review Status: Only fully peer-reviewed articles and top-tier conference papers from pub- lishers like Elsevier, IEEE, MDPI, and Springer were selected, with pre-prints and non-peer-reviewed studies omitted. <b>(d) </b>Methodological Transparency: Papers were required to clearly document their RL setup, control objectives, evaluation metrics, and comparative benchmarking. <b>(e) </b>Methodological Diversity: A balanced repre- sentation of value-based, policy-based, and actor-critic RL research applications was maintained, covering varying signal traffic control cases</p></li><li style="padding-left: 104pt;text-indent: -22pt;line-height: 112%;text-align: justify;"><p style="display: inline;"><b>Keyword Strategy: </b>A targeted keyword strategy was designed to maintain focus on reinforcement learning applications for traffic signal control. Primary search terms included “<i>Reinforcement Learning for Traffic Signal Control</i>”, “<i>RL-based Traffic Light Opti- mization</i>”, “<i>Adaptive Traffic Signal Control using RL</i>”, “<i>Deep Reinforcement Learning for Traffic Signal Management</i>”, “<i>Reinforcement Learning for Traffic Signal Control</i>”, and “<i>RL- based Adaptive Traffic Management</i>”. Care was given to select phrases closely aligned with signal control tasks, avoiding broader traffic management or routing topics.</p></li><li style="padding-left: 105pt;text-indent: -22pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Data Categorization: <span class="p">Each selected study was carefully classified across multiple dimensions to enable thorough and structured analysis. The categorization included the RL methodology type and specific algorithms employed, the agent structure (single-agent vs. multi-agent control), the nature of the reward function, the per- formance metrics used for evaluation, and the baseline control strategies adopted for comparison. Furthermore, the scale of the traffic environment—whether single intersections or larger networks—was documented. Finally, the simulation platforms utilized for model training and validation were systematically recorded to assess the realism and comparability of the reported results.</span></h3></li><li style="padding-left: 105pt;text-indent: -22pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Quality Assessment: <span class="p">A systematic evaluation process was applied to ensure the inclusion of impactful and credible research. Specifically, citation metrics were used as a primary filter, requiring a minimum of 30 citations (excluding self-citations), with data sourced from Scopus to guarantee reliability. Studies published in high-impact journals and leading conference proceedings were favored. Beyond citation counts, the academic profile of the authors was assessed, focusing on their contributions to the fields of reinforcement learning and traffic signal control. Special emphasis was placed on authors with proven expertise, evidenced by publications in top-tier venues, contributions to the development or refinement of RL techniques for control problems, and affiliations with prominent transportation research centers or AI laboratories. This multi-faceted evaluation helped to prioritize studies offering substantial and credible advancements in RL-based traffic signal control.</span></h3></li><li style="padding-left: 105pt;text-indent: -23pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Findings Synthesis: <span class="p">Key insights were systematically organized into thematic cate- gories to enable clear comparisons across different RL methodologies applied to traffic signal control. This structured synthesis not only contributed to a comprehensive understanding of the field’s development but also enabled the generation of statistical analyses across different study attributes, as well as the identification of emerging</span></h3></li></ol><p style="padding-top: 2pt;padding-left: 105pt;text-indent: 0pt;line-height: 112%;text-align: justify;">trends and persistent challenges within the domain. Furthermore, it provided a solid foundation for formulating informed future research directions, highlighting gaps in current approaches and proposing potential advancements in RL-driven traffic signal control optimization.</p><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;text-align: justify;"><a href="#bookmark7" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The following Figure </a>1<span style=" color: #000;">, summarizes the above mentioned literature analysis steps:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="544" height="113" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_011.jpg"/></span></p><h4 style="padding-top: 2pt;padding-left: 82pt;text-indent: 0pt;text-align: left;"><a name="bookmark7">Figure 1. </a><span class="s14">Literature analysis approach.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark3">Previous Work</a></p><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark100" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Several reviews have examined various facets of traffic signal control, ranging from classical optimization techniques and intelligent transportation systems to modern machine- learning-based solutions. Many focus on reinforcement learning and its role in optimizing signal timing and traffic flow. Wang et al. [</a>57<a href="#bookmark101" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] offered a comprehensive overview of deep reinforcement learning applications in traffic signal control, analyzing DRL architectures, methodologies, and simulation tools. Their findings emphasized DRL’s strength in manag- ing high-dimensional state spaces and alleviating congestion. Yau et al. [</a>58<a href="#bookmark102" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] reviewed core RL algorithms for traffic signal control, focusing on representations of state, action, and reward, as well as computational efficiency. Their work identified major advancements, compared RL techniques, and outlined future directions. Miletic et al. [</a>59<a href="#bookmark103" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] provided an ex- tensive review of RL methods for adaptive traffic signal control, showcasing the advantages over traditional approaches and exploring multi-agent systems, DRL, and the influence of connected and autonomous vehicles (CAVs). The review concluded with open challenges and prospective research paths. Zhao et al. [</a>60<span style=" color: #000;">] examined recent developments in DRL for traffic signal control, organizing methods by algorithm type, model setup, and applica- tion scenario. Their study addressed previous survey gaps by analyzing state-of-the-art advancements from the past five years and highlighting future research opportunities in DRL-powered traffic management.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark4">Contribution and Novelty</a></p><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">This review provides a comprehensive and structured examination of RL applications in traffic signal control, distinguishing itself from the existing literature through several key contributions. Unlike previous reviews that broadly discuss AI-driven traffic management, this study focuses specifically on RL-based control frameworks designed to optimize traffic signal timing, coordination, and adaptation across urban road networks.</p><p style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">A significant novelty of this work concerns its large-scale, in-depth assessment of RL-based traffic signal control methodologies, systematically summarizing, classifying, and analyzing a substantial number of influential studies published between 2015 and 2025. To ensure a high-impact evaluation, the current review prioritizes research contributions that have gained significant citations and practical relevance (more than 20 citations according to Scopus), allowing for a rigorous synthesis of key findings and trends in RL-driven traffic management. This work begins by illustrating the mathematical foundations of the most prominent RL methodologies for traffic signal control. It then presents detailed summary tables of the most influential studies, enabling researchers and practitioners to efficiently compare RL techniques, performance metrics, and control strategies.</p><p style="padding-top: 2pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">Beyond summarizing existing approaches, this review identifies and classifies RL applications based on essential key elements, offering a structured comparison of different methodologies, based on methodology type, agent architectures, reward function analysis, baseline control, performance benchmarks, intersection type, and simulation environment. By going beyond a descriptive review, this study offers a critical evaluation of the concerned methodologies, providing statistical analysis for each evaluation attribute as well as valu- able insights into existing <b>challenges</b>, emerging <b>trends</b>, and potential <b>future directions </b><a href="#bookmark8" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">in RL-driven traffic signal control. The following Table </a><span style=" color: #0774B7;">1 </span>summarizes the different evaluation aspects that current and previous works analyze.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 82pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark8">Table 1. </a><span class="s14">Contribution comparison between current and previous works considering the different evaluation attributes.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:82.394pt" cellspacing="0"><tr style="height:17pt"><td style="width:150pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s16" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Evaluation Aspect</p></td><td style="width:39pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s16" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;"><a href="#bookmark100" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt;">57</span>]</p></td><td style="width:39pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s16" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;"><a href="#bookmark101" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt;">58</span>]</p></td><td style="width:47pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s16" style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark102" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt;">59</span>]</p></td><td style="width:61pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s16" style="padding-top: 1pt;padding-right: 2pt;text-indent: 0pt;text-align: center;"><a href="#bookmark103" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt;">60</span>]</p></td><td style="width:57pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s16" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Current</p></td></tr><tr style="height:15pt"><td style="width:150pt;border-top-style:solid;border-top-width:1pt"><p class="s18" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">RL Methodology Types</p></td><td style="width:39pt;border-top-style:solid;border-top-width:1pt"><p class="s18" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">x</p></td><td style="width:39pt;border-top-style:solid;border-top-width:1pt"><p class="s18" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">x</p></td><td style="width:47pt;border-top-style:solid;border-top-width:1pt"><p class="s18" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">x</p></td><td style="width:61pt;border-top-style:solid;border-top-width:1pt"><p class="s18" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">x</p></td><td style="width:57pt;border-top-style:solid;border-top-width:1pt"><p class="s18" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">x</p></td></tr><tr style="height:12pt"><td style="width:150pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Agent Architectures</p></td><td style="width:39pt"/><td style="width:39pt"/><td style="width:47pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:61pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:57pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td></tr><tr style="height:12pt"><td style="width:150pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Reward Function Analysis</p></td><td style="width:39pt"/><td style="width:39pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:47pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:61pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:57pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td></tr><tr style="height:12pt"><td style="width:150pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Baseline Control Approaches</p></td><td style="width:39pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:39pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:47pt"/><td style="width:61pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:57pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td></tr><tr style="height:12pt"><td style="width:150pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Performance Indexes</p></td><td style="width:39pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:39pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:47pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:61pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:57pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td></tr><tr style="height:12pt"><td style="width:150pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Intersection Size</p></td><td style="width:39pt"/><td style="width:39pt"/><td style="width:47pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:61pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:57pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td></tr><tr style="height:12pt"><td style="width:150pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Practical Applicability</p></td><td style="width:39pt"/><td style="width:39pt"/><td style="width:47pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:61pt"/><td style="width:57pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td></tr><tr style="height:12pt"><td style="width:150pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Simulation Environments</p></td><td style="width:39pt"/><td style="width:39pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:47pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:61pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:57pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td></tr><tr style="height:12pt"><td style="width:150pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Statistical Analysis</p></td><td style="width:39pt"/><td style="width:39pt"/><td style="width:47pt"/><td style="width:61pt"/><td style="width:57pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td></tr><tr style="height:12pt"><td style="width:150pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Existing Challenges</p></td><td style="width:39pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:39pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:47pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:61pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:57pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td></tr><tr style="height:12pt"><td style="width:150pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Trend Identification</p></td><td style="width:39pt"/><td style="width:39pt"/><td style="width:47pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:61pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:57pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td></tr><tr style="height:12pt"><td style="width:150pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Future Directions</p></td><td style="width:39pt"/><td style="width:39pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:47pt"/><td style="width:61pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:57pt"><p class="s18" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td></tr><tr style="height:15pt"><td style="width:150pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Systematic Evaluation</p></td><td style="width:39pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s18" style="padding-left: 12pt;text-indent: 0pt;line-height: 12pt;text-align: left;">No</p></td><td style="width:39pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s18" style="padding-left: 12pt;text-indent: 0pt;line-height: 12pt;text-align: left;">No</p></td><td style="width:47pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Small</p></td><td style="width:61pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s18" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Medium</p></td><td style="width:57pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s18" style="padding-left: 16pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Large</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 81pt;text-indent: 21pt;line-height: 112%;text-align: justify;">Through this structured and in-depth approach, the current effort serves as a founda- tional resource for advancing intelligent, adaptive traffic signal management, paving the way for smarter, more efficient urban mobility solutions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark5">Paper Structure</a></p><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark9" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">As depicted in Figure </a>2<a href="#bookmark6" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, this paper is organized as follows: Section </a>1 <a href="#bookmark13" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">introduces the mo- tivation behind this review, outlines the methodology for literature analysis, examines prior research efforts, and highlights the key contributions and novelty of this study. Section </a>2 <a href="#bookmark22" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">provides an overview of RL fundamentals in the context of traffic signal control, discussing general RL-based strategies applied in intelligent transportation systems. Section </a>3 <a href="#bookmark29" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ex- plores the core RL methodologies used in traffic light signal management, presenting a generalized mathematical framework and categorizing approaches based on value-based, policy-based, and actor-critic algorithms. Section </a>4 <a href="#bookmark47" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">examines the most influential studies on RL-based traffic signal optimization from 2015 to 2025, summarizing their key charac- teristics and findings in tabular format for comparative analysis. Section </a>5 <a href="#bookmark63" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">provides an in-depth evaluation of RL-based traffic control methods, assessing their effectiveness based on critical key elements, including methodology type, multi-agent integration, reward function, baseline control, performance index, and simulation tools. Section </a>6 <a href="#bookmark65" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">identifies, summarizes, and analyzes the emerging trends and challenges in RL-driven traffic signal control, highlighting also the potential research directions for future advancements in the field. Section </a>7 <a href="#bookmark9" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">concludes the review by summarizing key insights and presenting final reflections on RL’s impact on adaptive and intelligent traffic management. Figure </a>2 <span style=" color: #000;">portrays the overall paper structure.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="522" height="157" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_012.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 3pt;padding-left: 82pt;text-indent: 0pt;text-align: left;"><a name="bookmark9">Figure 2. </a><span class="s14">Paper structure.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 95pt;text-indent: -12pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark10">Traffic Signal Control Approaches</a><a name="bookmark13">&zwnj;</a></h2><ol id="l4"><li style="padding-top: 3pt;padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark11">Primary TSC Control Types</a></p><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark14" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Traffic signal control frameworks play a critical role in managing urban mobility, en- suring safety, and optimizing intersection efficiency. Such traffic management approaches may be broadly classified into three main categories: fixed-time (pre-timed) control, ac- tuated control, and adaptive control, each comprising various subtypes (see Figure </a>3<span style=" color: #000;">). More specifically:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="524" height="212" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_013.jpg"/></span></p><h4 style="padding-top: 6pt;padding-left: 82pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark14">Figure 3.  </a><span class="s14">Traffic management systems classification:  fixed-time, actuated control, and adaptive control management systems.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l5"><li style="padding-left: 105pt;text-indent: -23pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Fixed-Time (Pre-Timed) Control: </span><a href="#bookmark104" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">This state-of-the-art approach operates on prede- termined schedules, assigning fixed durations to signal phases based on historical traffic patterns [</a>61<a href="#bookmark104" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. While effective in environments with consistent flows due to its simplicity, it lacks adaptability to real-time conditions. Consequently, it performs poorly during disruptions such as accidents, special events, or sudden demand surges. Subcategories include [</a>61<a href="#bookmark105" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>63<span style=" color: #000;">]:</span></p><ul id="l6"><li style="padding-top: 2pt;padding-left: 127pt;text-indent: -21pt;line-height: 112%;text-align: left;"><p class="s2" style="display: inline;">Isolated Fixed-Time Control: <span class="p">Each intersection follows a standalone schedule with no inter-coordination.</span></p></li><li style="padding-left: 126pt;text-indent: -21pt;line-height: 112%;text-align: left;"><p class="s2" style="display: inline;">Coordinated Fixed-Time Control: <span class="p">Intersections are synchronized to enable green wave progression along corridors.</span></p></li><li style="padding-left: 127pt;text-indent: -21pt;line-height: 112%;text-align: left;"><p class="s2" style="display: inline;">Time-of-Day Control: <span class="p">Schedules vary by time period but remain non-responsive to live traffic conditions.</span></p><p class="s15" style="padding-top: 2pt;padding-left: 105pt;text-indent: 0pt;line-height: 112%;text-align: left;"><a href="#bookmark106" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Despite its straightforwardness, FT control often leads to inefficiencies in dynamic traffic scenarios [</a>64<span style=" color: #000;">].</span></p></li></ul></li><li style="padding-left: 105pt;text-indent: -23pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Actuated Control: </span><a href="#bookmark107" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">These state-of-the-art systems adjust signal timings based on real- time inputs from sensors like loop detectors, cameras, or infrared devices [</a>65<a href="#bookmark108" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. They may be typically classified as [</a>66<a href="#bookmark109" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>67<span style=" color: #000;">]:</span></p><ul id="l7"><li style="padding-top: 2pt;padding-left: 127pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s2" style="display: inline;">Semi-Actuated Control: <a href="#bookmark110" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Detection is installed on minor roads, allowing the major road to remain green until a vehicle is detected on the minor approach [</a><span class="s15">68</span><span class="p">].</span></p></li><li style="padding-left: 127pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s2" style="display: inline;">Fully Actuated Control: <a href="#bookmark111" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Sensors monitor all approaches, enabling dynamic adjust- ments from all directions based on traffic conditions. This setup suits areas with unpredictable traffic and enhances flow compared to fixed-time systems [</a><span class="s15">69</span><span class="p">].</span></p><p class="s15" style="padding-top: 2pt;padding-left: 105pt;text-indent: 0pt;line-height: 112%;text-align: left;"><a href="#bookmark101" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">However, actuated systems operate locally and lack coordination across intersections, limiting their network-level optimization capabilities [</a>58<span style=" color: #000;">].</span></p></li></ul></li><li style="padding-left: 105pt;text-indent: -23pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Adaptive Control: </span><a href="#bookmark112" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Adaptive traffic signal control (ATSC) systems represent the most advanced form of signal management. They are able to continuously monitor traffic in real time and apply algorithmic models to optimize signals across networks [</a>70<a href="#bookmark88" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], aiming to reduce delays and adapt to fluctuating demand [</a>39<span style=" color: #000;">]. Subclasses include:</span></p><ul id="l8"><li style="padding-top: 4pt;padding-left: 126pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="s2">Model-Based Optimization Systems: </span><a href="#bookmark86" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">These rely on mathematical models to forecast traffic and adjust timings accordingly, using techniques such as model predic- tive control [</a>37<a href="#bookmark113" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>71<a href="#bookmark114" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], dynamic traffic assignment [</a>72<a href="#bookmark115" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>73<a href="#bookmark116" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], mixed-integer linear programming [</a>74<a href="#bookmark117" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>75<a href="#bookmark118" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and Petri nets [</a>76<span style=" color: #000;">].</span></p></li><li style="padding-left: 126pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="s2">Model-Free Optimization Systems: </span><a href="#bookmark119" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Employing AI methods like reinforcement learn- ing [</a>77<a href="#bookmark120" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>78<a href="#bookmark121" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], fuzzy logic [</a>79<a href="#bookmark122" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>80<a href="#bookmark123" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], evolutionary algorithms [</a>81<a href="#bookmark124" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>82<a href="#bookmark125" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and swarm intel- ligence [</a>83<a href="#bookmark126" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>85<a href="#bookmark127" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], these systems learn directly from traffic data, adjusting without predefined models [</a>86<span style=" color: #000;">].</span></p></li></ul></li></ol><p style="padding-top: 4pt;padding-left: 105pt;text-indent: 0pt;line-height: 112%;text-align: left;">Model-based systems are preferred for structured environments, while model-free methods excel in complex, dynamic scenarios.</p><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark14" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Figure </a><span style=" color: #0774B7;">3 </span>illustrates the classification of traffic management systems, with a focus on adaptive control and, specifically, on common model-free RL approaches. It presents key algorithm types—<b>value-based: </b>Q-learning (QL), deep Q-network (DQN), double deep Q-network (DDQN), dueling DQN; <b>policy-based: </b>proximal policy optimization (PPO), deterministic policy gradient (DPG), Monte Carlo policy gradient (REINFORCE), trust region policy optimization (TRPO); and <b>actor-critic: </b><a href="#bookmark22" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">deep deterministic policy gradient (DDPG), soft actor-critic (SAC), twin delayed DDPG (TD3), advantage actor-critic (A2C), asynchronous advantage actor-critic (A3C)). Section </a><span style=" color: #0774B7;">3 </span>provides a mathematical and concep- tual overview of such algorithms.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark12">General Concept of RL Control in TSC</a></p></li></ol></li></ol><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark15" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">In RL-based traffic signal control, the decision-making process is structured around key components that interact dynamically to optimize traffic flow. The overall operation of RL control within a traffic signal control may be described through the following key steps (see Figure </a>4<span style=" color: #000;">):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="529" height="212" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_014.jpg"/></span></p><h4 style="padding-top: 6pt;padding-left: 82pt;text-indent: 0pt;text-align: left;"><a name="bookmark15">Figure 4. </a><span class="s14">General concept of reinforcement learning in traffic signal control.</span></h4><ol id="l9"><li style="padding-top: 2pt;padding-left: 105pt;text-indent: -23pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Environment: <span class="p">The traffic network constitutes the environment in which the RL agent operates. It may include intersections, vehicle and pedestrian flows, sensors, and external disturbances like weather or sudden congestion. The RL agent does not control this environment directly but learns to respond to its dynamics by optimizing signal timings.</span></h3></li><li style="padding-left: 105pt;text-indent: -23pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Sensors: <span class="p">Real-time monitoring is enabled through loop detectors, video feeds, GPS data, LiDAR, and connected vehicle systems. Such sensors capture observa- tions—such as queue lengths, arrival rates, and pedestrian activity—that inform the RL agent’s decisions.</span></h3></li><li style="padding-left: 105pt;text-indent: -22pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">RL Agent(s): <span class="p">Acting as the system’s intelligence, the RL agent evaluates incoming traffic data and determines optimal signal phases. Depending on the method used (value-based, policy-based, or actor-critic), it learns to minimize congestion, delays, and emissions through continuous interaction with the environment.</span></h3></li><li style="padding-left: 105pt;text-indent: -23pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Control Decision Application: <span class="p">Once an action is selected, the system implements signal adjustments—such as extending green times, skipping phases, modifying cycle lengths, or coordinating multiple intersections—based on the agent’s output.</span></h3></li><li style="padding-left: 105pt;text-indent: -22pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Environment Update: <span class="p">After applying the control decisions, the environment evolves. Vehicles move, congestion shifts, and new traffic enters, resulting in a new state that becomes the input for the agent’s next decision.</span></h3></li><li style="padding-left: 105pt;text-indent: -23pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Reward Computation: <span class="p">The system evaluates the agent’s decision using prede- fined metrics. Typical reward functions assess reductions in waiting time, queue lengths, fuel consumption, emissions, and improvements in pedestrian safety and traffic balance.</span></h3></li><li style="padding-left: 105pt;text-indent: -23pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Reward Signal Feedback: <span class="p">The computed reward is fed back into the RL algorithm, refining its policy. Through this iterative process, the agent improves its control strategy, enabling real-time, adaptive signal management.</span></h3></li></ol><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">Through continuous learning and real-time feedback, RL-based traffic signal control frameworks surpass traditional FT and AC effectiveness—especially in large-scale dynamic traffic scenarios—offering more efficient, responsive, and sustainable urban traffic solutions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l10"><li style="padding-left: 95pt;text-indent: -12pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark16">Mathematical Framework of Reinforcement Learning</a><a name="bookmark22">&zwnj;</a></h2><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">RL frames traffic signal control as a Markov decision process (MDP), where an agent interacts with the traffic environment to learn optimal signal policies through trial-and-error. This is particularly advantageous for dynamic traffic conditions, allowing continuous adap- tation to real-time variations such as vehicle density, queue lengths, and arrival rates. Over time, RL-based systems outperform traditional approaches by offering more responsive, resilient, and sustainable urban mobility solutions. This section explores the mathematical foundation of RL in traffic control, detailing the roles of state representation, action space, reward function, and training paradigms in enabling intelligent traffic management.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l11"><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark17">The General Concept of RL</a></p><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;text-align: justify;">The mathematical structure of RL may be formalized via the MDP framework, defined as the tuple <span class="s19">(</span><i>S</i>, <i>A</i>, <i>P</i>, <i>R</i>, <span class="s20">γ</span><span class="s19">) </span><a href="#bookmark128" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">[</a><span style=" color: #0774B7;">87</span>].  Here, <i>S </i>is the set of environment states; <i>A </i>is the set of</p><p class="s2" style="padding-left: 82pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="p">possible actions; </span>P<span class="s19">(</span>s<span class="s21">′</span><span class="s22">|</span>s<span class="p">, </span>a<span class="s19">) </span><span class="p">represents the probability of transitioning to state </span>s<span class="s21">′</span><span class="s23"> </span><span class="p">from </span>s <span class="p">under</span></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 17pt;text-align: left;">action <i>a</i>; <i>R</i><span class="s19">(</span><i>s</i>, <i>a</i><span class="s19">) </span>denotes the reward for taking action <i>a </i>in state <i>s</i>; and <span class="s20">γ </span><span class="s22">∈ </span><span class="s19">[</span>0, 1<span class="s19">] </span>is the</p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 12pt;text-align: left;">discount factor balancing immediate and future rewards.</p><p class="s24" style="text-indent: 0pt;line-height: 8pt;text-align: left;">k<span class="s25">=</span><span class="s26">0</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 2pt;padding-left: 103pt;text-indent: 0pt;text-align: left;">The objective is to maximize the cumulative return <i>G</i><span class="s27">t</span><span class="s24">  </span><span class="s19">= </span><span class="s28">∑</span><span class="s29">∞</span></p><p class="s24" style="padding-top: 3pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><span class="s20">γ</span><span class="s30">k</span> <span class="s2">R</span><span class="s31">t</span><span class="s25">+</span>k  <span class="p">by learning a</span></p><p class="s2" style="padding-left: 82pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="p">policy </span><span class="s20">π</span><span class="s19">(</span>a<span class="s22">|</span>s<span class="s19">) </span><span class="p">that selects actions to maximize expected returns: </span>V<span class="s32">π</span><span class="s33"> </span><span class="s19">(</span>s<span class="s19">) = </span><span class="s34">E</span><span class="s19">[</span>G<span class="s27">t</span><span class="s24"> </span><span class="s22">|</span>s<span class="s27">t</span><span class="s24"> </span><span class="s19">= </span>s<span class="s19">]</span><span class="p">. The</span></p><p class="s2" style="padding-left: 82pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><span class="p">action-value function </span>Q<span class="s32">π</span><span class="s33"> </span><span class="s19">(</span>s<span class="p">, </span>a<span class="s19">) </span><span class="p">is defined as:</span></p><p class="s2" style="padding-top: 6pt;padding-left: 103pt;text-indent: 109pt;line-height: 160%;text-align: left;">Q<span class="s35">π</span><span class="s33"> </span><span class="s19">(</span>s<span class="p">, </span>a<span class="s19">) = </span><span class="s34">E</span><span class="s19">[</span>G<span class="s27">t</span><span class="s24"> </span><span class="s22">|</span>s<span class="s27">t</span><span class="s24"> </span><span class="s19">= </span>s<span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">= </span>a<span class="s19">]                                      </span><span class="p">(1) This function is updated using methods like Q-learning:</span></p><p class="s22" style="text-indent: 0pt;line-height: 7pt;text-align: left;">←                                   −</p><p style="text-indent: 0pt;text-align: left;"/><p class="s2" style="padding-left: 166pt;text-indent: 0pt;line-height: 16pt;text-align: left;">Q<span class="s19">(</span>s<span class="p">, </span>a<span class="s19">)    </span>Q<span class="s19">(</span>s<span class="p">, </span>a<span class="s19">) + </span><span class="s20">α</span><span class="s19">[</span>R <span class="s19">+ </span><span class="s20">γ </span><span class="p">max </span>Q<span class="s19">(</span>s<span class="s36">′</span><span class="p">, </span>a<span class="s36">′</span><span class="s19">)   </span>Q<span class="s19">(</span>s<span class="p">, </span>a<span class="s19">)]                     </span><span class="p">(2)</span></p><p class="s31" style="padding-left: 247pt;text-indent: 0pt;line-height: 9pt;text-align: center;">a<span class="s37">′</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">where <span class="s20">α </span>denotes the learning rate.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark18">Common RL Algorithms for Traffic Signal Control</a></p><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark90" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Reinforcement learning algorithms in traffic control may be classified into three core types [</a>43<span style=" color: #000;">]: value-based, policy-based, and actor-critic methods, each offering distinct mechanisms for optimizing traffic performance.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l12"><li style="padding-left: 107pt;text-indent: -25pt;text-align: left;"><p style="display: inline;"><a name="bookmark19">Value-Based Algorithms</a></p><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark129" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">This RL type estimates the expected return of actions in specific traffic states using a value function, selecting actions that maximize expected rewards [</a>88<a href="#bookmark90" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Value-based RL has been effective for discrete tasks such as phase selection or green-time allocation, though scalability becomes an issue in large or continuous action spaces. Common examples include Q-learning (QL) and deep Q-networks (DQNs) [</a>43<a href="#bookmark129" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>88<span style=" color: #000;">]:</span></p><ul id="l13"><li style="padding-top: 4pt;padding-left: 104pt;text-indent: -21pt;line-height: 106%;text-align: justify;"><p class="s2" style="display: inline;"><b>Q-Learning: </b><a href="#bookmark130" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">An off-policy algorithm that iteratively updates an action-value function independently of the current policy [</a><span class="s15">89</span><span class="p">]. Suitable for single-intersection control, it learns state-action values </span>Q<span class="s19">(</span>s<span class="p">, </span>a<span class="s19">) </span><span class="p">via:</span></p><p class="s24" style="text-indent: 0pt;line-height: 8pt;text-align: left;">a</p><p style="text-indent: 0pt;text-align: left;"/><p class="s2" style="padding-top: 1pt;padding-left: 104pt;text-indent: 44pt;line-height: 25pt;text-align: left;">Q<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">) </span><span class="s22">← </span>Q<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">) + </span><span class="s20">α</span><span class="s38">1</span>R<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">) + </span><span class="s20">γ </span><span class="p">max </span>Q<span class="s19">(</span>s<span class="s31">t</span><span class="s25">+</span><span class="s26">1</span><span class="p">, </span>a<span class="s19">) </span><span class="s22">− </span>Q<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">)</span><span class="s38">1</span><span class="s39">             </span><a href="#bookmark131" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">(3) QL is efficient in discrete environments but struggles with high-dimensional networks [</a><span class="s15">90</span><span class="p">].</span></p></li><li style="padding-top: 1pt;padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Deep Q-Networks: </span><span style=" color: #000;">DQN enhances Q-learning by approximating the </span><span class="s2">Q</span><a href="#bookmark132" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">-function with deep neural networks. Techniques like experience replay and target networks improve training stability [</a>91<a href="#bookmark133" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>92<a href="#bookmark134" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], enabling coordination across multiple intersections [</a>93<span style=" color: #000;">]. The update rule is as follows:</span></p><p class="s24" style="text-indent: 0pt;line-height: 8pt;text-align: left;">a</p><p style="text-indent: 0pt;text-align: left;"/><p class="s2" style="padding-top: 2pt;padding-left: 139pt;text-indent: 0pt;text-align: left;">Q<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">) </span><span class="s22">← </span>Q<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">) + </span><span class="s20">α</span><span class="s38">1</span>R<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">) + </span><span class="s20">γ </span><span class="p">max </span>Q<span class="s19">(</span>s<span class="s31">t</span><span class="s25">+</span><span class="s26">1</span><span class="p">, </span>a<span class="p">; </span><span class="s20">θ</span><span class="s19">) </span><span class="s22">− </span>Q<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="p">; </span><span class="s20">θ</span><span class="s19">)</span><span class="s38">1</span><span class="s39">        </span><span class="p">(4)</span></p></li></ul></li><li style="padding-top: 13pt;padding-left: 107pt;text-indent: -25pt;text-align: left;"><p style="display: inline;"><a name="bookmark20">Policy-Based Algorithms</a></p><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark135" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">This algorithm type bypasses value estimation by directly optimizing the policy. Ideal for continuous action spaces, they support fine-grained control like dynamic phase timing [</a>94<a href="#bookmark136" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. While more flexible in complex environments, they require precise hyperpa- rameter tuning. PPO stands out for its balance of stability and efficiency [</a>95<span style=" color: #000;">].</span></p><ul id="l14"><li style="padding-top: 4pt;padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Proximal Policy Optimization: <a href="#bookmark137" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">PPO constrains policy updates to prevent training instability. Widely used in multi-intersection control, it balances exploration and exploitation in large-scale networks [</a><span class="s15">96</span><span class="p">]. The clipped objective function may be expressed as:</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s19" style="padding-left: 171pt;text-indent: 0pt;text-align: left;"><span class="s2">L</span><span class="s40">CLIP</span><span class="s24"> </span>(<span class="s20">θ</span>) = <span class="s34">E</span>[<span class="p">min</span>(<span class="s2">r</span><span class="s27">t</span><span class="s24"> </span>(<span class="s20">θ</span>)<span class="s2">A</span><span class="s27">t</span><span class="p">, </span><span class="s2">clip</span>(<span class="s2">r</span><span class="s27">t</span><span class="s24"> </span>(<span class="s20">θ</span>)<span class="p">, 1 </span><span class="s22">− </span><span class="s20">ϵ</span><span class="p">, 1 </span>+ <span class="s20">ϵ</span>)<span class="s2">A</span><span class="s27">t</span><span class="s24"> </span>)]                   <span class="p">(5)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="47" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_015.png"/></span></p><p class="s24" style="text-indent: 0pt;line-height: 8pt;text-align: left;">t</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 0pt;line-height: 15pt;text-align: right;"><span class="p">with </span><span class="s2">r </span>(<span class="s20">θ</span>) =  <span class="s41">π</span><span class="s25">(</span><span class="s24">a</span><span class="s42">t</span><span class="s43"> </span><span class="s44">|</span><span class="s24">s</span><span class="s42">t</span><span class="s43"> </span><span class="s45">)</span></p><p class="s46" style="text-indent: 0pt;line-height: 9pt;text-align: right;"><span class="s33">π</span>old<span class="s43"> </span><span class="s25">(</span><span class="s24">a</span>t<span class="s43"> </span><span class="s23">|</span><span class="s24">s</span>t<span class="s43"> </span><span class="s25">)</span></p><p style="padding-top: 6pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">and <i>A</i><span class="s27">t</span><span class="s24"> </span><a href="#bookmark138" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">as the advantage function [</a><span style=" color: #0774B7;">97</span>].</p></li></ul></li><li style="padding-top: 2pt;padding-left: 107pt;text-indent: -25pt;text-align: left;"><p style="display: inline;"><a name="bookmark21">Actor-Critic Algorithms</a></p><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><span style=" color: #000;">Combining elements of both value and policy RL types, the actor-critic type uses an </span><span class="s47">actor </span><span style=" color: #000;">to propose actions and a </span><span class="s47">critic </span><a href="#bookmark90" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">to evaluate them [</a>43<a href="#bookmark139" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>98<a href="#bookmark139" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Such an architecture offers both stability and efficiency, making it suitable for large, complex traffic networks [</a>98<span style=" color: #000;">]. Notable examples include:</span></p><ul id="l15"><li style="padding-top: 4pt;padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Deep Deterministic Policy Gradient: <span class="p">The DDPG methodology is mostly suited for continuous control tasks, such as adjusting green durations in real time. It updates the critic and actor as follows:</span></h3><p class="s2" style="padding-top: 6pt;padding-left: 169pt;text-indent: 0pt;text-align: left;">CriticUpdate <span class="p">: </span>Q<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">) </span><span class="s22">← </span>R<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">) + </span><span class="s20">γ</span>Q<span class="s19">(</span>s<span class="s31">t</span><span class="s25">+</span><span class="s26">1</span><span class="p">, </span><span class="s20">µ</span><span class="s19">(</span>s<span class="s31">t</span><span class="s25">+</span><span class="s26">1</span><span class="s19">))                </span><span class="p">(6)</span></p><p class="s19" style="padding-top: 5pt;padding-left: 103pt;text-indent: 67pt;line-height: 135%;text-align: left;"><span class="s2">ActorUpdate </span><span class="p">: </span><span class="s22">∇</span><span class="s48">θ</span><span class="s33"> </span><span class="s2">J</span>(<span class="s20">θ</span>) = <span class="s34">E</span>[<span class="s22">∇</span><span class="s48">θ</span><span class="s33"> </span><span class="s2">Q</span>(<span class="s2">s</span><span class="s27">t</span><span class="p">, </span><span class="s20">µ</span>(<span class="s2">s</span><span class="s27">t</span><span class="p">; </span><span class="s20">θ</span>))<span class="s22">∇</span><span class="s48">θ</span><span class="s20">µ</span>(<span class="s2">s</span><span class="s27">t</span><span class="p">; </span><span class="s20">θ</span>)]                   <a href="#bookmark129" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">(7) While powerful, DDPG is sensitive to tuning [</a><span class="s15">88</span><a href="#bookmark140" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a><span class="s15">99</span><span class="p">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 10pt;text-align: justify;"><h3 style="display: inline;">Soft Actor-Critic: <span class="p">The SAC methodology integrates entropy into the reward function,</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: left;"><a href="#bookmark141" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">encouraging exploration and improving robustness in multi-agent settings [</a>100<span style=" color: #000;">]. Its objective includes an entropy term:</span></p><p class="s19" style="padding-top: 6pt;padding-left: 210pt;text-indent: 0pt;line-height: 19pt;text-align: left;"><span class="s2">J</span>(<span class="s20">π</span>) = <span class="s49">∑</span><span class="s50"> </span><span class="s34">E</span>[<span class="s2">R</span>(<span class="s2">s</span><span class="s27">t</span><span class="p">, </span><span class="s2">a</span><span class="s27">t</span><span class="s24"> </span>) + <span class="s20">α</span><span class="s2">H</span>(<span class="s20">π</span>(<span class="s22">·|</span><span class="s2">s</span><span class="s27">t</span><span class="s24"> </span>))]                          <span class="p">(8)</span></p><p class="s24" style="padding-left: 196pt;text-indent: 0pt;line-height: 8pt;text-align: center;">t</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 3pt;padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: left;"><h3 style="display: inline;">Advantage Actor-Critic: <span class="p">A2C improves training by incorporating an advantage func- tion to reduce variance:</span></h3><p class="s2" style="padding-left: 229pt;text-indent: 0pt;line-height: 14pt;text-align: left;">A<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">) = </span>R<span class="s19">(</span>s<span class="s27">t</span><span class="p">, </span>a<span class="s27">t</span><span class="s24"> </span><span class="s19">) </span><span class="s22">− </span>V<span class="s19">(</span>s<span class="s27">t</span><span class="s24"> </span><span class="s19">)                               </span><span class="p">(9)</span></p><p class="s19" style="padding-top: 1pt;padding-left: 103pt;text-indent: 96pt;text-align: left;"><span class="s22">∇</span><span class="s48">θ</span><span class="s33"> </span><span class="s2">J</span>(<span class="s20">π</span><span class="s48">θ</span><span class="s33"> </span>) = <span class="s34">E</span><span class="s51">π</span><span class="s52">θ</span><span class="s53"> </span>[<span class="s22">∇</span><span class="s48">θ</span><span class="s33"> </span><span class="p">log </span><span class="s20">π</span><span class="s48">θ</span><span class="s33"> </span>(<span class="s2">a</span><span class="s27">t</span><span class="s24"> </span><span class="s22">|</span><span class="s2">s</span><span class="s27">t</span><span class="s24"> </span>)<span class="s2">A</span>(<span class="s2">s</span><span class="s27">t</span><span class="p">, </span><span class="s2">a</span><span class="s27">t</span><span class="s24"> </span>)]                       <span class="p">(10)</span></p><p class="s15" style="padding-top: 6pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: left;"><a href="#bookmark142" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">A2C has shown effective results in real-time adaptive control with reduced training variance [</a>101<a href="#bookmark143" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>102<span style=" color: #000;">], especially in coordinated multi-agent systems.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li></ol></li></ol></li><li style="padding-left: 95pt;text-indent: -12pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark23">Attribute Tables and Summaries of RL Applications</a><a name="bookmark29">&zwnj;</a></h2><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">This section provides a structured analysis of RL applications in TSC from 2015 to 2025. Each subsection offers a high-level overview, of Value-based, Policy-based, Actor-Critic and Hybrid high-impact applications found in literature. This approach allows readers to quickly identify relevant applications in the tables and refer to the detailed summaries for deeper insights into their methodologies and findings.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l16"><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark24">Attribute Tables and Summaries Description</a></p><h3 style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">Attribute Tables <span class="p">systematically illustrate each application according to key character- istics, ensuring a comprehensive understanding of the overall approach:</span></h3><ul id="l17"><li style="padding-top: 4pt;padding-left: 104pt;text-indent: -21pt;text-align: left;"><h3 style="display: inline;">Ref.: <span class="p">contains the reference application is listed in the first column;</span></h3></li><li style="padding-top: 1pt;padding-left: 103pt;text-indent: -21pt;text-align: left;"><h3 style="display: inline;">Year: <span class="p">contains the publication year for each research application;</span></h3></li><li style="padding-top: 1pt;padding-left: 104pt;text-indent: -21pt;text-align: left;"><h3 style="display: inline;">Method: <span class="p">contains the specific RL algorithmic methodology applied in each application;</span></h3></li><li style="padding-top: 1pt;padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Agent: <span class="p">contains the agent type of the concerned methodology (single-agent or multi- agent RL approach);</span></h3></li><li style="padding-left: 103pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Baseline: <span class="p">illustrates the comparison methods used to evaluate the proposed RL approach, such as fixed-time (FT), actuated control (AC), max-pressure (MP), CoLight (CL), PressLight (PL), MetaLight (ML), FRAP, SOTL, or other RL-based strategies;</span></h3></li><li style="padding-left: 103pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">City Network: <span class="p">contains the main urban traffic network or city where simulations were conducted; each country is abbreviated in parentheses, while simulated traffic networks that do not correspond to a specific city are abbreviated as “synthetic”;</span></h3><ul id="l18"><li style="padding-top: 2pt;padding-left: 158pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Junctions: <span class="p">illustrates the number of intersections involved in the study (if multiple scenarios or city networks were evaluated, values for different networks are separated by a “/”);</span></h3></li><li style="padding-left: 158pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Simulation: <span class="p">illustrates the traffic simulation platform used to test and validate the RL method, such as SUMO, VISSIM, Aimsun, CityLearn, or others;</span></h3></li><li style="padding-left: 157pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Data: <span class="p">describes the utilized data that the RL algorithms were trained, tested, and validate on: real data from actual traffic networks are denoted as “real”, simulated and synthetic data as “sim”, and cases where both types of data were utilized are denoted as “both”;</span></h3></li><li style="padding-left: 158pt;text-indent: -21pt;text-align: justify;"><h3 style="display: inline;">Cit.: <span class="p">the number of citations—according to Scopus—of each research application.</span></h3><p style="padding-top: 6pt;padding-left: 136pt;text-indent: 21pt;line-height: 112%;text-align: left;">Moreover, <b>Summaries </b>that following each Attribute Table concern a brief description of the concerned application:</p></li><li style="padding-top: 2pt;padding-left: 157pt;text-indent: -21pt;text-align: left;"><h3 style="display: inline;">Author: <span class="p">contains the name of the author along with the reference application;</span></h3></li><li style="padding-top: 1pt;padding-left: 158pt;text-indent: -21pt;text-align: left;"><h3 style="display: inline;">Summary: <span class="p">contains a brief description of the research work;</span></h3><p style="padding-top: 4pt;padding-left: 136pt;text-indent: 0pt;line-height: 112%;text-align: left;">The abbreviations “-” or “N/A” represent the “not identified” elements in tables and figures.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li></ul></li><li style="padding-left: 154pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark25">Value-Based RL Applications</a></p><p class="s15" style="padding-top: 4pt;padding-left: 136pt;text-indent: 21pt;line-height: 112%;text-align: left;"><a href="#bookmark30" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The value-based RL applications for TSC and their primary attributes are integrated in Table </a>2<a href="#bookmark31" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, while the brief summaries of the applications are illustrated in Table </a>3<span style=" color: #000;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 136pt;text-indent: 0pt;text-align: left;">Table 2. <span class="s14">Value-based RL applications and their basic attributes.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.716pt" cellspacing="0"><tr style="height:16pt"><td style="width:31pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Ref.</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Year</p></td><td style="width:66pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Method</p></td><td style="width:37pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Agent</p></td><td style="width:91pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Baseline</p></td><td style="width:85pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">City Network</p></td><td style="width:64pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Junctions</p></td><td style="width:61pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Simulation</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Data</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Cit.</p></td></tr><tr style="height:14pt"><td style="width:31pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark144" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">103</span>]</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">2016</p></td><td style="width:66pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">DQN</p></td><td style="width:37pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Single</p></td><td style="width:91pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">QL</p></td><td style="width:85pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Synthetic</p></td><td style="width:64pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">1</p></td><td style="width:61pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">PARAMICS</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Sim</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">482</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark145" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">104</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2018</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">NFQI</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">NFQI</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">6</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">119</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark146" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">105</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2018</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SARSA</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">AC</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Stockholm (SE)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">3</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">59</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark147" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">106</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2018</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">RMART</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/QL/SARSA</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Tippecanoe (US)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">18</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Vissim</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">50</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark148" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">107</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2019</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/Other</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">60</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark149" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">108</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2019</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/DQN</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">429</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark150" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">109</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2019</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">NAQL/RBC</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">24</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">160</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark151" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">110</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/AC</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Luxembourg(LU)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">100</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark152" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">111</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN/dDQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/AC/PPO/DQN</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">25/100/225</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">51</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark153" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">112</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/RL/SOTL</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Hangzhou (CN)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">24</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">CityFlow</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Real</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">128</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark154" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">113</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/QL</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sunway (MY)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">7</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">57</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark155" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">114</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Context QL</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">QL/RUQL</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Vissim</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">75</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark156" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">115</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/FRAP</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Manhattan (US)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">2510</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">CityFlow</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Both</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">262</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark157" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">116</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">QL</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">AC</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Surrey (CA)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">2</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Vissim</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Real</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">50</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark158" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">117</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">3DQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">AC/Other RL</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Seminole (US)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Aimsun</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Real</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">45</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark159" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">118</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">QL</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">QL</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">76</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark160" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">119</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Co-DQL</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">RL</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Qingdao (CN)</p></td><td style="width:64pt"><p class="s55" style="padding-left: 20pt;text-indent: 0pt;line-height: 10pt;text-align: left;">47/57</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">120</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark161" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">120</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DRQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/QL/DQN</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt"><p class="s55" style="padding-left: 10pt;text-indent: 0pt;line-height: 10pt;text-align: left;">20/50/100</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">59</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">121</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2021</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">IQL</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/IQL/RBC</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Manhattan (US)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">3971</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">50</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark163" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">122</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2021</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Islamabad (PK)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">41</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark164" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">123</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2021</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">CGB-MAQL</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/Other RL</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Hangzhou (CN)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">16</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">CityFlow</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Real</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">95</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark165" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">124</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2022</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN/Other RL</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">4/6</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">37</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark166" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">125</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2023</p></td><td style="width:66pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">QL</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:91pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/AC</p></td><td style="width:85pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">6</p></td><td style="width:61pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">26</p></td></tr><tr style="height:14pt"><td style="width:31pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark167" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">126</span>]</p></td><td style="width:30pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2023</p></td><td style="width:66pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DDQN</p></td><td style="width:37pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:91pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/PS/Other</p></td><td style="width:85pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:61pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">45</p></td></tr></table><h4 style="padding-top: 2pt;padding-left: 136pt;text-indent: 0pt;text-align: left;"><a name="bookmark30">Table 2. </a><i>Cont.</i></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.716pt" cellspacing="0"><tr style="height:16pt"><td style="width:31pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Ref.</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Year</p></td><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Method</p></td><td style="width:47pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Agent</p></td><td style="width:96pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Baseline</p></td><td style="width:84pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">City Network</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Junctions</p></td><td style="width:62pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Simulation</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Data</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Cit.</p></td></tr><tr style="height:14pt"><td style="width:31pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark168" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">127</span>]</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">2023</p></td><td style="width:55pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">DDQN</p></td><td style="width:47pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Single</p></td><td style="width:96pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">AC</p></td><td style="width:84pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Melbourne(AU)</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">1</p></td><td style="width:62pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Vissim</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Real</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">30</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark169" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">128</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2023</p></td><td style="width:55pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:47pt"><p class="s55" style="padding-left: 16pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:96pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/Other</p></td><td style="width:84pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Bloomsbury(UK)</p></td><td style="width:60pt"><p class="s55" style="padding-left: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">25/15</p></td><td style="width:62pt"><p class="s55" style="padding-left: 10pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">25</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark170" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">129</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2023</p></td><td style="width:55pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">3DQN</p></td><td style="width:47pt"><p class="s55" style="padding-left: 16pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:96pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/Other</p></td><td style="width:84pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Cologne (DE)</p></td><td style="width:60pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:62pt"><p class="s55" style="padding-left: 10pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Both</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">21</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark171" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">130</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2024</p></td><td style="width:55pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:47pt"><p class="s55" style="padding-left: 16pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:96pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/DQN/RBC</p></td><td style="width:84pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:60pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:62pt"><p class="s55" style="padding-left: 10pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">28</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark172" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">131</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2024</p></td><td style="width:55pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN</p></td><td style="width:47pt"><p class="s55" style="padding-left: 16pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:96pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT</p></td><td style="width:84pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Lisbon (PT)</p></td><td style="width:60pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">2</p></td><td style="width:62pt"><p class="s55" style="padding-left: 10pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">10</p></td></tr><tr style="height:14pt"><td style="width:31pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark173" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">132</span>]</p></td><td style="width:30pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2024</p></td><td style="width:55pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">D3QN</p></td><td style="width:47pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 16pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:96pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/AC/D3QN/Other</p></td><td style="width:84pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Changsha (CN)</p></td><td style="width:60pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:62pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 10pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Real</p></td><td style="width:26pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">16</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 3pt;padding-left: 136pt;text-indent: 0pt;text-align: left;">Table 3. <span class="s14">Summaries of value-based RL applications for traffic signal control.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="695" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_016.png"/></span></p><h4 style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Author                              Summary</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_017.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark149" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Liang et al. [</a>108<span style=" color: #000;">]   Introduced a single-agent DQN framework where intersections were modeled as grids with vehicle position and speed as state inputs. Actions adjusted signal phase duration in 5 s increments, and the reward concerned the alteration in cumulative wait time between cycles. The model used dueling and double QL with prioritized replay. SUMO simulations showed a 26.7% reduction in average waiting time (AWT) compared to FT, ATSC, and standard DQN.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_018.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark150" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Tan et al. [</a>109<span style=" color: #000;">]          Proposed CODER, a hierarchical MARL system with regional agents using DQN for local control and a global agent for coordination. The state was based on queue length per direction, and actions toggled NS/EW green lights. Reward penalized high queue length and sparse movement. In a 4 × 6 SUMO network, CODER achieved 30% congestion reduction over linear QL and RBC.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_019.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark151" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Zhang et al. [</a>110<span style=" color: #000;">] Developed a DQN adaptive TSC system (PD-ITSC) using low-rate vehicle detection via V2I. The state included detected vehicle speeds, queue length, distance to stop line, and signal phase; actions involved switching or holding phases, while the reward function minimized vehicle delay. Trained in SUMO, it reduced AWT by up to 30% and remained robust even with just 20% detection rates.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_020.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark152" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Zhou et al. [</a>111<span style=" color: #000;">]     Presented DRLE, a distributed DQN/dDQN-based MARL architecture operating at three levels: inter- section, intra-edge, and inter-edge servers. Input states included the number of halted vehicles, speed lag, and traffic light status; agents perform binary signal switching, while the reward penalized stops and speed loss. SUMO results showed 65.66% faster convergence vs. PPO.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_021.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark153" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Zang et al. [</a>112<span style=" color: #000;">]       Proposed MetaLight, a meta-RL system combining MAML and FRAP++ (DQN-based) to enable quick policy adaptation across networks. The agent used lane-based queue length as state and selected traffic light phases to minimize queues. Reward was focused on queue length minimization. Evaluated in CityFlow on four real-world datasets, Metalight reduced travel time by up to 22.57%.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_022.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark154" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Rasheed et al. [</a>113<span style=" color: #000;">]   Developed a MARL DQN for disturbed environments integrating weather data and red timings in the state. Agents coordinated to dynamically adapt phase splits, with reward based on reduced wait and queue length. Trained in SUMO, it reduced queue by 75%, wait time by 70%, and increased throughput by 70% over baselines.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_023.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark155" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Padakandla et al. [</a>114<span style=" color: #000;">] Introduced Context QL to adapt to non-stationary traffic by integrating change detection into a QL agent. The state included queue length and signal phase, while actions modified green durations. Reward optimized flow under shifting conditions. In VISSIM, it reduced queue length by 21.4% vs. QL and 45.3% vs. RUQL.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_024.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark159" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Joo et al. [</a>118<span style=" color: #000;">]            Presented a single-agent QL framework using queue length and throughput as input and selecting from three fixed phases. The reward minimized queue length deviation and optimized vehicle throughput. Trained in SUMO across 3- to 6-way intersections, queue length was reduced by 63% and waiting time by 40% vs. basic QL.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_025.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark156" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Chen et al. [</a>115<span style=" color: #000;">]    Proposed MPLight, a scalable DQN-based MARL with parameter sharing for large-scale urban networks. Agents used queue length as state and choose from eight phases; reward was pressure based. Trained on 2510 real-world intersections in CityFlow, MPLight reduced travel time by 19.2% and increased throughput by 3.08% vs. GCN, FRAP, and FT.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_026.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark157" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Essa et al. [</a>116<span style=" color: #000;">] Introduced a QL approach for adaptive signal control focused on safety using CV data. The state captured real-time conflicts, platoon ratio, and shockwaves, with binary actions (green extension/switching). Reward minimized rear-end risks. VISSIM simulations using real data showed a 40% reduction in rear-end conflicts vs. actuated signals.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="692" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_027.png"/></span></p><h4 style="padding-top: 2pt;padding-left: 136pt;text-indent: 0pt;text-align: left;"><a name="bookmark31">Table 3. </a><i>Cont.</i></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="695" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_028.png"/></span></p><h4 style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Author                              Summary</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_029.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark158" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Gong et al. [</a>117<span style=" color: #000;">]                Developed a safety-enhanced ATSC framework using 3DQN combining efficiency and crash risk via a weighted reward. The state included high-res traffic data, and the agent selected signal phases. In AIMSUN, the model reduced delays by 25.93% and crash risk by 8.89% compared to actuated and RL efficiency-only controllers.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_030.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Devailly et al. [</a>121<span style=" color: #000;">]  Proposed IG-RL, a decentralized MARL using Independent QL with GCNs for spatial traffic patterns. The state is a dynamic lane-based graph, with actions for phase switching. The system transfered policies to unseen networks. In SUMO, it reduced trip time by 20–30% and delay by 15–40% across 3971 Manhattan signals.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_031.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark160" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Wang et al. [</a>119<span style=" color: #000;">]      Presented Co-DQL, a decentralized MARL combining IDQL with UCB exploration, mean-field approxima- tion, and reward allocation. States include queue length and waiting time; actions control phase sequences. In a 49-intersection SUMO setup, Co-DQL cut delay by 22–38% and increased throughput by 15% vs. DDPG and MA2C.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_032.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark161" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Xu et al. [</a>120<span style=" color: #000;">]         Used a DRQN with LSTM to learn from temporal traffic patterns using flow, speed, and signal status. Actions included phase holding or switching, and reward aimed to reduce delay with fairness. In SUMO’s 100-node network, travel time dropped 23.6% vs. FT and outperformed DQN, QL, and SOTL baselines.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_033.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark163" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Mushtaq et al. [</a>122<span style=" color: #000;">] Proposed a DQN system for AV networks integrated with smart re-routing. The state included vehicle speed, position, and queue length, while action space modified signal timing. SUMO training demonstrated a 31% congestion reduction vs. FT, showing potential scalability in AV-rich environments.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_034.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark164" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Wang et al. [</a>123<span style=" color: #000;">] Developed CGB-MAQL, a group-based MARL method with spatial reward discounting, pheromone coordination, and k-NN state inputs. Agents controlled adaptive phase sequences. In SUMO-based Monaco and Harbin city models, the approach reduced waiting times by up to 78.84% vs. FT and IQL.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_035.png"/></span></p><p class="s14" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark165" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Haddad et al. [</a><span style=" color: #0774B7;">124</span>] Introduced a cooperative MADQN with neighbor-informed state/action/reward sharing. Input from real-time sensors drives phase decisions. SUMO simulations on 2 × 2 and 2 × 3 networks showed 26.8% lower AWT, 20.54% shorter average queue length, and 18.25% less CO<span class="s59">2</span><span class="s8"> </span>than baseline MARL methods.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_036.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark166" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Kolat et al. [</a>125<span style=" color: #000;">]   Designed a MARL DQN with a custom reward minimizing halting std-dev per intersection. State included halting percentage and intersection ID; actions toggled NS/EW green. SUMO (six nodes) showed a 13% travel time and 11% fuel reduction over baseline control.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_037.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark167" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Bouktif et al. [</a>126<span style=" color: #000;">] Used DDQN with PER and standardized state/reward design across vehicle count, queue length, and waiting time. Actions select among four predefined phases; reward considered the negative of selected state metric. SUMO results showed up to 8.3% travel time reduction over baselines.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_038.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark168" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Yazdani et al. [</a>127<span style=" color: #000;">] Developed IVPL, a DDQN-based system for joint vehicle and pedestrian control. State included vehicle occupancy, speed, pedestrian volume, and signal status; reward penalized all user delays. In VISSIM, it reduced total delays by 5%, pedestrian delays by 13%, and vehicle travel time by 9% vs. baselines.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_039.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark169" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Su et al. [</a>128<span style=" color: #000;">] Integrated DQN-based perimeter control with max-pressure signal control in a hierarchical RL framework. Upper-level DQN regulated inflow, while lower-level MP managed internal flow. In SUMO, throughput increased by 100.5% and waiting time decreased by 49.64% vs. baseline control.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_040.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark170" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Du et al. [</a>129<span style=" color: #000;">]               Presented SafeLight, a safety-aware RL system using 3DQN with multi-objective loss and reward shap- ing. State includes queue length and position; actions support cyclic/acyclic controls. In SUMO using real/synthetic data, it cut collisions by 99% and waiting time by 30% vs. FT.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_041.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark171" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Li et al. [</a>130<span style=" color: #000;">]            Proposed a DQN system integrating adaptive signal control with connected and automated vehicles (CAVs) coordination. States represented traffic mix and phase data; CAVs bypassed signals via platooning. SUMO showed travel time reduction of 37.33% and 15.95% less fuel use under high CAV rates.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_042.png"/></span></p><p class="s61" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark172" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Vieira et al. [</a>131<span style=" color: #000;">] Used MARL with visible-light-based V2X localization for dynamic vehicle/pedestrian signal timing. State captured queue, location, and wait info; action selects phase. In a Lisbon SUMO scenario, the method reduced vehicle wait by 30% and pedestrian delay by 30% vs. FT.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_043.png"/></span></p><p class="s14" style="padding-left: 111pt;text-indent: -99pt;line-height: 11pt;text-align: justify;"><a href="#bookmark173" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Zhang et al. [</a><span style=" color: #0774B7;">132</span>]       Proposed a D3QN-based multi-objective RL system optimizing safety, delay, and emissions using DTSE state encoding. Actions selected one of four signal phases, while reward balanced conflicts, delay, and CO<span class="s59">2</span>. In SUMO, it reduced wait by 85.36%, CO<span class="s59">2</span><span class="s8"> </span>by 35.58%, and conflicts by 55.39% vs. baselines.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="695" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_044.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 154pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark26">Policy-Based RL Applications</a></p><p class="s15" style="padding-top: 4pt;padding-left: 136pt;text-indent: 21pt;line-height: 112%;text-align: left;"><a href="#bookmark32" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The policy-based RL applications for TSC and their primary attributes are integrated in Table </a>4<a href="#bookmark33" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, while brief summaries of the applications are illustrated in Table </a>5<span style=" color: #000;">:</span></p><h4 style="padding-top: 2pt;padding-left: 136pt;text-indent: 0pt;text-align: left;"><a name="bookmark32">Table 4. </a><span class="s14">Policy-based RL applications and their basic attributes.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.716pt" cellspacing="0"><tr style="height:16pt"><td style="width:31pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Ref.</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Year</p></td><td style="width:64pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Method</p></td><td style="width:37pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Agent</p></td><td style="width:70pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Baseline</p></td><td style="width:82pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">City Network</p></td><td style="width:79pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Junctions</p></td><td style="width:72pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Simulation</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Data</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Cit.</p></td></tr><tr style="height:14pt"><td style="width:31pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark174" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">133</span>]</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">2017</p></td><td style="width:64pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">PG</p></td><td style="width:37pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Single</p></td><td style="width:70pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">FT/ANN</p></td><td style="width:82pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Synthetic</p></td><td style="width:79pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">1</p></td><td style="width:72pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">SUMO</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Sim</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">253</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark175" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">134</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2018</p></td><td style="width:64pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">PG</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:70pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT</p></td><td style="width:82pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:79pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:72pt"><p class="s55" style="padding-left: 20pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Unity3D</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">53</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark176" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">135</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:64pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">REINFORCE</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:70pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/MP/FRAP</p></td><td style="width:82pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Atlanta (US)</p></td><td style="width:79pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">9</p></td><td style="width:72pt"><p class="s55" style="padding-left: 20pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Real</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">55</p></td></tr><tr style="height:14pt"><td style="width:31pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark177" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">136</span>]</p></td><td style="width:30pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2023</p></td><td style="width:64pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">PPO</p></td><td style="width:37pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:70pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">PL/Other</p></td><td style="width:82pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Dublin (IE)</p></td><td style="width:79pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">31</p></td><td style="width:72pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 20pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">37</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 3pt;padding-left: 136pt;text-indent: 0pt;text-align: left;"><a name="bookmark33">Table 5. </a><span class="s14">Summaries of policy-based RL applications for traffic signal control.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="695" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_045.png"/></span></p><h4 style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Author                            Summary</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_046.png"/></span></p><p class="s61" style="padding-left: 106pt;text-indent: -94pt;line-height: 11pt;text-align: justify;"><a href="#bookmark174" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Mousavi et al. [</a>133<span style=" color: #000;">] Proposed a CNN-based policy-gradient (PG) model using raw traffic camera images to output phase- selection probabilities. Trained via A2C, the system avoided queue length instabilities and directly op- timized delay reduction. It achieved a 67% drop in cumulative delay and 72% queue length reduction compared to baseline control.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_047.png"/></span></p><p class="s61" style="padding-left: 105pt;text-indent: -94pt;line-height: 11pt;text-align: justify;"><a href="#bookmark175" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Garg et al. [</a>134<span style=" color: #000;">]   Introduced a vision-based PG method using pixel-level image input to dynamically control red/green phases. The single-agent RL framework optimized traffic throughput and was trained in a custom Unity3D traffic simulator. It performed comparably to FT signals with expected benefits in non-stationary traffic.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_048.png"/></span></p><p class="s61" style="padding-left: 106pt;text-indent: -94pt;line-height: 11pt;text-align: justify;"><a href="#bookmark176" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Oroojlooy et al. [</a>135<span style=" color: #000;">] Developed AttendLight, a REINFORCE-based PG system with attention modules allowing generaliza- tion across varied intersection layouts. Input states reflected lane/phase configs and reward minimized intersection pressure. Trained in CityFlow, it reduced travel time by 46% vs. FT and 9% over FRAP.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_049.png"/></span></p><p class="s14" style="padding-left: 106pt;text-indent: -94pt;line-height: 11pt;text-align: justify;"><a href="#bookmark177" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Guo et al. [</a><span style=" color: #0774B7;">136</span>]       Proposed CoTV, a PPO-based MARL for coordinating traffic lights and CAVs using real-time V2X data. States captured vehicle flow and signal status; binary actions switched phases. SUMO results from Dublin scenarios showed 30% shorter travel time and 28% lower CO<span class="s59">2</span><span class="s8"> </span>emissions vs. PressLight and actuated control.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="695" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_050.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 154pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark27">Actor-Critic RL Applications</a></p><p class="s15" style="padding-top: 4pt;padding-left: 136pt;text-indent: 21pt;line-height: 112%;text-align: left;"><a href="#bookmark34" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The actor-critic RL applications for TSC and their primary attributes are integrated in Table </a>6<a href="#bookmark35" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, while the brief summaries of the applications are illustrated in Table </a>7<span style=" color: #000;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 136pt;text-indent: 0pt;text-align: left;"><a name="bookmark34">Table 6. </a><span class="s14">Actor-critic RL applications and their basic attributes.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.716pt" cellspacing="0"><tr style="height:16pt"><td style="width:31pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Ref.</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Year</p></td><td style="width:53pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Method</p></td><td style="width:37pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Agent</p></td><td style="width:100pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Baseline</p></td><td style="width:87pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">City Network</p></td><td style="width:64pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Junctions</p></td><td style="width:63pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Simulation</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Data</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Cit.</p></td></tr><tr style="height:14pt"><td style="width:31pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark178" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">137</span>]</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">2017</p></td><td style="width:53pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">A-CATs</p></td><td style="width:37pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Single</p></td><td style="width:100pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">FT/QL/BQL</p></td><td style="width:87pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Tehran (IR)</p></td><td style="width:64pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">50</p></td><td style="width:63pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Aimsun</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Real</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">183</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark179" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">138</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2019</p></td><td style="width:53pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">MA2C</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:100pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">QL/A2C</p></td><td style="width:87pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Monaco (MC)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">30</p></td><td style="width:63pt"><p class="s55" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">653</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark180" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">139</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2021</p></td><td style="width:53pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DDPG</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:100pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/MP/DQN/DDPG</p></td><td style="width:87pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Montgomery(US)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">100</p></td><td style="width:63pt"><p class="s55" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Both</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">106</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark181" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">140</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2021</p></td><td style="width:53pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DDPG</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:100pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/DQN/DDPG</p></td><td style="width:87pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Montgomery(US)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">N/A</p></td><td style="width:63pt"><p class="s55" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Real</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">20</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark182" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">141</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2021</p></td><td style="width:53pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DDPG</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:100pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">AC/DP</p></td><td style="width:87pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:63pt"><p class="s55" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Vissim</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">35</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark183" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">142</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2021</p></td><td style="width:53pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SAC</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:100pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">MP/CL/ML/MA2C</p></td><td style="width:87pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Chengdu (CN)</p></td><td style="width:64pt"><p class="s55" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">15/20/121</p></td><td style="width:63pt"><p class="s55" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">45</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark184" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">143</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2022</p></td><td style="width:53pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">A2C</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:100pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT</p></td><td style="width:87pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Shiraz (IR)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">6</p></td><td style="width:63pt"><p class="s55" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Real</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">38</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark185" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">144</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2022</p></td><td style="width:53pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">A2C/A3C</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:100pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/QL/DQN</p></td><td style="width:87pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">N/A (CN)</p></td><td style="width:64pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">27</p></td><td style="width:63pt"><p class="s55" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">CityFlow</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Real</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">49</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark186" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">145</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2022</p></td><td style="width:53pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">A2C</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:100pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN/PL/MP</p></td><td style="width:87pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Penn State (US)</p></td><td style="width:64pt"><p class="s55" style="padding-left: 17pt;text-indent: 0pt;line-height: 10pt;text-align: left;">4/4/25</p></td><td style="width:63pt"><p class="s55" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">38</p></td></tr><tr style="height:14pt"><td style="width:31pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark187" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">146</span>]</p></td><td style="width:30pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2023</p></td><td style="width:53pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">A2C</p></td><td style="width:37pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:100pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/PL/MP/CL</p></td><td style="width:87pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Manhattan (US)</p></td><td style="width:64pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 13pt;text-indent: 0pt;line-height: 10pt;text-align: left;">48/16/25</p></td><td style="width:63pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Both</p></td><td style="width:26pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">25</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 3pt;padding-left: 136pt;text-indent: 0pt;text-align: left;">Table 7. <span class="s14">Summaries of actor-critic RL applications for traffic signal control.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="695" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_051.png"/></span></p><h4 style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Author                      Summary</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_052.png"/></span></p><p class="s61" style="padding-left: 92pt;text-indent: -80pt;line-height: 11pt;text-align: justify;"><a href="#bookmark178" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Aslani et al. [</a>137<span style=" color: #000;">] Proposed A-CATs, an actor-critic controller using RBF approximation to manage real-world traffic dis- ruptions in Tehran. This framework handled sensor noise and stochastic events better than QL and BQL. Compared to FT control, it reduced travel time by 59.2%, fuel by 19.7%, and emissions by 41.5%.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_053.png"/></span></p><p class="s61" style="padding-left: 92pt;text-indent: -80pt;line-height: 11pt;text-align: justify;"><a href="#bookmark179" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Chu et al. [</a>138<span style=" color: #000;">] Presented MA2C, a decentralized A2C-based MARL for ATSC, integrating neighbor fingerprints and spatial discounting. Trained in SUMO on a 5 × 5 grid and Monaco network, it reduced queue length by 35%, delay by 28%, and congestion peaks by 40% vs. indep. A2C and QL.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="695" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_054.png"/></span></p><h4 style="padding-top: 2pt;padding-left: 136pt;text-indent: 0pt;text-align: left;"><a name="bookmark35">Table 7. </a><i>Cont.</i></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="695" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_055.png"/></span></p><h4 style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Author                             Summary</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_056.png"/></span></p><p class="s61" style="padding-left: 107pt;text-indent: -96pt;line-height: 11pt;text-align: justify;"><a href="#bookmark181" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Li et al. [</a>140<span style=" color: #000;">]           Used MADDPG for centralized training with decentralized execution, with global critics and local actors. States included vehicle wait time and volume; actions involved flexible phase switching. Trained in SUMO (Montgomery County), it lowered queue length (0.8 vs. 2.3–9.8), delay (5.2 s vs. 9–21 s), and increased throughput (6118 vs. &lt;5600).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_057.png"/></span></p><p class="s61" style="padding-left: 108pt;text-indent: -96pt;line-height: 11pt;text-align: justify;"><a href="#bookmark180" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Li et al. [</a>139<span style=" color: #000;">]    Developed KS-DDPG, extending MARL DDPG with agent communication for shared state learning. Inputs included traffic volume and signal phase, while actions modified signal timing. SUMO tests on synthetic and real networks showed 28.9% queue length and 35.1% delay reduction vs. FT, MaxPressure, DQN, DDPG, and MADDPG.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_058.png"/></span></p><p class="s61" style="padding-left: 108pt;text-indent: -96pt;line-height: 11pt;text-align: justify;"><a href="#bookmark182" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Guo et al. [</a>141<span style=" color: #000;">]        Introduced DRL-TP3 using DDPG and LSTM to optimize signal timing with CAV trajectory prediction. States included speed, position, and acceleration. In VISSIM, it reduced delay by 89.4% and raised throughput by 25.2% vs. actuated and DP-based methods.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_059.png"/></span></p><p class="s61" style="padding-left: 107pt;text-indent: -96pt;line-height: 11pt;text-align: justify;"><a href="#bookmark183" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Yang et al. [</a>142<span style=" color: #000;">] Proposed HG-MA, an SAC-based MARL model using inductive heterogeneous GNNs for state abstraction. TN-HetG captured signal, lane, and vehicle data; actions controlled lane-wise phases. In SUMO, it reduced delay by 30.5%, QL by 41.8%, and travel time by 29.6% vs. baselines.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_060.png"/></span></p><p class="s61" style="padding-left: 108pt;text-indent: -96pt;line-height: 11pt;text-align: justify;"><a href="#bookmark185" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Wu et al. [</a>144<span style=" color: #000;">]      Developed Nash-A2C and Nash-A3C, integrating Nash equilibrium into A2C/A3C for large-scale traffic control. Input states used queue length, while rewards minimized delay. In CityFlow with 27 intersections, Nash-A3C reduced congestion by 22.1% and network delay by 9.7%.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_061.png"/></span></p><p class="s61" style="padding-left: 107pt;text-indent: -96pt;line-height: 11pt;text-align: justify;"><a href="#bookmark184" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Damadam et al. [</a>143<span style=" color: #000;">] Used real-time IoT sensor data with A2C in Shiraz City. The fingerprint-based MARL optimized light phase switching to reduce queue length and waiting time. SUMO simulations using real data showed 37.8% queue length and 31.5% wait time reduction vs. baseline controls.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_062.png"/></span></p><p class="s61" style="padding-left: 108pt;text-indent: -96pt;line-height: 11pt;text-align: justify;"><a href="#bookmark186" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Mo et al. [</a>145<span style=" color: #000;">]      Proposed CVLight using Asym-A2C where the actor used connected vehicle (CV) data and the critic includes both CV and non-CV info. States included vehicle count, delay, and phase duration, while reward concerned intersection pressure. SUMO results showed 6.2 s delay at 10% CV, beating PressLight (6.8 s) and DQN (8.7 s).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_063.png"/></span></p><p class="s61" style="padding-left: 108pt;text-indent: -96pt;line-height: 11pt;text-align: justify;"><a href="#bookmark187" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Su et al. [</a>146<span style=" color: #000;">]         Introduced EMVLight, a multi-agent A2C with policy sharing for emergency vehicle (EMV) routing and general traffic. The agent prioritized pressure-based rewards to favor EMVs while balancing network flow. SUMO with real/synthetic maps showed 42.6% faster EMV travel and 23.5% less overall delay vs. multiple baselines.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="695" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_064.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 154pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark28">Hybrid RL Applications</a></p><p class="s15" style="padding-top: 4pt;padding-left: 136pt;text-indent: 21pt;line-height: 112%;text-align: left;"><a href="#bookmark36" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The Hybrid RL applications for TSC and their primary attributes are integrated in Table </a>8<a href="#bookmark37" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, while brief summaries of the applications are illustrated in Table </a>9<span style=" color: #000;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 136pt;text-indent: 0pt;text-align: left;"><a name="bookmark36">Table 8. </a><span class="s14">Hybrid RL applications and their basic attributes.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.716pt" cellspacing="0"><tr style="height:16pt"><td style="width:31pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Ref.</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Year</p></td><td style="width:68pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Method</p></td><td style="width:37pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Agent</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Baseline</p></td><td style="width:78pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">City Network</p></td><td style="width:69pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Junctions</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Simulation</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Data</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s54" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Cit.</p></td></tr><tr style="height:14pt"><td style="width:31pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark188" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">147</span>]</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">2017</p></td><td style="width:68pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">RL/AIN</p></td><td style="width:37pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Multi</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">FT/Other</p></td><td style="width:78pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Synthetic</p></td><td style="width:69pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">6</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Vissim</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Sim</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt"><p class="s55" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">43</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">148</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2019</p></td><td style="width:68pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN/GAT</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:92pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">MP/GCN</p></td><td style="width:78pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Manhattan (US)</p></td><td style="width:69pt"><p class="s55" style="padding-left: 13pt;text-indent: 0pt;line-height: 10pt;text-align: left;">196/16/12</p></td><td style="width:60pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">CityFlow</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Both</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">271</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">149</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2019</p></td><td style="width:68pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN/LSTM</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:92pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">QL/DQN</p></td><td style="width:78pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:69pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">16</p></td><td style="width:60pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">60</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark191" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">150</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:68pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DDPG/LSTM</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:92pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/DDPG/DQN</p></td><td style="width:78pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:69pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">2-16</p></td><td style="width:60pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">209</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark192" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">151</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2021</p></td><td style="width:68pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN/PPO</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:92pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">PL/CL</p></td><td style="width:78pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Jinan (CN)</p></td><td style="width:69pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">12/16/48/33</p></td><td style="width:60pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">CityFlow</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Real</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">43</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark193" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">152</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2020</p></td><td style="width:68pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN/FLC</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:92pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FLC/ANN/RBC</p></td><td style="width:78pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Gwalior (IN)</p></td><td style="width:69pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">-</p></td><td style="width:60pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">132</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark194" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">153</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2021</p></td><td style="width:68pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">A2C/GAT</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:92pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">A2C/IQL/SOTL</p></td><td style="width:78pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Monaco (MC)</p></td><td style="width:69pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">30</p></td><td style="width:60pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Both</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">42</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark195" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">154</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2021</p></td><td style="width:68pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">QL/LSTM</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:92pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/QL</p></td><td style="width:78pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:69pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">16</p></td><td style="width:60pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Aimsun</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">50</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark196" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">155</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2021</p></td><td style="width:68pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">PPO/MPC</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:92pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/PPO/Other</p></td><td style="width:78pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Dalian (CN)</p></td><td style="width:69pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">9</p></td><td style="width:60pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Both</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">38</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark197" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">156</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2022</p></td><td style="width:68pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN/LSTM</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:92pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/CL/GAT/Other</p></td><td style="width:78pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Hangzhou (CN)</p></td><td style="width:69pt"><p class="s55" style="padding-left: 15pt;text-indent: 0pt;line-height: 10pt;text-align: left;">16/12/16</p></td><td style="width:60pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">CityFlow</p></td><td style="width:32pt"><p class="s55" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Both</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">38</p></td></tr><tr style="height:11pt"><td style="width:31pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark198" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">157</span>]</p></td><td style="width:30pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2023</p></td><td style="width:68pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DQN/FLC</p></td><td style="width:37pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Single</p></td><td style="width:92pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">FT/QL/FLC</p></td><td style="width:78pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Synthetic</p></td><td style="width:69pt"><p class="s55" style="text-indent: 0pt;line-height: 10pt;text-align: center;">1</p></td><td style="width:60pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">35</p></td></tr><tr style="height:14pt"><td style="width:31pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">[</a><span style=" color: #0774B7; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">158</span>]</p></td><td style="width:30pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2024</p></td><td style="width:68pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">DDQN/GAT</p></td><td style="width:37pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Multi</p></td><td style="width:92pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">CL/MP/IGRL</p></td><td style="width:78pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Qingdao (CN)</p></td><td style="width:69pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 15pt;text-indent: 0pt;line-height: 10pt;text-align: left;">45/57/12</p></td><td style="width:60pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">SUMO</p></td><td style="width:32pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sim</p></td><td style="width:26pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s55" style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;">10</p></td></tr></table><h4 style="padding-top: 2pt;padding-left: 136pt;text-indent: 0pt;text-align: left;"><a name="bookmark37">Table 9. </a><span class="s14">Summaries of hybrid RL applications for traffic signal control.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="695" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_065.png"/></span></p><h4 style="padding-top: 1pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Author                          Summary</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_066.png"/></span></p><p class="s61" style="padding-left: 102pt;text-indent: -90pt;line-height: 11pt;text-align: justify;"><a href="#bookmark188" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Darmoul et al. [</a>147<span style=" color: #000;">] Proposed INAMAS, a hierarchical framework combining RL with artificial immune networks for adaptive control. Agents learned phase sequencing offline and adjusted online based on queue length, wait time, and time-of-day states. VISSIM results showed 32.07% and 27.85% delay reduction in high and extreme congestion vs. FT and LQF-MWM.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_067.png"/></span></p><p class="s61" style="padding-left: 102pt;text-indent: -90pt;line-height: 11pt;text-align: justify;"><a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Wei et al. [</a>148<span style=" color: #000;">] Developed CoLight, a MARL model integrating DQN and graph attention networks (GATs) for spatial- temporal coordination. States included phase and vehicle count per lane; actions selected signal phases. In CityFlow (NY, Jinan, Hangzhou), it reduced travel time by 19.89% vs. MaxPressure and improved efficiency 6.98–11.69%.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_068.png"/></span></p><p class="s61" style="padding-left: 102pt;text-indent: -90pt;line-height: 11pt;text-align: justify;"><a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Kim et al. [</a>149<span style=" color: #000;">] Enhanced DQN with prioritized experience replay, double/dueling DQN, LSTM traffic prediction, and neighbor sharing in a 4×4 SUMO grid. States used vehicle/lane count and LSTM forecast; actions controlled green phases. Outperformed QL and DQN in delay, wait, and QL under real-world condition modeling.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_069.png"/></span></p><p class="s14" style="padding-top: 4pt;padding-left: 101pt;text-indent: -90pt;line-height: 56%;text-align: justify;"><a href="#bookmark191" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Wu et al. [</a><span style=" color: #0774B7;">150</span>]      Used multi-agent R-DDPG and LSTM to improve coordination under partial observability. States included queue length, speed, and pedestrian/bus data; actions toggled binary phase. SUMO simulations (<span class="s60">≤</span>16 ints.)</p><p class="s14" style="padding-left: 18pt;text-indent: 0pt;text-align: center;">reduced queue length by 22.08%, vehicle delay by 9.57%, and pedestrian wait by 19.66%.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_070.png"/></span></p><p class="s14" style="padding-left: 102pt;text-indent: -90pt;line-height: 11pt;text-align: justify;"><a href="#bookmark193" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Kumar et al. [</a><span style=" color: #0774B7;">152</span>] Combined DQN with fuzzy inference for single-agent phase switching using vehicle grid state. Reward minimized wait, queue length, and emissions. SUMO results showed 14.21% CO<span class="s59">2</span><span class="s8"> </span>reduction, shorter AWT, and higher throughput than fuzzy and NN controllers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_071.png"/></span></p><p class="s61" style="padding-left: 101pt;text-indent: -90pt;line-height: 11pt;text-align: justify;"><a href="#bookmark192" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Xu et al. [</a>151<span style=" color: #000;">]     Proposed HiLight, a hybrid DQN and PPO hierarchical MARL optimizing sub-policy and high-level control. States included phase and vehicle counts, while actions considered the phase choice. Trained in CityFlow (Jinan, Hangzhou, Manhattan), HiLight reduced travel time 9.6–20.1% and improved throughput 2.5–10.5% vs. CoLight and PressLight.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_072.png"/></span></p><p class="s61" style="padding-left: 102pt;text-indent: -90pt;line-height: 11pt;text-align: justify;"><a href="#bookmark194" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Wang et al. [</a>153<span style=" color: #000;">] Introduced RACS, a hybrid, A2C and GAT, framework that enhanced coordination via neighboring state attention. Tested on Monaco and 5 × 5 grid, it reduced wait time by 22% vs. IA2C, 14% vs. IQL-DNN, and outperformed other baselines in convergence speed and stability.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_073.png"/></span></p><p class="s61" style="padding-left: 101pt;text-indent: -90pt;line-height: 11pt;text-align: justify;"><a href="#bookmark195" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Abdoos et al. [</a>154<span style=" color: #000;">] Used a hierarchical hybrid, QL and LSTM, prediction model for traffic signal coordination in a 16-intersection AIMSUN network. The state actions concerned queue length, delay, and green time, while the reward considered delay reduction. Achieved 23.7% lower delay vs. FT and standard QL control.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_074.png"/></span></p><p class="s61" style="padding-left: 102pt;text-indent: -90pt;line-height: 11pt;text-align: justify;"><a href="#bookmark196" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Guo et al. [</a>155<span style=" color: #000;">] Proposed PPOMA, combining PPO with MPC for tram priority and general traffic. States included real-time speed/position, while actions adjusted signal timing. SUMO results showed &gt;90% tram stop reduction and lower wait times vs. baseline control.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_075.png"/></span></p><p class="s61" style="padding-left: 102pt;text-indent: -90pt;line-height: 11pt;text-align: justify;"><a href="#bookmark197" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Wang et al. [</a>156<span style=" color: #000;">] Presented MetaSTGAT, combining GAT, LSTM, and meta-learning with DQN for adaptive control. Meta- learner updated GAT/LSTM weights dynamically. On CityFlow—using synthetic and real data—travel time dropped by 19.3% vs. CoLight and other baselines</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_076.png"/></span></p><p class="s14" style="padding-left: 102pt;text-indent: -90pt;line-height: 11pt;text-align: justify;"><a href="#bookmark198" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Tunc et al. [</a><span style=" color: #0774B7;">157</span>]     Developed DQ-FLSI, combining DQN for phase and FLC for green duration; states used adaptive lane- based vehicle positions. SUMO results showed average queue length = 8.88 vehicles, delay = 13.32 h, and CO<span class="s59">2</span><span class="s8"> </span>= 228.7 kg, outperforming FT, fuzzy, and standard RL controllers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_077.png"/></span></p><p class="s61" style="padding-left: 102pt;text-indent: -90pt;line-height: 11pt;text-align: justify;"><a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">Wang et al. [</a>158<span style=" color: #000;">]   Introduced MGMQ, combining DDQN with GAT and GraphSAGE using multi-layer graphs and action masks for phase control. Tested on SUMO (45 intersections), it reduced delay by 40%, queue length by 26%, and congestion by 35%, outperforming baseline control.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="695" height="1" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_078.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 149pt;text-indent: -12pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark38">Evaluation</a><a name="bookmark47">&zwnj;</a></h2></li></ol><p style="padding-top: 4pt;padding-left: 136pt;text-indent: 21pt;line-height: 112%;text-align: left;">To offer a structured and comparative overview of existing RL solutions for RES- integrated BEMS, the evaluation focuses on seven key attributes:</p><ol id="l19"><li style="padding-top: 2pt;padding-left: 159pt;text-indent: -23pt;text-align: left;"><h3 style="display: inline;">Methodology and Type: <span class="p">Defines the core structure and features of each RL algorithm.</span></h3></li><li style="padding-top: 1pt;padding-left: 159pt;text-indent: -23pt;line-height: 112%;text-align: left;"><h3 style="display: inline;">Agent Architectures:  <span class="p">Explores prevailing agent architectures, emphasizing multi- agent RL trends.</span></h3></li><li style="padding-left: 159pt;text-indent: -23pt;text-align: left;"><h3 style="display: inline;">Reward Functions: <span class="p">Analyzes reward design variations across implementations.</span></h3></li><li style="padding-top: 1pt;padding-left: 159pt;text-indent: -22pt;text-align: left;"><h3 style="display: inline;">Performance Indexes: <span class="p">Identifies commonly used evaluation metrics and their characteristics.</span></h3></li><li style="padding-top: 1pt;padding-left: 159pt;text-indent: -23pt;text-align: left;"><h3 style="display: inline;">Baseline Control Types: <span class="p">Reviews baseline strategies used to benchmark RL performance.</span></h3></li><li style="padding-top: 1pt;padding-left: 159pt;text-indent: -23pt;line-height: 112%;text-align: left;"><h3 style="display: inline;">Intersection Sizes: <span class="p">Assesses intersection modeling complexity, revealing scalability challenges.</span></h3></li><li style="padding-top: 2pt;padding-left: 103pt;text-indent: -21pt;line-height: 136%;text-align: left;"><h3 style="display: inline;">Simulation Tools: <span class="p">Lists the platforms used for simulating RL-based traffic control. These dimensions capture the critical factors in designing, deploying, and benchmark-</span></h3></li></ol><p style="padding-left: 82pt;text-indent: 0pt;line-height: 9pt;text-align: left;">ing RL-based controllers. Dissecting the literature along these axes helps readers under-</p><p style="padding-top: 1pt;padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: left;">stand how RL techniques align with various traffic control frameworks, design constraints, and performance objectives—supporting informed choices for different network scenarios.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l20"><ol id="l21"><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark39">Methodologies and Types</a></p><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><span class="h3">Value-Based: </span><a href="#bookmark48" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Value-based methods dominate the RL landscape for TSC (see Figure </a>5<a href="#bookmark49" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, left and right), with QL and DQN widely applied—DQN being the most prominent (Figure </a>6<a href="#bookmark149" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, left and right). QL is favored for its simplicity in discrete state-action spaces, while DQN extends capabilities to high-dimensional environments via deep function approxima- tion. The evolution of RL methods is marked by a shift from basic QL to advanced DQN variants such as DDQN, dueling DQN, and hierarchical multi-agent frameworks. This tran- sition reflects the need for richer state representations and improved scalability [</a>108<a href="#bookmark152" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>111<a href="#bookmark144" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Early studies focused on single intersections or agents [</a>103<a href="#bookmark148" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>107<a href="#bookmark146" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], while more recent efforts explore multi-agent systems and hierarchical coordination [</a>105<a href="#bookmark169" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>128<span style=" color: #000;">]. These decentralized frameworks enable scalable and synchronized control across complex networks.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="522" height="157" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_079.jpg"/></span></p><h4 style="padding-top: 6pt;padding-left: 82pt;text-indent: 0pt;text-align: left;"><a name="bookmark48">Figure 5. </a><span class="s14">RL types occurrence (</span>left<span class="s14">) and share (%) (</span>right<span class="s14">) for traffic signal control.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 81pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark147" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Another key trend is the integration of domain-specific modeling, including connected vehicle (CV) data [</a>106<a href="#bookmark151" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], partial observability [</a>110<a href="#bookmark158" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and explicit safety constraints [</a>117<a href="#bookmark170" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>129<a href="#bookmark145" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Advances in graph-based neural networks and spatial feature extraction [</a>104<a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>121<a href="#bookmark153" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] further enable adaptive policies that account for topology and vehicle dynamics. Meta-learning approaches [</a>112<a href="#bookmark155" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], non-stationary adaptation [</a>114<a href="#bookmark173" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and multi-objective optimization (e.g., emissions, safety, equity) [</a>132<a href="#bookmark152" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] signal a more holistic view of urban mobility. Additionally, work on edge computing [</a>111<a href="#bookmark172" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] and new sensor modalities [</a>131<span style=" color: #000;">] reflects the push toward real-world deployment. These developments collectively indicate a mature field evolving methodologically and operationally.</span></p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><span class="h3">Policy-Based: </span><a href="#bookmark174" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Initial efforts, such as [</a>133<a href="#bookmark175" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], leveraged convolutional networks, showcas- ing stable policy optimization without value estimation. Later work utilized image-based inputs [</a>134<a href="#bookmark176" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], attention mechanisms [</a>135<a href="#bookmark177" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and CAV coordination [</a>136<span style=" color: #000;">], expanding the method’s applicability. Despite these advances, however, policy-based RL remains less prevalent due to high sample complexity and training variance. Its strength in continu- ous action control contrasts with its limitations in discrete settings, where value-based methods are more efficient. Nonetheless, newer strategies incorporating attention and actor-critic frameworks are addressing these challenges by improving sample efficiency and robustness.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="522" height="157" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_080.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 3pt;padding-left: 82pt;text-indent: 0pt;text-align: left;"><a name="bookmark49">Figure 6. </a><span class="s14">RL methodologies occurrence (</span>left<span class="s14">) and share (%) (</span>right<span class="s14">) for traffic signal control.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><span class="h3">Actor-Critic: </span><a href="#bookmark178" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Such combined methods have progressed from simple continuous-state applications [</a>137<a href="#bookmark179" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] to complex multi-agent frameworks using policy sharing and advanced networks. MA2C models [</a>138<a href="#bookmark180" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] introduced decentralized learning, while improvements like neighborhood fingerprinting and spatial discounting enhanced coordination [</a>139<a href="#bookmark184" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>143<a href="#bookmark181" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. DDPG architectures [</a>140<a href="#bookmark182" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] employ global critics and local actors to balance centralized training with decentralized control. Recent innovations include LSTM-based estimators for CAV–human synergy [</a>141<a href="#bookmark183" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], graph neural networks for inductive spatial features [</a>142<a href="#bookmark186" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and asymmetric designs for partial CV data [</a>145<a href="#bookmark187" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Actor-critic models have also been adapted to safety-critical tasks, such as emergency vehicle prioritization [</a>146<span style=" color: #000;">], reinforcing their versatility in diverse traffic scenarios.</span></p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><span class="h3">Hybrids: </span><a href="#bookmark188" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Hybrid RL combines multiple learning strategies and external frameworks (e.g., fuzzy logic, immune networks) to tackle complex traffic objectives. Early approaches integrated RL with immune learning [</a>147<a href="#bookmark193" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], while others adopted fuzzy logic for adaptive reward tuning [</a>152<a href="#bookmark198" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>157<a href="#bookmark192" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Hierarchical and layered architectures now blend DQN and PPO modules [</a>151<a href="#bookmark195" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>154<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], often using deep models like GATs [</a>148<a href="#bookmark194" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>153<a href="#bookmark191" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] and LSTMs [</a>150<a href="#bookmark197" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Hybrid frameworks increasingly feature meta-learning and graph-based reasoning [</a>156<a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], with centralized critics and knowledge-sharing mechanisms [</a>149<a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>158<a href="#bookmark196" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] enhancing coordination. Model-based modules [</a>155<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] have been also integrated to reduce learning time and improve policy performance. Among hybrid approaches, DQN–ANN combinations represent the most common hybrid scheme [</a>148<a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>149<a href="#bookmark194" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>153<a href="#bookmark193" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], followed by DQN–FLC methods [</a>152<a href="#bookmark198" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>157<a href="#bookmark50" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] (see Figure </a>7<a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, left). ANN-based hybrids—particularly those using LSTM [</a>149<a href="#bookmark191" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>150<a href="#bookmark197" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>156<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] and GAT [</a>148<a href="#bookmark194" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>153<a href="#bookmark197" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>156<a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>158<a href="#bookmark50" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]—dominate, while FLC remains a competitive alternative (see Figure </a>7<span style=" color: #000;">, right).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="522" height="157" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_081.jpg"/></span></p><h4 style="padding-top: 6pt;padding-left: 82pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark50">Figure 7. </a><span class="s14">Hybrid scheme occurrence (</span>left<span class="s14">) and algorithmic counterparts (%) (</span>right<span class="s14">) in hybrid RL applications.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">Future developments will likely include meta-learning for faster adaptation, transfer learning for model reuse, and expanded integration with V2X, pedestrian detection, and multimodal transport systems. These trends will drive RL solutions to optimize safety, emissions, travel time, and equity. Finally, advances in sim-to-real transfer, edge deploy-</p><p style="padding-top: 2pt;padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: left;">ment, and explainable RL will accelerate the adoption of adaptive, data-driven urban traffic control at the city scale.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark40">Agent Architectures</a></p><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark51" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Multi-agent RL (MARL) approaches clearly dominate the traffic signal control liter- ature, particularly throughout the 2015–2025 period. Over 60% of studies utilize MARL methods for managing complex intersection scenarios (see Figure </a>8<a href="#bookmark152" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, left and right). This trend reflects a shift toward more realistic, large-scale applications. Technological ad- vances in parallel computing, distributed systems, and the availability of open-source traffic simulators—such as SUMO and CityFlow—have significantly enabled scalable, decentralized MARL designs for urban traffic networks [</a>111<a href="#bookmark160" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>119<a href="#bookmark161" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>120<a href="#bookmark187" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>146<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>148<a href="#bookmark196" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>155<span style=" color: #000;">].</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="522" height="157" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_082.jpg"/></span></p><h4 style="padding-top: 6pt;padding-left: 82pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark51">Figure 8. </a><span class="s14">Multi-agent vs. single-agent RL applications occurrence (</span>left<span class="s14">) and share (%) (</span>right<span class="s14">) for traffic signal control via reinforcement learning.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark52" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">As Figure </a>9 <a href="#bookmark154" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">(left and right) illustrates, value-based methods are the most prevalent MARL frameworks. MARL–DQN is particularly dominant, with numerous studies applying DQNs for decentralized optimization across intersection networks [</a>113<a href="#bookmark160" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>119<a href="#bookmark161" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>120<a href="#bookmark164" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>123<a href="#bookmark165" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>124<a href="#bookmark171" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>130<a href="#bookmark186" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>145<a href="#bookmark188" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>147<a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>149<a href="#bookmark197" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>156<a href="#bookmark146" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. These approaches demonstrate high scalabil- ity and robustness, especially when combined with local observations and experience replay. Classical MARL QL remains relevant [</a>105<a href="#bookmark147" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>106<a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>149<a href="#bookmark195" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>154<a href="#bookmark160" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], particularly in grid-based or hierarchical settings where simpler Q-value models suffice. More advanced versions like DDQN and 3DQN, though promising, are underexplored, with limited implementa- tions showing potential to stabilize learning in high-dimensional settings [</a>119<a href="#bookmark165" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>124<a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. On the actor-critic front, A2C and A3C models appear most frequently [</a>121<a href="#bookmark183" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>142<a href="#bookmark185" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>144<a href="#bookmark192" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>151<a href="#bookmark194" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>153<a href="#bookmark177" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], using policy-gradient methods and entropy regularization to support distributed behaviors. DDPG-based systems [</a>136<a href="#bookmark180" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>139<a href="#bookmark181" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>140<a href="#bookmark52" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] also show promise, enabling continuous control and precise signal adjustments. However, their adoption is limited by challenges in reward design and instability under partial observability (see Figure </a>9<span style=" color: #000;">, left and right).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="522" height="157" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_083.jpg"/></span></p><h4 style="padding-top: 6pt;padding-left: 82pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark52">Figure 9.  </a><span class="s14">Multi-agent RL methodologies occurrence (</span>left<span class="s14">) and share (%) (</span>right<span class="s14">) for traffic signal control.</span></h4><p style="padding-top: 2pt;padding-left: 81pt;text-indent: 21pt;line-height: 112%;text-align: justify;">The comprehensive analysis of the reviewed studies reveals that, to date, no multi- agent traffic signal control application has been deployed in real-world environments. Even though MARL-based methods have become increasingly sophisticated and show great promise in simulations, bringing them into real-world operation remains a major challenge. A big part of the difficulty comes from the fact that traffic patterns vary greatly between different real-life traffic networks, making it hard to create models that generalize well. Building reliable, fast, and secure communication networks concerns another ma- jor hurdle, and even when strong simulation results exist, transferring those models to the messy, unpredictable real world is far from straightforward. Coordination between agents also becomes much harder taking into account real-world issues like communica- tion delays, partial views of the environment, and the unpredictable behavior of human drivers—factors that simulations often oversimplify or overlook. On top of that, problems like scaling up to bigger real-life networks, dealing with unexpected traffic events, ensuring communication standards, and meeting regulatory and safety requirements all add more layers of complexity. Inevitably, overcoming such challenges will be essential to transition from promising research prototypes to fully operational smart traffic systems.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark41">Reward Functions</a></p><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">In RL-based traffic signal control, the reward function defines the optimization ob- jectives by assigning feedback based on each action’s performance. The reward design plays a crucial role in guiding the agent’s behavior—well-aligned functions promote ef- fective traffic management, while poorly constructed ones can hinder learning or lead to undesirable outcomes. Based on our evaluation, reward functions in TSC fall into six primary categories:</p><ul id="l22"><li style="padding-top: 4pt;padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Queue/Waiting-Time-Based: </span><a href="#bookmark144" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Such functions measure and penalize vehicle queues or total waiting time, offering direct feedback to reduce congestion. This is the most common reward type, due to its simplicity and effectiveness [</a>103<a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>121<a href="#bookmark184" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>143<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>148<a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>149<a href="#bookmark53" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] (see Figure </a>10<span style=" color: #000;">).</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Delay/Throughput-Based:  </span><a href="#bookmark146" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">These  reward  structures  target  average  vehicle delay or throughput, encouraging  faster  clearance  and  efficient  flow.  Studies like [</a>105<a href="#bookmark174" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>133<a href="#bookmark176" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>135<a href="#bookmark180" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>139<a href="#bookmark195" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>154<span style=" color: #000;">] apply these to align control with travel time efficiency.</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Pressure/Cost-Based: </span><a href="#bookmark156" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">These rewards assess the difference between inflow and outflow to balance traffic at intersections. By minimizing intersection pressure, they prevent excessive queue buildup [</a>115<a href="#bookmark177" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>136<a href="#bookmark186" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>145<a href="#bookmark197" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>156<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Safety/Conflict-Focused: </span><a href="#bookmark157" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">These functions penalize potential collisions or unsafe behav- iors, promoting safer signal plans. Though less common due to modeling complexity, notable examples include [</a>116<a href="#bookmark158" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>117<a href="#bookmark170" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>129<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Emissions/Fuel-Centered: </span><a href="#bookmark171" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">These rewards promote eco-efficiency by minimizing fuel use and emissions. Studies like [</a>130<a href="#bookmark173" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>132<span style=" color: #000;">] integrate such metrics with other objectives to support sustainable traffic control.</span></p></li><li style="padding-left: 103pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Hybrid/Multi-Objective: </span><a href="#bookmark159" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">These combine several performance indicators—delays, queues, emissions, safety—into a composite reward. Examples include [</a>118<a href="#bookmark168" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>127<a href="#bookmark196" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>155<span style=" color: #000;">], where fairness, tram priority, and pedestrian flow are jointly optimized.</span></p><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark53" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">As illustrated in Figure </a>10<span style=" color: #000;">, a large proportion of studies (43%) primarily focus on queue length or waiting time minimization, followed by delay/throughput objectives (16%), and only a small fraction address safety (5%) or environmental impacts (4%). Hybrid or multi-objective reward functions account for 25% of the evaluated studies, reflecting a growing recognition of the need to address the inherently multi-objective, nonlinear, and dynamic nature of real-world TSC systems.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="544" height="163" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_084.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 3pt;padding-left: 82pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark53">Figure 10. </a><span class="s14">Reward function types occurrence (</span>left<span class="s14">) and share (%) (</span>right<span class="s14">) for traffic signal control via reinforcement learning.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">According to the evaluation, a substantial gap remains in systematically balancing efficiency, safety, environmental sustainability, and fairness within RL frameworks. Existing reward designs predominantly focus on optimizing a limited number of objectives, often neglecting others, limiting the robustness and adaptability of learned policies under the variability and uncertainty of real-world traffic conditions. Future research will potentially require the development of hybrid reward functions that integrate multiple heterogeneous performance indicators, where the relative importance of objectives such as delay minimiza- tion, collision risk reduction, and emission control may dynamically adapt to varying traffic contexts. Moreover, incorporating constrained reinforcement learning techniques, where safety-critical or environmental thresholds are explicitly embedded within the learning process, could ensure compliance with regulatory or societal requirements even as agents optimize their policies. An additional promising direction involves the application of Pareto-optimal multi-objective learning, enabling agents to discover a spectrum of optimal trade-offs between competing goals without relying on static, hand-crafted weightings. Through these advancements, RL agents could achieve greater generalization and resilience, ultimately making them more applicable to the complex, heterogeneous, and evolving demands of real-world urban traffic management systems.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark42">Performance Indexes</a></p><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">Performance metrics are used to evaluate how effectively a trained reinforcement learning agent manages traffic signal control. While they may align with the reward function used during training, they often extend beyond it to provide a more comprehensive assessment of the agent’s real-world impact. Common metrics include average travel time, vehicle throughput, stop frequency, and emissions levels. Such indicators help researchers and practitioners compare different approaches, validate generalizations, and ensure that the learned policies meet broader traffic management objectives.</p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark54" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Based on the collective analysis of value-based, policy-based, actor-critic, and hybrid RL methodologies for traffic signal control, the most prevalent target outputs relate to travel time/delay reduction, queue length reduction, waiting time reduction, scalability to large networks, and emission reduction (see Figure </a>11<span style=" color: #000;">, left). </span><span class="h3">Travel time and delay reduction </span><a href="#bookmark178" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">emerged as the most dominant performance indicator across all RL types, demonstrating its central role in evaluating TSC efficiency. Numerous studies reported impressive improve- ments, including Aslani et al. [</a>137<a href="#bookmark187" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] (59.2%), Su et al. [</a>146<a href="#bookmark176" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] (23.5%), Oroojlooy et al. [</a>135<a href="#bookmark192" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] (46%), and Xu et al. [</a>151<span style=" color: #000;">] (up to 20.1%). This trend reflected the field’s focus on enhancing network-level fluidity and commuter experience by minimizing delay under dynamic conditions. </span><span class="h3">Queue length reduction </span><a href="#bookmark179" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">was also a consistent target, especially in value-based and actor-critic methods, emphasizing localized congestion management. Studies such as Chu et al. [</a>138<a href="#bookmark184" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], Damadam et al. [</a>143<a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and Wang et al. [</a>158<span style=" color: #000;">] highlighted reductions ranging</span></p><p style="padding-top: 2pt;padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: left;">from 18% to over 40%, illustrating how RL adapts to node-level traffic accumulation and optimizes phase switching accordingly.</p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><span class="h3">Waiting time reduction </span><a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">aligns closely with the above metrics but emphasizes user- centric performance. It was commonly optimized in hybrid and actor-critic setups like Kim et al. [</a>149<a href="#bookmark182" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], Guo et al. [</a>141<a href="#bookmark193" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and Kumar et al. [</a>152<span style=" color: #000;">], confirming its relevance in enhancing intersection throughput and driver satisfaction, especially under heterogeneous traffic loads. </span><span class="h3">Scalability </span><a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">to large networks surfaced as a vital output, particularly in multi-agent and graph-based methods such as Wei et al. [</a>148<a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] (tested on 196 intersections), Wang et al. [</a>158<a href="#bookmark185" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and Wu et al. [</a>144<span style=" color: #000;">]. Such works prove the growing maturity of RL frameworks that can generalize across cities and dynamically coordinate multiple nodes in parallel, a key enabler for real-world deployment. Moreover, </span><span class="h3">emission reduction</span><a href="#bookmark178" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, while less frequently reported, represents an emerging priority, especially within hybrid systems that integrate sustainability objectives. Significant reductions were achieved by Aslani et al. [</a>137<a href="#bookmark147" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], Aziz et al. [</a>106<a href="#bookmark198" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and Tunc et al. [</a>157<span style=" color: #000;">], showcasing RL’s potential in aligning traffic control with environmental policies.</span></p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark181" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Beyond the core performance indicators, several other target outputs were identified, reflecting more specialized or emerging research interests. These include throughput increase, often used as a complementary metric to assess network capacity improvements [</a>140<a href="#bookmark182" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>141<a href="#bookmark158" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]; crash/conflict reduction, focusing on safety-aware control strategies such as in [</a>117<a href="#bookmark170" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>129<a href="#bookmark152" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]; and training stability or policy learning speed, important in studies seeking real-time deployment or efficient learning in complex environments (e.g., [</a>111<a href="#bookmark174" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>133<a href="#bookmark183" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]). Additionally, outputs like generalization to unseen scenarios and zero-shot transfer [</a>142<a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>158<span style=" color: #000;">] highlight the growing interest in robust, transferable RL policies. Collectively, these secondary targets underscore the field’s progression toward safe, reliable, and adaptable RL-based traffic control systems.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="522" height="157" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_085.jpg"/></span></p><h4 style="padding-top: 6pt;padding-left: 82pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark54">Figure 11. </a><span class="s14">Performance index type occurrence (</span>left<span class="s14">) and share (%) (</span>right<span class="s14">) for traffic signal control via reinforcement learning.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 81pt;text-indent: 21pt;line-height: 112%;text-align: right;"><a href="#bookmark54" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">In summary, the most prevalent target outputs show a clear shift in RL research toward not only improving traffic flow and reducing delays but also scaling solutions to real-world urban systems, enhancing responsiveness, and incorporating sustainability. This trend signals an evolution from early-stage intersection-specific optimization to city-scale, multi-agent coordination frameworks that balance mobility, efficiency, and environmental impact—a direction likely to dominate future research in intelligent transportation systems. The classification of target outputs into four thematic groups offers a comprehensive  view of the evolving research priorities in RL-based TLC (see Figure </a>11<span style=" color: #000;">, right).   The efficiency-oriented outputs group—comprising travel time/delay reduction, queue length, waiting time, throughput, and green phase optimization—reflects the field’s core mission to enhance traffic flow and intersection performance. The scalability and learning-centric outputs focus on the robustness and deployability of RL models, addressing challenges like large-scale coordination, stable learning, and rapid policy adaptation through metrics</span></p><p style="padding-top: 2pt;padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: justify;">such as generalization and policy convergence. The sustainability-oriented outputs, which include emission and fuel consumption reductions, show an emerging environmental consciousness in traffic management, integrating RL with broader climate and energy goals. Lastly, the safety and real-time responsiveness group introduces a human-centric layer, prioritizing crash/conflict reduction, responsiveness to dynamic traffic or emergency events, and overall congestion mitigation—highlighting a shift toward safer, more resilient urban mobility systems.</p><p style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">The grouped target outputs reveal a clear prioritization in RL-based TSC research. Efficiency-oriented goals dominate, highlighting the field’s longstanding emphasis on reducing travel time, delay, and congestion to improve urban mobility. Scalability and learning-centric outputs reflect growing interest in deploying RL methods across large, com- plex networks while ensuring generalization, stability, and adaptability—key for real-world applicability. Meanwhile, the increasing attention to safety and real-time responsiveness shows a shift toward more human-centric and reactive systems, especially under mixed traffic and emergency scenarios. Finally, the presence of sustainability-oriented outputs, though less frequent, signals an emerging alignment with environmental objectives, mark- ing a multidimensional evolution of RL-TLC research from isolated efficiency gains to broader, integrated smart city priorities.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: justify;"><p class="s2" style="display: inline;"><a name="bookmark43">Baseline Control Types</a></p><p style="padding-top: 4pt;padding-left: 81pt;text-indent: 21pt;line-height: 112%;text-align: justify;">In RL for traffic signal control, baseline methods serve as benchmarks for evaluating proposed RL approaches. These include classical control strategies (e.g., FT, AC), heuristic models (e.g., max-pressure (MP), rule-based control (RBC)), and prior RL-based frame- works. Employing robust baselines is essential for assessing effectiveness, adaptability, and real-world feasibility; thus, evaluations benefit from comparisons across both traditional and state-of-the-art methods.</p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><span style=" color: #000;">The current review identifies </span><span class="h3">RL algorithms</span><a href="#bookmark55" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">—such as QL, DQN, A2C, and FRAP—as the most commonly used baselines (see Figure </a>12<a href="#bookmark149" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, left), reflecting a trend toward bench- marking against advanced RL models rather than solely conventional techniques [</a>108<a href="#bookmark165" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>124<span style=" color: #000;">]. </span><span class="h3">Light RL </span><a href="#bookmark153" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">frameworks like CoLight, PressLight, and MetaLight, known for integrating graph structures, meta-learning, or pressure logic, are often employed as RL baselines due to their strong performance in multi-agent coordination and generalization [</a>112<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>148<a href="#bookmark192" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>151<span style=" color: #000;">]. The inclusion of </span><span class="h3">ANN-based </span><a href="#bookmark156" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">architectures (e.g., GAT, GCN) as baselines further demon- strates the growing relevance of deep representation learning in capturing topological traffic dependencies [</a>115<a href="#bookmark197" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>156<a href="#bookmark55" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. These intelligent baselines exhibit strong adaptability, real-time decision making, and coordinated control across complex networks (see Figure </a>12<span style=" color: #000;">, right). </span><span class="h3">Classical </span><a href="#bookmark148" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">control strategies, including fixed-time and actuated control, remain prevalent baseline choices. FT control, long dominant in urban systems, is favored for its simplicity, reliability, and historical use, serving as a static benchmark [</a>107<a href="#bookmark168" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>127<a href="#bookmark146" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. AC, by contrast, adjusts signals based on sensor input, offering a middle ground between static and fully adaptive systems [</a>105<a href="#bookmark157" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>116<span style=" color: #000;">]. </span><span class="h3">Heuristic-based </span><a href="#bookmark183" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">approaches, such as rule-based control (RBC) and max-pressure (MP), offer interpretable, domain-informed logic. MP, in particular, is notable for its decentralized, queue-aware control that performs competitively with modern RL models [</a>142<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>148<a href="#bookmark150" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. RBC applies predefined rules without learning, offering simplicity and serving as a stand-in for traffic engineering expertise [</a>109<a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>121<span style=" color: #000;">]. The self-organizing traffic lights (SOTL) model also appears in this category, using local heuristics—such as vehicle counts or wait thresholds—for decentralized decisions.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="522" height="157" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_086.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 3pt;padding-left: 82pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark55">Figure 12. </a><span class="s14">Baseline control types occurrence (</span>left<span class="s14">) and share (%) (</span>right<span class="s14">) for traffic signal control via reinforcement learning.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark193" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Less common baselines include priority-based methods, gap-based logic, and hybrid models like MPC and FLC. Such baselines typically address niche scenarios or integrate expert knowledge into fuzzy logic frameworks [</a>152<a href="#bookmark196" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>155<span style=" color: #000;">], benchmarking RL agents in tasks requiring nuanced control strategies.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark44">Intersections Sizes</a></p><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">The number of intersections modeled in RL studies directly impacts the computational complexity of the traffic network. As intersection count increases, the problem scales from localized optimization to dynamic, multi-agent coordination—demanding more advanced learning architectures. This review categorizes intersection complexity into four types:</p><ul id="l23"><li style="padding-top: 2pt;padding-left: 104pt;text-indent: -21pt;text-align: left;"><h3 style="display: inline;">Single-scale: <span class="p">1 intersection;</span></h3></li><li style="padding-top: 1pt;padding-left: 104pt;text-indent: -21pt;text-align: left;"><h3 style="display: inline;">Minimal-scale: <span class="p">2–6 intersections;</span></h3></li><li style="padding-top: 1pt;padding-left: 104pt;text-indent: -21pt;text-align: left;"><h3 style="display: inline;">Moderate-scale: <span class="p">7–30 intersections;</span></h3></li><li style="padding-top: 1pt;padding-left: 104pt;text-indent: -21pt;text-align: left;"><h3 style="display: inline;">Large-scale: <span class="p">30+ intersections.</span></h3><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark56" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">To this end, according to the comprehensive examination of each integrated high- impact research work, Figure </a>13 <a href="#bookmark56" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">(left) shows the distribution of study types, while Figure </a>13 <span style=" color: #000;">(right) contrasts single vs. multi-intersection implementations.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="522" height="157" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_087.jpg"/></span></p><h4 style="padding-top: 6pt;padding-left: 82pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark56">Figure 13.  </a><span class="s14">Intersection types occurrence (</span>left<span class="s14">) and share (%) (</span>right<span class="s14">) for traffic signal control via reinforcement learning.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><span style=" color: #000;">According to the evaluation, most research remains concentrated on </span><span class="h3">single-scale </span><a href="#bookmark56" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">scenarios (see Figure </a>13<a href="#bookmark144" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, left and right), where RL models are first tested under controlled conditions. These studies often benchmark against fixed-time or actuated systems. Ex- amples include DQN-based controllers for single intersections [</a>103<a href="#bookmark148" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>107<a href="#bookmark193" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and fuzzy–DQN hybrids [</a>152<span style=" color: #000;">], offering enhanced adaptability in real-world settings. </span><span class="h3">Minimal-scale </span><a href="#bookmark145" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">scenar- ios mark the initial transition toward MARL coordination. These works typically simulate small networks (e.g., 2 × 2 or 2 × 3 grids) to evaluate local cooperation. Nishi et al. [</a>104<a href="#bookmark165" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] used a GCNN-based RL controller across six intersections, while Haddad et al. [</a>124<span style=" color: #000;">] showed coordination improvements in 2 × 2 and 2 × 3 grids. </span><span class="h3">Moderate-scale </span><a href="#bookmark56" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">studies are signifi- cantly less frequent (see Figure </a>13<span style=" color: #000;">, left), highlighting a gap in research targeting mid-sized</span></p><p class="s15" style="padding-top: 2pt;padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark147" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">urban networks. Such networks require regional coordination but fall short of city-wide complexity. In [</a>106<a href="#bookmark179" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], RMART was deployed across 18 intersections using neighborhood data sharing. Chu et al. [</a>138<span style=" color: #000;">] demonstrated MA2C’s performance on a 5 × 5 grid and a real-world 30-intersection network in Monaco. Last but not least, the </span><span class="h3">large-scale </span><a href="#bookmark156" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">studies in- dicate a growing shift toward city-scale adaptive control. Graph-based RL and hierarchical architectures are key in managing such complexity. For example, in [</a>115<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], researchers eval- uated MPLight on Manhattan’s 2510 intersections, while CoLight [</a>148<a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] achieved a 19.89% travel time reduction across 196 intersections.In [</a>158<span style=" color: #000;">], researchers introduced a multi-layer graph mask Q-learning system across 45+ intersections, demonstrating scalability and zero-shot generalization.</span></p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark56" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">As Figure </a>13 <a href="#bookmark144" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">(right) illustrates, there is a clear evolution from early single-intersection studies [</a>103<a href="#bookmark148" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>107<a href="#bookmark193" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>152<a href="#bookmark146" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] to scalable, network-level systems. MARL strategies [</a>105<a href="#bookmark150" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>109<a href="#bookmark179" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>138<a href="#bookmark192" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] have facilitated decentralized control, while hybrid RL frameworks [</a>151<a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>158<a href="#bookmark182" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] integrate graph models and hierarchy to enable real-time, city-scale management. The inclusion of IoT data and predictive modules [</a>141<a href="#bookmark184" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>143<a href="#bookmark195" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>154<a href="#bookmark196" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>155<span style=" color: #000;">] suggests a trend toward adaptive, transferable RL systems for deployment in real-world urban environments.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: justify;"><p class="s2" style="display: inline;"><a name="bookmark45">Practical Applicability</a></p><p style="padding-top: 4pt;padding-left: 81pt;text-indent: 21pt;line-height: 112%;text-align: justify;">In the context of RL for traffic signal control, evaluating the practical applicability of each method requires moving beyond algorithmic novelty and considering operational attributes that determine real-world viability. Three pivotal dimensions for such assess- ment concern the <b>computational efficiency, scalability, and generalization ability </b>of each relevant algorithmic application. Computational efficiency reflects the model’s feasibil- ity for deployment on traffic control hardware and is influenced by neural network size, training convergence speed, and real-time inference potential. Scalability pertains to how well a method adapts to larger, more complex urban networks, often shaped by the agent architecture—e.g., decentralized MARL—and communication overhead. Generalization captures the robustness of the trained policy when exposed to diverse traffic conditions, layouts, or environments from the training scenario—an essential trait for adaptive urban mobility systems.</p><p style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">To enable a meaningful cross-study evaluation, each of the three dimensions— computational efficiency, scalability, and generalization ability—are qualitatively assessed as high, medium, or low based on evidence found in each study. The assignment is grounded in clearly defined criteria derived from reported methodologies, network sizes, architectural choices, and validation protocols. More specifically:</p><ul id="l24"><li style="padding-top: 4pt;padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Computational Efficiency: <span class="p">This reflects both training and inference complexity. A high score concern studies that utilized lightweight models (e.g., shallow networks or simplified tabular Q-learning) and demonstrated fast convergence, often within a few episodes or minutes of simulated time. Such models are typically suitable for real- time deployment or hardware-constrained environments. For example, simple DQN architectures applied to four-phase intersections with prioritized experience replay often fall into this category. A medium score indicates moderate model complexity— such as e.g., deep actor-critic methods—that require longer training times but are still computationally feasible using standard GPUs or cloud-based environments. These may include multi-agent PPO or A3C models applied to mid-sized grids. A low rating is reserved for models involving large neural networks, extensive hyperparameter tuning, or unstable learning dynamics (e.g., convergence issues, reward oscillations). Such models may include, for instance, DDPG-based methods with continuous actions across dozens of agents, which are computationally intensive and often unsuitable for real-time use without high-performance computing infrastructure.</span></h3></li><li style="padding-top: 2pt;padding-left: 103pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Scalability: <span class="p">The scalability attribute assesses how well the RL approach extends from small-scale to large-scale or city-wide deployments. A high rating concern RL ap- plications validated on large urban networks—e.g., large-scale networks with 30+ intersections or city-level topologies—often by using decentralized or hierarchical multi-agent structures that minimize communication overhead. Examples include MARL–DQN variants tested on synthetic or real urban networks with asynchronous updates. A medium score is assigned to studies that demonstrated applicability to moderate-scale scenarios, such as 3 × 3 or 5 × 5 intersection grids—typically moderate- scale complexity considering 7–30 intersections—where scalability was addressed but not rigorously tested or discussed. Centralized training with decentralized ex- ecution (CTDE) frameworks often fall here. A low score indicates evaluation on isolated intersections or minimal-node networks, without explicit consideration of inter-agent coordination or traffic spill-back effects: single and minimal-scale networks of 1–6 intersections. Such applications tend to lack architectural features or experi- mental adequacy, necessary for generalizing to larger, more complex road networks.</span></h3></li><li style="padding-left: 103pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Generalization Ability: <span class="p">Generalization measures robustness to unseen traffic scenar- ios, layouts, or non-stationary environments. A high score applies to RL cases that explicitly evaluate transfer learning, domain randomization, or training under variable traffic demands—indicating adaptability beyond a single training case. Examples include studies using multi-scenario training or that successfully transfer policies to different intersection geometries or demand profiles. The medium rating is for cases where models are trained with minor variations in traffic input or tested on different seed initializations, but without formal generalization analysis. RL methods that utilized fixed synthetic routes or rely on a single SUMO simulation file usually fall into this category. The low rating is for RL applications that have been solely trained and evaluated in static, deterministic settings, with no consideration for transferability, noise resilience, or policy degradation under new conditions.</span></h3><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark57" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">By systematically applying these assessment criteria across the surveyed studies, this subsection aims to highlight not only the algorithmic sophistication but also the operational readiness and deployment potential of each RL approach. Figure </a>14 <span style=" color: #000;">shows the number of applications with regards to computational efficiency, scalability, and generalization in high, medium, and low grades as depicted by the examination of the high-impact integrated papers.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="522" height="157" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_088.jpg"/></span></p><h4 style="padding-top: 6pt;padding-left: 82pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark57">Figure 14. </a><span class="s14">Number of applications with regard to computational efficiency (</span>left<span class="s14">), scalability (</span>center<span class="s14">), and generalization (</span>right<span class="s14">) categorized in high, medium, and low grades.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark147" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">According to the evaluation, among the integrated studies, only a few achieve high computational efficiency, notably those using tabular or lightweight value-based RL al- gorithms. For instance, in [</a>106<a href="#bookmark155" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>114<a href="#bookmark157" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>116<a href="#bookmark159" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], researchers leveraged tabular QL or average- reward methods that forgo deep architectures, enabling rapid convergence and real-time deployment.  Similarly, in [</a>118<a href="#bookmark167" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>126<span style=" color: #000;">], researchers achieved high efficiency through sim-</span></p><p class="s15" style="padding-top: 2pt;padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark144" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">plified double DQN or rule-based models optimized for low-latency environments. In contrast, most medium-rated methods such as in [</a>103<a href="#bookmark145" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>104<a href="#bookmark153" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>112<a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>149<a href="#bookmark175" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], employed moderate DQNs or actor-critic networks that balance learning capacity with hardware feasibility, often using experience replay, shared parameters, or target networks to stabilize learning. Low-computational-efficiency cases such as in [</a>134<a href="#bookmark180" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>139<span style=" color: #000;">] typically involved deep policy gradient models with LSTM, attention, or graph structures, incurring high training cost, slow convergence, and GPU dependencies that challenge deployment in real-time traffic signal environments.</span></p><p class="s15" style="padding-left: 81pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark156" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Scalability emerged as the most successfully achieved attribute, with numerous ap- plications rated as high. Such approaches included hierarchical or decentralized MARL frameworks, often using parameter sharing or communication-efficient strategies. No- tably, in [</a>115<a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>121<a href="#bookmark161" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], researchers scaled up to 2510 and 3971 intersections, respectively, by adopting decentralized policies and shared GNN-based representations. Similarly, in refs. [</a>120<a href="#bookmark183" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>142<a href="#bookmark194" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>153<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], authors demonstrated city-scale control through decentralized agents with limited or implicit coordination, often enhanced with graph attention or localized observations. Architectures like CoLight [</a>148<a href="#bookmark177" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] and CoTV [</a>136<a href="#bookmark165" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] utilized graph-based commu- nication and actor-critic setups to maintain linear scalability without centralized bottlenecks. In contrast, medium-scalability approaches, such as those in [</a>124<a href="#bookmark166" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>125<a href="#bookmark144" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], were validated only on modest-sized grids and lack robust coordination protocols. Low scalability is prevalent in methods focused on single intersections (e.g., [</a>103<a href="#bookmark198" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>157<span style=" color: #000;">]), which lack multi-agent logic or architecture modularity for broader network control.</span></p><p class="s15" style="padding-left: 81pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark153" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Only a limited subset of studies demonstrated high generalization, typically by in- corporating transfer learning, meta-learning, or evaluation across unseen environments. MetaLight [</a>112<a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], IG-RL [</a>121<a href="#bookmark186" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], and CVLight [</a>145<a href="#bookmark178" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] exemplify strong generalization by lever- aging meta-initialization, inductive GNNs, or policy pre-training across varied traffic scenarios and topologies, achieving robust zero-shot transfer. For instance, both [</a>137<a href="#bookmark187" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>146<a href="#bookmark148" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] approaches similarly integrated dynamic data or function approximation (e.g., RBF net- works) to support adaptation across environmental variations. Most medium-rated gen- eralization studies, such as in [</a>107<a href="#bookmark151" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>110<a href="#bookmark154" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>113<a href="#bookmark163" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], were evaluated on synthetic scenarios with some variability in traffic flow or seed, but they lack formal robustness analysis or domain transfer. A significant number of studies, however, fell under low generalization, often training on fixed layouts with static flows and no evaluation on unseen networks (e.g., in [</a>122<a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>149<a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>158<span style=" color: #000;">]). Such RL applications were highly scenario-dependent, limiting their adaptability to real-world deployment or unexpected traffic conditions.</span></p><p class="s15" style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">It is noticeable that only a very small subset of works manage to simultaneously achieve high computational efficiency, scalability, and generalization ability, standing out as exemplary frameworks in the landscape. Notably, the IG–RL model proposed by Devailly et al. [</a>121<a href="#bookmark186" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] integrated a deep graph convolutional network (GCN) with shared parameters and object-level abstraction, allowing for lightweight inference, near-zero-shot transferabil- ity, and massive-scale deployment (up to 3971 intersections in Manhattan). Similarly, CV- Light by Mo et al. [</a>145<a href="#bookmark153" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] achieved such balance using an asymmetric advantage actor-critic model with pre-training, decentralized control, and communication-free execution. Its de- sign supported real-time operation, generalized across demand levels and CV penetration rates, and scales effortlessly to 5 × 5 grids. The MetaLight framework [</a>112<span style=" color: #000;">] also demon- strated all three attributes through a meta-learning-based FRAP++ architecture, providing fast adaptation to new environments, structure-agnostic deployment, and efficient training via parameter sharing. These models typically combined: (i) architectural simplicity or modularity (e.g., parameter sharing, decentralized inference); (ii) learning acceleration techniques (e.g., meta-initialization, curriculum learning); and (iii) broad validation across synthetic and real-world conditions to achieve high grade in all three attributes.</span></p><p style="padding-top: 2pt;padding-left: 81pt;text-indent: 21pt;line-height: 112%;text-align: justify;">Inevitably, the path forward for RL-based traffic signal control research lies in jointly optimizing computational efficiency, scalability, and generalization ability—a triad es- sential for real-world deployment. Future systems should favor modular architectures with shared policies or decentralized agents that minimize inter-agent communication but support coordination (e.g., via attention or graph representations). Meta-learning, domain randomization, and transfer learning should be standard components in training pipelines to ensure robustness across diverse traffic scenarios and topologies. Importantly, computational parsimony must not be sacrificed for complexity: lightweight DQN variants or actor-critic methods with optimized network depth may achieve competitive results if well structured. Research should also embrace cross-domain benchmarks, testbeds with variable layouts, and real-world data (e.g., LuST, OpenStreetMap imports) to stress-test generalization. Promoting such attributes mutually may prove beneficial to ensure that RL- TSC systems transition from academic proofs-of-concept to scalable, reliable, and adaptive traffic control platforms in smart cities.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark46">Simulation Tools</a></p></li></ol></ol><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">Researchers applying RL to traffic signal control rely on flexible, high-fidelity sim- ulation environments to rigorously test and benchmark algorithms prior to real-world deployment. These tools enable the creation of detailed urban traffic networks, definition of diverse traffic patterns, and incorporation of realistic driver behavior and vehicle het- erogeneity. They also allow adjustments to road layout, signal timing, and traffic volume, supporting controlled experimentation and reproducibility. Importantly, most offer APIs or scripting tools for seamless RL integration. According to the evaluation, the primary simulation platforms used in RL-based traffic signal control research include:</p><ul id="l25"><li style="padding-top: 4pt;padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">SUMO: <a href="#bookmark200" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">A widely adopted open-source, microscopic, multi-modal traffic simulator offering scalability, adaptability, and robust modeling capabilities (e.g., lane changing, multi-lane configurations, emissions analysis) [</a><span class="s15">159</span><span class="p">]. SUMO’s modular structure and tools (e.g., netconvert, duarouter) streamline network preparation and traffic scenario simulation, making it ideal for RL-based signal control studies.</span></h3></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Vissim: <a href="#bookmark201" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">A commercial, high-resolution traffic simulation platform from  PTV Group [</a><span class="s15">160</span><span class="p">]. Known for accurate multimodal traffic modeling, advanced driver be- havior algorithms, and scenario management, Vissim is suited for evaluating transport infrastructure and integrating RL-based control with high realism.</span></h3></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><h3 style="display: inline;">Aimsun: <a href="#bookmark202" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">A commercial predictive traffic analytics platform supporting adaptive con- trol and intelligent transportation systems (ITS) [</a><span class="s15">161</span><span class="p">]. Aimsun enables performance assessment, network evolution forecasting, and real-time optimization, making it suitable for RL applications in large-scale digital mobility solutions.</span></h3></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">CityFlow: </span><a href="#bookmark203" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">An open-source, high-performance traffic simulator designed for scalability in RL-based traffic control [</a>162<a href="#bookmark203" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. CityFlow features optimized data structures and algorithms for city-wide simulations, outperforming SUMO in speed while supporting flexible configurations and seamless RL integration [</a>162<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 112%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Other: </span><a href="#bookmark144" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">A limited number of studies employed alternative simulators. For instance, in [</a>103<a href="#bookmark204" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], researchers used Paramics [</a>163<a href="#bookmark175" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">], known for detailed micro-simulations and behavior modeling. Meanwhile, in [</a>134<a href="#bookmark205" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] they used a Unity3D-based custom simulator [</a>164<span style=" color: #000;">], offering scenario flexibility for advanced sensor integration and agent modeling.</span></p></li></ul><p class="s15" style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;"><a href="#bookmark58" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Overall, simulator choice depends on factors such as budget, fidelity requirements, integration ease, and project scale. According to the review, SUMO remained the most frequently used platform between 2015 and 2025 (See Figure </a>15<span style=" color: #000;">). Its open-source license, ro- bust community support, scalable architecture, and Python 3.6 TraCI interface make it ideal</span></p><p class="s15" style="padding-top: 2pt;padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark147" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">for RL experimentation, especially in academic and collaborative contexts. SUMO’s script- ing capabilities allow rapid prototyping of diverse algorithms for adaptive traffic control. Commercial tools like Vissim and Aimsun continue to support high-fidelity modeling and seamless ITS integration. Their use in studies such as [</a>106<a href="#bookmark155" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>114<a href="#bookmark157" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>116<a href="#bookmark168" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>127<a href="#bookmark182" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>141<a href="#bookmark188" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>147<a href="#bookmark158" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] for Vissim and [</a>117<a href="#bookmark178" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>137<a href="#bookmark195" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>154<a href="#bookmark153" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">] for Aimsun demonstrate their utility in advanced traffic signal control sce- narios. CityFlow has gained traction as a lightweight yet powerful open-source simulator tailored for large-scale MARL applications. Studies such as [</a>112<a href="#bookmark156" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>115<a href="#bookmark176" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>135<a href="#bookmark185" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>144<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>148<a href="#bookmark192" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>151<a href="#bookmark197" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>156<span style=" color: #000;">] utilize CityFlow to simulate city-wide coordination tasks efficiently, reflecting its rising role in enabling real-time, scalable, RL-based urban traffic control.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 91pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="499" height="150" alt="image" src="2025-Traffic Signal Control via Reinforcement Learning A Review on Applications and Innovations/Image_089.jpg"/></span></p><h4 style="padding-top: 6pt;padding-left: 82pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark58">Figure 15.  </a><span class="s14">Simulation tools occurrence (</span>left<span class="s14">) and share (%) (</span>right<span class="s14">) for traffic signal control via reinforcement learning.</span></h4><ol id="l26"><li style="padding-top: 6pt;padding-left: 95pt;text-indent: -12pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark59">Discussion</a><a name="bookmark63">&zwnj;</a></h2><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">This section is structured into three parts, <b>Current Trends</b>, <b>Identified Challenges</b>, and <b>Future Directions</b>, offering a clear synthesis of the review’s findings and their broader implications. The Current Trends subsection is grounded on the evaluation section obser- vations and outlines dominant patterns and recurring strategies in RL-based traffic signal control (TSC), while the Challenges Identification subsection highlights the open challenges and unexplored opportunities for advancing both theoretical research and practical imple- mentation. The Future Directions subsection concerns future research needs that focus on specific areas to effectively address the challenges and identified limitations in order to enhance the real-world applicability of RL in TSC.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l27"><li style="padding-left: 100pt;text-indent: -18pt;line-height: 12pt;text-align: justify;"><p class="s2" style="display: inline;"><a name="bookmark60">Current Trends</a></p><ul id="l28"><li style="padding-left: 103pt;text-indent: -21pt;line-height: 111%;text-align: justify;"><p class="s15" style="display: inline;"><span class="h3">Dominance of Value-Based RL: </span><a href="#bookmark149" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Value-based methods, particularly DQN and its variants, remain the most widely used due to their alignment with the discrete na- ture of traffic signal phases and their stable convergence properties [</a>108<a href="#bookmark152" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>111<a href="#bookmark154" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>113<a href="#bookmark165" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>124<a href="#bookmark144" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Their origins in game-based RL tasks made them well-suited for modeling discrete decisions like phase switching, supporting simplified policy updates and lower train- ing overhead [</a>103<a href="#bookmark148" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>107<a href="#bookmark169" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>128<a href="#bookmark160" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Enhancements like double and dueling DQN address overestimation and feature learning, improving the capture of spatiotemporal traffic patterns [</a>119<a href="#bookmark169" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>128<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 13pt;text-align: left;"><h3 style="display: inline;">Growing Adoption of Hybrid and Domain-Specific Techniques: <span class="p">A marked shift</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark188" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">toward combining RL paradigms—value-based, policy-based, fuzzy logic, and MPC—has emerged to better handle real-world traffic complexities [</a>147<a href="#bookmark193" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>152<a href="#bookmark195" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>154<a href="#bookmark198" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>157<a href="#bookmark186" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. These hybrids leverage complementary strengths such as fuzzy logic’s interpretability or model-based components’ predictive accuracy. Embedding domain-specific priors (e.g., road topology, priority rules) helps align RL with engineering logic and enhances performance in dynamic settings [</a>145<a href="#bookmark196" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>155<span style=" color: #000;">].</span></p></li><li style="padding-left: 103pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Advanced Neural Modules as Key Enablers: <span class="p">Neural enhancements like GNNs, atten-</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark145" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">tion mechanisms, and temporal encoders (e.g., LSTMs) have been instrumental in cap- turing spatial and temporal dependencies in multi-intersection networks [</a>104<a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>121<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>148<span style=" color: #000;">].</span></p><p class="s15" style="padding-top: 2pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark176" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">GNNs facilitate inter-agent communication through relational state embeddings, while attention layers highlight critical lanes or intersections within complex urban grids [</a>135<a href="#bookmark177" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>136<a href="#bookmark186" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>145<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Prevalence of Multi-Agent Architectures: <span class="p">The decentralized nature of urban traffic</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark161" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">has driven widespread adoption of multi-agent RL [</a>120<a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>149<a href="#bookmark146" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Treating each inter- section as an agent allows localized decision making with shared coordination logic via mechanisms such as neighborhood masking and graph-based message passing, enabling scale-out from grids to entire cities [</a>105<a href="#bookmark160" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>119<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Reliance on Classic Traffic Metrics: <span class="p">Most studies anchor their reward functions to</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark144" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">traditional indicators like waiting time, queue length, or average delay [</a>103<a href="#bookmark174" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>133<a href="#bookmark148" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. These metrics offer a clear bridge to conventional traffic management practices, facilitating both validation and interpretability in comparative analysis [</a>107<a href="#bookmark176" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>135<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Emergence  of  Broader  Objectives  in  Reward  Design:   <span class="p">Recent  works  have  ex-</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark158" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">panded RL goals to include emissions, safety, and equity, pushing beyond congestion alone [</a>117<a href="#bookmark170" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>129<a href="#bookmark173" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>132<a href="#bookmark171" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. These additions demand nuanced reward formulations that bal- ance multiple, often competing, performance criteria [</a>130<a href="#bookmark177" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>136<span style=" color: #000;">].</span></p></li><li style="padding-left: 103pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Algorithm Choice Driven by Objective Complexity, Not Scale:  <span class="p">The selected RL</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark155" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">framework often correlates more with task complexity (e.g., safety, sustainability) than with network size [</a>114<a href="#bookmark186" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>145<a href="#bookmark177" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Sophisticated methods appear even in single-intersection studies, while simpler techniques persist in large-scale scenarios when congestion is the primary concern [</a>136<a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>158<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Blurring of Discrete and Continuous Control Paradigms: <span class="p">While traffic signals operate</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark178" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">on discrete cycles, researchers increasingly employ continuous action spaces through actor-critic architectures, enhancing the granularity of phase control and reducing abrupt transitions [</a>137<a href="#bookmark181" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>140<a href="#bookmark186" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>145<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Benchmarking Against Classical and RL Baselines: <span class="p">Iterative benchmarking—against</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark149" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">fixed-time, actuated, or prior RL models (e.g., CoLight, PressLight)—remains standard practice, promoting consistency and cumulative progress in the field [</a>108<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>148<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Scalability and Real-World Relevance: <span class="p">An increasing number of studies target multi-</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark156" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">intersection or city-scale deployments using advanced modules like GNNs and hi- erarchical decomposition, aligning academic research with real-world deployment challenges [</a>115<a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>121<a href="#bookmark197" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>156<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">SUMO as the Dominant Simulation Platform: <span class="p">SUMO’s open-source nature, mod-</span></h3></li></ul><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark156" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ularity, and Python API (TraCI) have made it the preferred tool for RL-TSC experi- mentation [</a>115<a href="#bookmark200" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>159<a href="#bookmark147" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Its adaptability supports complex network modeling, while its ecosystem promotes replicability across institutions [</a>106<a href="#bookmark160" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>119<span style=" color: #000;">].</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark61">Challenge Identification</a></p><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">Challenges in current RL implementations for traffic signal control remain significant despite considerable advancements. Some challenges stem directly from algorithmic complexity and control design, while others relate to practical issues, prohibiting the real-world deployment of RL in traffic signal control. More Specifically:</p><ul id="l29"><li style="padding-left: 103pt;text-indent: -21pt;line-height: 15pt;text-align: left;"><h3 style="display: inline;">Algorithmic Efficiency: <span class="p">As denoted in the evaluation section, value-based RL algo-</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark146" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">rithms such as QL and DQN frequently encounter limitations considering sample inefficiency and instability during training, particularly in environments with exten- sive state-action spaces or complex intersection networks [</a>105<a href="#bookmark148" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>107<a href="#bookmark149" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>108<a href="#bookmark152" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>111<a href="#bookmark179" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. This type of limitation is exacerbated as the scale of deployment expands, necessitating richer state representations and advanced architectures—such as DDQN, dueling DQN, or hierarchical frameworks—which, although addressing some issues, bring addi- tional complexities [</a>138<a href="#bookmark180" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>139<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>148<a href="#bookmark199" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>158<span style=" color: #000;">]. Furthermore, policy-based approaches, while</span></p><p class="s15" style="padding-top: 2pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark174" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">beneficial for continuous control tasks, remain computationally demanding due to their high sample complexity and training variance, restricting broader adoption in discrete, complex traffic environments [</a>133<a href="#bookmark176" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–</a>135<a href="#bookmark178" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Additionally, actor-critic methods, such as A2C and DDPG, are limited by issues related to training stability, especially under partially observable or highly dynamic conditions, reducing their practical applicability in complex traffic signal control applications [</a>137<a href="#bookmark181" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>140<a href="#bookmark182" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>141<span style=" color: #000;">].</span></p></li><li style="padding-left: 103pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Reward Function Design: <span class="p">Reward function design is another critical challenge identified</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark144" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">in the literature, with most current studies focusing on simplified queue- or waiting- time-based metrics, which frequently fail to capture essential dimensions like emis- sions, safety, and multimodal traffic dynamics comprehensively [</a>103<a href="#bookmark162" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>121<a href="#bookmark184" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>143<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>148<a href="#bookmark190" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>149<a href="#bookmark159" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Although recent efforts toward multi-objective and hybrid reward functions at- tempt to address such limitations, balancing competing objectives remains a sig- nificant challenge, requiring sophisticated algorithms capable of navigating complex trade-offs [</a>118<a href="#bookmark168" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>127<a href="#bookmark173" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>132<a href="#bookmark177" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>136<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Limited Safety-Critical Aspects: <span class="p">Additionally, few studies have comprehensively</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark158" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">addressed safety-critical aspects, such as crash or conflict avoidance, within real-world contexts. Current RL methodologies tend to prioritize efficiency metrics like travel time reduction and throughput optimization, often neglecting thorough validation of safety considerations. This oversight indicates a notable gap, as the integration of safety constraints is essential for gaining public trust and ensuring regulatory compliance, especially in dense urban environments where the potential for accidents and conflicts remains high [</a>117<a href="#bookmark170" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>129<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Computational Complexity and Real-Time Responsiveness: <span class="p">Advanced RL models</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark180" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">such as deep neural networks, graph-based methods, GAT-based hybrids, DDPG architectures, and multi-agent frameworks introduce significant computational com- plexity, which often conflicts with stringent real-time decision-making requirements. Ensuring that RL-based traffic signal decisions can be computed and implemented promptly under realistic temporal constraints remains a critical bottleneck. Such computational demands potentially restrict the real-world applicability and scalability of these sophisticated algorithms, emphasizing the need for efficient, lightweight RL solutions [</a>139<a href="#bookmark181" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>140<a href="#bookmark183" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>142<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>148<a href="#bookmark194" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>153<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Continuous Retraining Demand:  <span class="p">Last but not least, ongoing maintenance and</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark155" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">the requirement for continuous retraining of RL systems to adapt to evolving traf- fic patterns and urban conditions represent a considerable long-term commitment in terms of resources and technical expertise, adding complexity to sustainable deployment [</a>114<a href="#bookmark195" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>154<a href="#bookmark196" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>155<span style=" color: #000;">].</span></p></li><li style="padding-left: 103pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Transferability and Scalability to Other TSC Frameworks: <span class="p">RL models frequently</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark153" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">exhibit limited transferability, demonstrating a notable degradation in performance when applied to different urban scenarios or varied traffic conditions [</a>112<a href="#bookmark183" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>142<a href="#bookmark197" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>156<a href="#bookmark156" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. This highlights challenges regarding robustness and the generalization capabilities of existing RL frameworks, especially in unseen environments. Especially in the case of multi-agent control applications, the coordination among agents under real-world communication delays and partial observability makes scaling up even more difficult. Concurrently, scalability remains a significant obstacle, as expanding RL systems from single intersections to large urban networks introduces complexities such as non- stationarity, increased inter-agent communication overhead, and difficulties maintain- ing convergence stability. Thus, enhancing both transferability and scalability is critical for practical RL deployment in diverse and dynamic scenarios [</a>115<a href="#bookmark160" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>119<a href="#bookmark161" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>120<a href="#bookmark189" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>148<span style=" color: #000;">].</span></p></li><li style="padding-left: 103pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Transition to Real-World Deployment: <span class="p">Previous limitations strongly prohibit real-</span></h3><p style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: justify;">life implementations, which remain significantly low in number. Traffic simulations,</p><p class="s15" style="padding-top: 2pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark200" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">although sophisticated, often oversimplify real-world complexities, including unpre- dictable driver behaviors, diverse vehicle types, and unforeseen incidents, making simulation-to-reality transfer problematic [</a>159<a href="#bookmark203" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>162<a href="#bookmark200" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">]. Additionally, heterogeneous traffic patterns across cities, the need for fast and reliable communication, and the difficulty of transferring models trained in idealized environments to dynamic, noisy real-world conditions further inhibit deployment [</a>159<a href="#bookmark202" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>161<a href="#bookmark203" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>162<span style=" color: #000;">]. As a result, simula- tive implementations prove inadequate and potentially unsafe for managing real-life TSC scenarios.</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Standardization and Stakeholders Trust:  <span class="p">Other practical deployment challenges</span></h3><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark157" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">include issues related to standardization, as existing urban infrastructures often vary significantly, complicating the integration and uniform application of RL-based sys- tems. This attribute is grounded also in the regulatory and infrastructure heterogeneity between networks, which worsens standardization challenges. Moreover, real-world implementations face further acceptance barriers due to trust and transparency con- cerns among city planners, policymakers, and the public, necessitating explainable and interpretable RL solutions, which are currently scarce [</a>116<a href="#bookmark198" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>157<span style=" color: #000;">].</span></p></li><li style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;"><h3 style="display: inline;">Interpretability Issues: <span class="p">Last but no least, the lack of interpretability in RL-based deci-</span></h3></li></ul><p class="s15" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="#bookmark157" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">sions is hindering stakeholders and policymakers in terms of trusting and adopting such technologies as depicted in [</a>116<a href="#bookmark158" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>117<a href="#bookmark198" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">,</a>157<span style=" color: #000;">]. Developing transparent and explain- able models is a critical yet unresolved issue in RL deployment within urban traffic management, as indicated in the literature.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 100pt;text-indent: -18pt;text-align: left;"><p class="s2" style="display: inline;"><a name="bookmark62">Future Directions</a></p><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">In accordance with the identified challenges in RL-based traffic signal control, future research needs to focus on specific areas to effectively address current limitations and enhance real-world applicability. More specifically, future research should focus on:</p><p class="s22" style="padding-left: 104pt;text-indent: -21pt;line-height: 15pt;text-align: left;">➠    <span class="h3">Algorithmic Efficiency Enhancement: </span><span class="p">Future research should focus on developing</span></p><p style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;">more sample-efficient, stable RL algorithms by leveraging distributed RL architectures, hybrid implementations, prioritized experience replay, and lightweight actor-critic architectures. Improving algorithmic efficiency will directly lead to faster convergence under large-scale and dynamic urban traffic conditions, minimizing training costs and making RL-based traffic control more viable for real-world, resource-constrained deployments.</p><p class="s22" style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;">➠    <span class="h3">Expanding Multi-Objective Reward Functions: </span><span class="p">Work should additionally prioritize</span></p><p style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;">multi-objective frameworks that balance diverse goals such as mobility, safety, sus- tainability, and fairness. Hierarchical or compositional reward structures could enable RL agents to make trade-offs intelligently, aligning operational behavior with broader urban policy objectives. This direction will lead to traffic control strategies that are not only efficient but also socially equitable and environmentally responsible.</p><p class="s22" style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;">➠    <span class="h3">Embedding Safety and Ethics into RL Policies: </span><span class="p">Future RL frameworks should in-</span></p><p style="padding-top: 1pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: justify;">tegrate explicit safety constraints and ethical considerations into the policy training phase. Approaches like constrained reinforcement learning and risk-aware opti- mization will ensure the proactive avoidance of accidents and the prioritization of vulnerable users. Advancing safety-centric RL will be critical for gaining public trust and securing regulatory approvals for city-wide deployments.</p><p class="s22" style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;">➠  <span class="h3">Reducing Computational Complexity and Improving Real-Time Responsiveness: </span><span class="p">To</span></p><p style="padding-top: 1pt;padding-left: 103pt;text-indent: 0pt;line-height: 112%;text-align: justify;">address the real-time control requirements, research must focus on model compression techniques, such as pruning, quantization, and knowledge distillation, combined with efficient online decision-making architectures. Achieving lower computational</p><p style="padding-top: 2pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;">complexity will facilitate the deployment of RL controllers on edge devices or existing urban infrastructure, opening the door for broader, cost-effective adoption.</p><p class="s22" style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;">➠   <span class="h3">Continuous Retraining through Lifelong Learning: </span><span class="p">Future RL traffic systems should</span></p><p style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;">incorporate lifelong learning and online adaptation mechanisms that allow incremen- tal updates without catastrophic forgetting. By enabling agents to continually learn from new traffic patterns and events, the need for expensive and disruptive offline retraining cycles will be minimized, ultimately supporting the long-term sustainability and robustness of urban traffic management systems.</p><p class="s22" style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;">➠   <span class="h3">Enhancing Transferability and Scalability: </span><span class="p">Research efforts should emphasize meta-</span></p><p style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;">learning, domain adaptation, and modular policy design, aiming to create RL models that generalize effectively across different traffic conditions, intersection topologies, and city layouts. Enhancing transferability and scalability will allow RL systems to move beyond isolated deployments and towards managing complex, heterogeneous urban areas at the city scale.</p><p class="s22" style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;">➠    <span class="h3">Closing the Simulation-to-Reality Gap: </span><span class="p">Emphasis must shift toward robust sim-to-</span></p><p style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;">real transfer techniques such as domain randomization, real-time adaptation layers, and robust policy optimization under uncertainty. Strengthening sim-to-real capa- bilities will ensure that RL agents trained in synthetic environments are adequate to operate safely and reliably under real-world conditions, significantly accelerating the deployment timeline of RL-controlled traffic systems.</p><p class="s22" style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;">➠     <span class="h3">Establishing Standardization Protocols and Stakeholder Trust:  </span><span class="p">Future research</span></p><p style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;">should propose standardized evaluation metrics, deployment protocols, and valida- tion benchmarks for RL-based traffic systems. Simultaneously, fostering participatory design involving city stakeholders and transparent performance reporting will be critical for ensuring acceptance, facilitating cross-city model transfer, and enabling systematic scaling of RL-based urban mobility solutions.</p><p class="s22" style="padding-left: 104pt;text-indent: -21pt;line-height: 12pt;text-align: left;">➠     <span class="h3">Boosting Explainability and Interpretability: </span><span class="p">Developing inherently interpretable</span></p><p style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: justify;">RL policies or applying explainable AI techniques like attention visualization, causal analysis, or symbolic policy extraction will be beneficial. This will empower traffic engineers, policymakers, and the public to understand, trust, and regulate RL systems, creating transparent governance frameworks and making RL-driven traffic control socially acceptable.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 95pt;text-indent: -12pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark64">Conclusions</a><a name="bookmark65">&zwnj;</a></h2></li></ol><p style="padding-top: 4pt;padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: right;">Over the past decade,  reinforcement learning has emerged as a transformative paradigm in traffic signal control, steering the field away from static and heuristic-based strategies toward adaptive, data-driven decision making. This review has consolidated a broad spectrum of RL-based TSC research, examining algorithmic developments, scalability trends, and performance metrics across diverse intersection types, reward structures, and simulation platforms. By distilling findings from extensive prior work, the study offers both a comprehensive overview and technical depth, framing the evolution of RL as a key enabler for tackling real-world challenges such as congestion, emissions, and urban safety. Notable trends highlight this progression—chief among them is the dominance of value-based methods, particularly DQN variants, and the widespread adoption of multi- agent architectures that enable decentralized, scalable control. Reward design has evolved from traditional congestion-focused metrics to incorporate safety, sustainability, and fair- ness, reflecting broader smart city goals.  Benchmarking practices have matured, with comparisons now including established RL frameworks like CoLight and PressLight along- side conventional baselines. Furthermore, simulation tools such as SUMO and CityFlow</p><p style="padding-top: 2pt;padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: left;">have become foundational to experimentation, offering the scalability and integration necessary for modeling realistic urban networks.</p><p style="padding-left: 82pt;text-indent: 21pt;line-height: 112%;text-align: justify;">Looking ahead, future research should prioritize the development of hybrid RL mod- els that integrate domain-specific knowledge with powerful neural architectures—such as graph neural networks (GNNs) and attention mechanisms—to strengthen spatiotemporal reasoning and adaptability in traffic environments. Multi-objective optimization will be key to aligning RL-driven decisions with broader urban policy goals, encompassing emissions reduction, safety improvement, and equitable mobility access. To achieve scalable and resilient traffic control, meta-learning and transfer learning approaches must be lever- aged to enable RL systems to continually adapt across evolving and heterogeneous traffic contexts. Critically, advancing explainable reinforcement learning frameworks will be indispensable for ensuring that traffic control decisions are interpretable, transparent, and auditable by engineers, policymakers, and the public, thereby fostering trust and regulatory acceptance. Equally important, robust sim-to-real transfer methodologies must be devel- oped to bridge the persistent gap between simulation-based training and unpredictable real-world conditions, ensuring that learned policies retain their reliability and safety post- deployment. Ultimately, future RL-based traffic control systems must not only surpass traditional performance metrics but also embody the transparency, adaptability, and robust- ness needed to meet the increasingly complex societal, ethical, and environmental demands of next-generation urban mobility ecosystems.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 82pt;text-indent: 0pt;line-height: 116%;text-align: justify;">Author Contributions: <span class="s14">Conceptualization, P.M.; methodology, P.M.; software, P.M.; validation, all authors; formal analysis, P.M. and C.R.L.; investigation, P.M. and I.M.; resources, all authors; writing—original draft preparation, P.M.; writing—review and editing, P.M. and I.M.; visualization,</span></h4><p class="s14" style="padding-left: 82pt;text-indent: 0pt;line-height: 116%;text-align: left;">P.M. and C.R.L.; supervision, P.M. and E.K. All authors have read and agreed to the published version of the manuscript.</p><h4 style="padding-top: 5pt;padding-left: 82pt;text-indent: 0pt;text-align: left;">Funding: <span class="s14">This research received no external funding.</span></h4><h4 style="padding-top: 7pt;padding-left: 82pt;text-indent: 0pt;text-align: left;">Conflicts of Interest: <span class="s14">The authors declare no conflicts of interest.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 81pt;text-indent: 0pt;text-align: left;">Abbreviations</h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: left;">A2C         Advantage actor-critic AC           Actuated control</p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: left;">ANN       Artificial neural network ATSC      Adaptive traffic signal control</p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: left;">CAV        Connected and autonomous vehicles CL           CoLight</p><p style="padding-left: 82pt;text-indent: 0pt;text-align: left;">CV           Connected vehicles</p><p style="padding-top: 1pt;padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: left;">D3QN     Dueling double deep Q-network DDPG     Deep deterministic policy gradient DDQN    Double deep Q-network</p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: left;">DHW      Domestic got water DQN       Deep Q-network</p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: left;">DRL        Deep reinforcement learning ESS          Energy storage system</p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 112%;text-align: left;">FLC         Fuzzy logic control FT            Fixed-time</p><p style="padding-left: 82pt;text-indent: 0pt;text-align: left;">GA          Genetic algorithm</p><p style="padding-top: 1pt;padding-left: 82pt;text-indent: 0pt;text-align: left;">GAT        Graph attention network</p><p style="padding-top: 2pt;padding-left: 136pt;text-indent: 0pt;line-height: 112%;text-align: left;">GCNN    Graph convolutional neural network ITS          Intelligent transportation systems LSTM      Long short-term memory</p><p style="padding-left: 136pt;text-indent: 0pt;text-align: left;">MA2C     Multi-agent actor-critic</p><p style="padding-top: 1pt;padding-left: 136pt;text-indent: 0pt;line-height: 112%;text-align: left;">MARL    Multi-agent reinforcement learning MCTS     Monte Carlo tree search</p><p style="padding-left: 136pt;text-indent: 0pt;line-height: 112%;text-align: left;">MDP       Markov decision process ML          MetaLight</p><p style="padding-left: 136pt;text-indent: 0pt;text-align: left;">MP          Max-pressure</p><p style="padding-top: 1pt;padding-left: 136pt;text-indent: 0pt;line-height: 112%;text-align: left;">MPC       Model predictive control PG           Policy gradient</p><p style="padding-left: 136pt;text-indent: 0pt;text-align: left;">PL            PressLight</p><p style="padding-top: 1pt;padding-left: 136pt;text-indent: 0pt;text-align: left;">QL           Q-learning</p><p style="padding-top: 1pt;padding-left: 136pt;text-indent: 0pt;text-align: left;">RBC        Rule-based control</p><p style="padding-top: 1pt;padding-left: 136pt;text-indent: 0pt;text-align: left;">RL           Reinforcement learning</p><p style="padding-top: 1pt;padding-left: 136pt;text-indent: 0pt;text-align: left;">SAC        Soft actor-critic</p><p style="padding-top: 1pt;padding-left: 136pt;text-indent: 0pt;text-align: left;">SOTL      Self-organizing traffic lights</p><p style="padding-top: 1pt;padding-left: 136pt;text-indent: 0pt;line-height: 112%;text-align: left;">TD3         Twin delayed deep deterministic policy gradient TSC         Traffic signal control</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 5pt;text-indent: 0pt;text-align: left;">References</h2><ol id="l30"><li style="padding-top: 3pt;padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark66">Pergantis, E.N.; Sangamnerkar, A.S. Sensors, Storage, and Algorithms for Practical Optimal Controls in Residential Buildings.</a><a name="bookmark67">&zwnj;</a></p><p class="s57" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">ASHRAE Trans. <b>2023</b><span class="s14">, </span>129<span class="s14">, 437–444.</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Pergantis, E.N.; Dhillon, P.; Premer, L.D.R.; Lee, A.H.; Ziviani, D.; Kircher, K.J. Humidity-aware model predictive control for residential air conditioning: A field study. <i>Build. Environ. </i><b>2024</b>, <i>266</i><a href="http://doi.org/10.1016/j.buildenv.2024.112093" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 112093. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">D’Agostino, D.; D’Auria, M.; Minelli, F.; Minichiello, F. Multi-criteria Decision-Making for Thermal Insulation of an Existing Office Building Considering Environmental, Energy, and Economic Performance. In <i>Sustainability in Energy and Buildings 2023</i>; Springer: Singapore, 2024; pp. 167–177.</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark68">D’Agostino, D.; Minelli, F.; Minichiello, F. New genetic algorithm-based workflow for multi-objective optimization of Net Zero Energy Buildings integrating robustness assessment. </a><i>Energy Build. </i><b>2023</b>, <i>284</i><a href="http://dx.doi.org/10.1016/j.enbuild.2023.112841" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 112841. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark69">Minelli, F.; D’Agostino, D.; Migliozzi, M.; Minichiello, F.; D’Agostino, P. PhloVer: A modular and integrated tracking photovoltaic shading device for sustainable large urban spaces—Preliminary study and prototyping. </a><i>Energies </i><b>2023</b>, <i>16</i><a href="http://dx.doi.org/10.3390/en16155786" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 5786. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Takayanagi, M.; Yoshino, M.; Kikuchi, G.; Kanke, T.; Suzuki, N. Autonomous adaptive control of manufacturing parameters based on local regression modeling. <i>Behaviormetrika </i><b>2024</b>, <i>51</i><a href="http://dx.doi.org/10.1007/s41237-022-00176-w" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 499–513. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark70">Shirazi, B.; Mahdavi, I.; Solimanpur, M. Intelligent decision support system for the adaptive control of a flexible manufacturing system with machine and tool flexibility. </a><i>Int. J. Prod. Res. </i><b>2012</b>, <i>50</i><a href="http://dx.doi.org/10.1080/00207543.2011.574504" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 3288–3314. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark71">Rudomanenko, A.; Chernyadev, N.; Vorotnikov, S. Adaptive control system for industrial robotic manipulator. In Proceedings of the Modern Problems of Robotics: Second International Conference, MPoR 2020, Moscow, Russia, 25–26 March 2020; Revised Selected Papers 2; pp. 153–164.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Karatzinis, G.D.; Michailidis, P.; Michailidis, I.T.; Kapoutsis, A.C.; Kosmatopoulos, E.B.; Boutalis, Y.S. Coordinating heterogeneous mobile sensing platforms for effectively monitoring a dispersed gas plume. <i>Integr. Comput.-Aided Eng. </i><b>2022</b>, <i>29</i><a href="http://dx.doi.org/10.3233/ICA-220690" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 411–429. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark72">Salavasidis, G.; Kapoutsis, A.C.; Chatzichristofis, S.A.; Michailidis, P.; Kosmatopoulos, E.B. Autonomous trajectory design system for mapping of unknown sea-floors using a team of AUVs. In Proceedings of the 2018 European Control Conference (ECC), Limassol, Cyprus, 12–15 June 2018; pp. 1080–1087.</a></p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark73">Keroglou, C.; Kansizoglou, I.; Michailidis, P.; Oikonomou, K.M.; Papapetros, I.T.; Dragkola, P.; Michailidis, I.T.; Gasteratos, A.; Kosmatopoulos, E.B.; Sirakoulis, G.C. A survey on technical challenges of assistive robotics for elder people in domestic environments: The aspida concept. </a><i>IEEE Trans. Med. Robot. Bionics </i><b>2023</b>, <i>5</i><a href="http://dx.doi.org/10.1109/TMRB.2023.3261342" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 196–205. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Van der Pol, E.; Oliehoek, F.A. Coordinated deep reinforcement learners for traffic light control. In Proceedings of the Learning, Inference and Control of Multi-Agent Systems (at NIPS 2016), Barcelona, Spain, 10 December 2016; Volume 8, pp. 21–38.</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Zhang, K.; Cui, Z.; Ma, W. A survey on reinforcement learning-based control for signalized intersections with connected automated vehicles. <i>Transp. Rev. </i><b>2024</b>, <i>44</i><a href="http://dx.doi.org/10.1080/01441647.2024.2377637" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1187–1208. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-top: 2pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark74">Hashmi, H.T.; Ud-Din, S.; Khan, M.A.; Khan, J.A.; Arshad, M.; Hassan, M.U. Traffic Flow Optimization at Toll Plaza Using Proactive Deep Learning Strategies. </a><i>Infrastructures </i><b>2024</b>, <i>9</i><a href="http://dx.doi.org/10.3390/infrastructures9050087" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 87. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark75">Guerrieri, M.; Parla, G. Deep learning and yolov3 systems for automatic traffic data measurement by moving car observer technique. </a><i>Infrastructures </i><b>2021</b>, <i>6</i><a href="http://dx.doi.org/10.3390/infrastructures6090134" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 134. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark76">El-Tantawy, S.; Abdulhai, B.; Abdelgawad, H. Multiagent reinforcement learning for integrated network of adaptive traffic signal controllers (MARLIN-ATSC): Methodology and large-scale application on downtown Toronto. </a><i>IEEE Trans. Intell. Transp. Syst.  </i><b>2013</b>, <i>14</i><a href="http://dx.doi.org/10.1109/TITS.2013.2255286" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1140–1150. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Zhao, D.; Dai, Y.; Zhang, Z. Computational intelligence in urban traffic signal control: A survey. <i>IEEE Trans. Syst. Man Cybern. Part C (Appl. Rev.) </i><b>2011</b>, <i>42</i><a href="http://dx.doi.org/10.1109/TSMCC.2011.2161577" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 485–494. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -20pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark77">Efthymiou, I.P.; Egleton, T.E. Artificial intelligence for sustainable smart cities. In </a><i>Handbook of Research on Applications of AI, Digital Twin, and Internet of Things for Sustainable Development</i>; IGI Global: Hershey, PA, USA, 2023; pp. 1–11.</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark78">Choosakun, A.; Chaiittipornwong, Y.; Yeom, C. Development of the cooperative intelligent transport system in Thailand: A prospective approach. </a><i>Infrastructures </i><b>2021</b>, <i>6</i><a href="http://dx.doi.org/10.3390/infrastructures6030036" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 36. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -20pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark79">Qadri, S.S.S.M.; Gökçe, M.A.; Öner, E. State-of-art review of traffic signal control methods: Challenges and opportunities. </a><i>Eur. Transp. Res. Rev. </i><b>2020</b>, <i>12</i><a href="http://dx.doi.org/10.1186/s12544-020-00439-1" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1–23. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Pell, A.; Meingast, A.; Schauer, O. Trends in real-time traffic simulation. <i>Transp. Res. Procedia </i><b>2017</b>, <i>25</i><a href="http://dx.doi.org/10.1016/j.trpro.2017.05.175" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1477–1484. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark80">Michailidis, I.T.; Michailidis, P.; Rizos, A.; Korkas, C.; Kosmatopoulos, E.B. Automatically fine-tuned speed control system for fuel and travel-time efficiency: A microscopic simulation case study. In Proceedings of the 2017 25th Mediterranean Conference on Control and Automation (MED), Valletta, Malta, 3–6 July 2017; pp. 915–920.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark81">Zheng, X.; Wu, F.; Chen, W.; Naghizade, E.; Khoshelham, K. Show me a safer way: Detecting anomalous driving behavior using online traffic footage. </a><i>Infrastructures </i><b>2019</b>, <i>4</i><a href="http://dx.doi.org/10.3390/infrastructures4020022" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 22. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Michailidis, I.T.; Kapoutsis, A.C.; Korkas, C.D.; Michailidis, P.T.; Alexandridou, K.A.; Ravanis, C.; Kosmatopoulos, E.B. Embed- ding autonomy in large-scale IoT ecosystems using CAO and L4G-CAO. <i>Discov. Internet Things </i><b>2021</b>, <i>1</i><a href="http://dx.doi.org/10.1007/s43926-021-00003-w" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1–22. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Michailidis, I.T.; Manolis, D.; Michailidis, P.; Diakaki, C.; Kosmatopoulos, E.B. A decentralized optimization approach employing cooperative cycle-regulation in an intersection-centric manner: A complex urban simulative case study. <i>Transp. Res. Interdiscip. Perspect. </i><b>2020</b>, <i>8</i><a href="http://dx.doi.org/10.1016/j.trip.2020.100232" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 100232. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Wang, S.; Xie, X.; Huang, K.; Zeng, J.; Cai, Z. Deep reinforcement learning-based traffic signal control using high-resolution event-based data. <i>Entropy </i><b>2019</b>, <i>21</i><a href="http://dx.doi.org/10.3390/e21080744" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 744. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Kotsi, A.; Politis, I.; Chaniotakis, E.; Mitsakis, E. A Multi-Player Framework for Sustainable Traffic Optimization in the Era of Digital Transportation. <i>Infrastructures </i><b>2024</b>, <i>10</i><a href="http://dx.doi.org/10.3390/infrastructures10010006" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 6. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Ištoka Otkovic´, I.; Deluka-Tibljaš, A.; Zecˇevic´, Ð.; Šimunovic´, M. Reconstructing Intersection Conflict Zones: Microsimulation- Based Analysis of Traffic Safety for Pedestrians. <i>Infrastructures </i><b>2024</b>, <i>9</i><a href="http://dx.doi.org/10.3390/infrastructures9120215" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 215. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Šurdonja, S.; Maršanic´, R.; Deluka-Tibljaš, A. Speed Analyses of Intersections Reconstructed into Roundabouts: A Case Study from Rijeka, Croatia. <i>Infrastructures </i><b>2024</b>, <i>9</i><a href="http://dx.doi.org/10.3390/infrastructures9090159" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 159. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark82">Olayode, I.O.; Severino, A.; Tartibu, L.K.; Arena, F.; Cakici, Z. Performance evaluation of a hybrid PSO enhanced ANFIS model in prediction of traffic flow of vehicles on freeways: Traffic data evidence from South Africa. </a><i>Infrastructures </i><b>2021</b>, <i>7</i><a href="http://dx.doi.org/10.3390/infrastructures7010002" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 2. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark83">Amézquita-López, J.; Valdés-Atencio, J.; Angulo-García, D. Understanding traffic congestion via network analysis, agent modeling, and the trajectory of urban expansion: A coastal city case. </a><i>Infrastructures </i><b>2021</b>, <i>6</i><a href="http://dx.doi.org/10.3390/infrastructures6060085" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 85. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Pergantis, E.N.; Premer, L.D.R.; Lee, A.H.; Ziviani, D.; Groll, E.A.; Kircher, K.J. Latent and Sensible Model Predictive Controller Demonstration in a House During Cooling Operation. <i>ASHRAE Trans. </i><b>2024</b>, <i>130</i>, 177–185.</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark84">Wan, C.H.; Hwang, M.C. Adaptive traffic signal control methods based on deep reinforcement learning. In </a><i>Intelligent Transport Systems for Everyone’s Mobility</i>; Springer: Singapore, 2019; pp. 195–209.</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark85">Pergantis, E.N.; Al Theeb, N.; Dhillon, P.; Ore, J.P.; Ziviani, D.; Groll, E.A.; Kircher, K.J. Field demonstration of predictive heating control for an all-electric house in a cold climate. </a><i>Appl. Energy </i><b>2024</b>, <i>360</i><a href="http://dx.doi.org/10.1016/j.apenergy.2024.122820" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 122820. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Wei, H.; Zheng, G.; Yao, H.; Li, Z. Intellilight: A reinforcement learning approach for intelligent traffic light control. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, London, UK, 19–23 August 2018; pp. 2496–2505.</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Wang, H.; Zhu, J.; Gu, B. Model-based deep reinforcement learning with traffic inference for traffic signal control. <i>Appl. Sci. </i><b>2023</b>,</p><p class="s57" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark86">13</a><a href="http://dx.doi.org/10.3390/app13064010" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 4010. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark87">Ye, B.L.; Wu, W.; Ruan, K.; Li, L.; Chen, T.; Gao, H.; Chen, Y. A survey of model predictive control methods for traffic signal control. </a><i>IEEE/CAA J. Autom. Sin. </i><b>2019</b>, <i>6</i><a href="http://dx.doi.org/10.1109/JAS.2019.1911471" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 623–640. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Michailidis, I.T.; Manolis, D.; Michailidis, P.; Diakaki, C.; Kosmatopoulos, E.B. Autonomous self-regulating intersections in large-scale urban traffic networks: A Chania City case study. In Proceedings of the 2018 5th International Conference on Control, Decision and Information Technologies (CoDIT), Thessaloniki, Greece, 10–13 April 2018; pp. 853–858.</p></li><li style="padding-top: 2pt;padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark88">Spall, J.C.; Chin, D.C. A model-free approach to optimal signal light timing for system-wide traffic control. In Proceedings of the 1994 33rd IEEE Conference on Decision and Control, Lake Buena Vista, FL, USA, 14–16 December 1994; Volume 2, pp. 1868–1875.</a><a name="bookmark89">&zwnj;</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;">Lazaridis, C.R.; Michailidis, I.; Karatzinis, G.; Michailidis, P.; Kosmatopoulos, E. Evaluating reinforcement learning algorithms in residential energy saving and comfort management. <i>Energies </i><b>2024</b>, <i>17</i><a href="http://dx.doi.org/10.3390/en17030581" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 581. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Michailidis, P.; Michailidis, I.; Vamvakas, D.; Kosmatopoulos, E. Model-free HVAC control in buildings: A review. <i>Energies </i><b>2023</b>,</p><p class="s57" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">16<a href="http://dx.doi.org/10.3390/en16207124" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 7124. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark90">D’Agostino, D.; De Falco, F.; Minelli, F.; Minichiello, F.  New robust multi-criteria decision-making framework for thermal insulation of buildings under conflicting stakeholder interests. </a><i>Appl. Energy </i><b>2024</b>, <i>376</i><a href="http://dx.doi.org/10.1016/j.apenergy.2024.124262" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 124262. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark91">Vamvakas, D.; Michailidis, P.; Korkas, C.; Kosmatopoulos, E. Review and evaluation of reinforcement learning frameworks on smart grid applications. </a><i>Energies </i><b>2023</b>, <i>16</i><a href="http://dx.doi.org/10.3390/en16145326" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 5326. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark92">Michailidis, P.; Michailidis, I.; Kosmatopoulos, E.  Review and Evaluation of Multi-Agent Control Applications for Energy Management in Buildings. </a><i>Energies </i><b>2024</b>, <i>17</i><a href="http://dx.doi.org/10.3390/en17194835" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 4835. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;">Dogru, O.; Xie, J.; Prakash, O.; Chiplunkar, R.; Soesanto, J.; Chen, H.; Velswamy, K.; Ibrahim, F.; Huang, B. Reinforcement learning in process industries: Review and perspective. <i>IEEE/CAA J. Autom. Sin. </i><b>2024</b>, <i>11</i><a href="http://dx.doi.org/10.1109/JAS.2024.124227" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 283–300. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Nian, R.; Liu, J.; Huang, B.  A review on reinforcement learning: Introduction and applications in industrial process control.</p><p class="s57" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Comput. Chem. Eng. <b>2020</b><span class="s14">, </span>139<a href="http://dx.doi.org/10.1016/j.compchemeng.2020.106886" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 106886. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark93">Espinosa-Leal, L.; Chapman, A.; Westerlund, M. Autonomous industrial management via reinforcement learning. </a><i>J. Intell. Fuzzy Syst. </i><b>2020</b>, <i>39</i><a href="http://dx.doi.org/10.3233/JIFS-189161" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 8427–8439. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark94">Chen, X.; Hu, J.; Chen, Z.; Lin, B.; Xiong, N.; Min, G. A reinforcement learning-empowered feedback control system for industrial internet of things. </a><i>IEEE Trans. Ind. Inform. </i><b>2021</b>, <i>18</i><a href="http://dx.doi.org/10.1109/TII.2021.3076393" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 2724–2733. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;">Meyes, R.; Tercan, H.; Roggendorf, S.; Thiele, T.; Büscher, C.; Obdenbusch, M.; Brecher, C.; Jeschke, S.; Meisen, T. Motion planning for industrial robots using reinforcement learning. <i>Procedia CIRP </i><b>2017</b>, <i>63</i><a href="http://dx.doi.org/10.1016/j.procir.2017.03.095" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 107–112. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;">Orr, J.; Dutta, A.  Multi-agent deep reinforcement learning for multi-robot applications:  A survey.   <i>Sensors </i><b>2023</b>, <i>23</i><a href="http://dx.doi.org/10.3390/s23073625" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 3625. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Singh, B.; Kumar, R.; Singh, V.P. Reinforcement learning in robotic applications: A comprehensive survey. <i>Artif. Intell. Rev. </i><b>2022</b>,</p><p class="s57" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark95">55</a><a href="http://dx.doi.org/10.1007/s10462-021-09997-9" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 945–990. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark96">Wang, Y.; Damani, M.; Wang, P.; Cao, Y.; Sartoretti, G. Distributed reinforcement learning for robot teams: A review. </a><i>Curr. Robot. Rep. </i><b>2022</b>, <i>3</i><a href="http://dx.doi.org/10.1007/s43154-022-00091-8" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 239–257. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark97">Noaeen, M.; Naik, A.; Goodman, L.; Crebo, J.; Abrar, T.; Abad, Z.S.H.; Bazzan, A.L.; Far, B. Reinforcement learning in urban network traffic signal control: A systematic literature review. </a><i>Expert Syst. Appl. </i><b>2022</b>, <i>199</i><a href="http://dx.doi.org/10.1016/j.eswa.2022.116830" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 116830. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -20pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark98">Xiao, Y.; Liu, J.; Wu, J.; Ansari, N. Leveraging deep reinforcement learning for traffic engineering: A survey. </a><i>IEEE Commun. Surv. Tutorials </i><b>2021</b>, <i>23</i><a href="http://dx.doi.org/10.1109/COMST.2021.3102580" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 2064–2097. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark99">Borges, F.d.S.P.; Fonseca, A.P.; Garcia, R.C. Deep reinforcement learning model to mitigate congestion in real-time traffic light networks. </a><i>Infrastructures </i><b>2021</b>, <i>6</i><a href="http://dx.doi.org/10.3390/infrastructures6100138" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 138. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -20pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark100">Lai, X.; Yang, Z.; Xie, J.; Liu, Y. Reinforcement learning in transportation research: Frontiers and future directions. </a><i>Multimodal Transp. </i><b>2024</b><a href="http://dx.doi.org/10.1016/j.multra.2024.100164" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 100164 [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark101">Wang, B.; He, Z.; Sheng, J.; Chen, Y. Deep reinforcement learning for traffic light timing optimization. </a><i>Processes </i><b>2022</b>, <i>10</i><a href="http://dx.doi.org/10.3390/pr10112458" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 2458. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark102">Yau, K.L.A.; Qadir, J.; Khoo, H.L.; Ling, M.H.; Komisarczuk, P. A survey on reinforcement learning models and algorithms for traffic signal control. </a><i>ACM Comput. Surv. (CSUR) </i><b>2017</b>, <i>50</i><a href="http://dx.doi.org/10.1145/3068287" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1–38. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Miletic´, M.; Ivanjko, E.; Greguric´, M.; Kušic´, K. A review of reinforcement learning applications in adaptive traffic signal control.</p><p class="s57" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark103">IET Intell. Transp. Syst. </a><b>2022</b><span class="s14">, </span>16<a href="http://dx.doi.org/10.1049/itr2.12208" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1269–1285. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark104">Zhao, H.; Dong, C.; Cao, J.; Chen, Q. A survey on deep reinforcement learning approaches for traffic signal control. </a><i>Eng. Appl. Artif. Intell. </i><b>2024</b>, <i>133</i><a href="http://dx.doi.org/10.1016/j.engappai.2024.108100" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 108100. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;">Tang, K.; Boltze, M.; Nakamura, H.; Tian, Z. <i>Global Practices on Road Traffic Signal Control: Fixed-Time Control at Isolated Intersections</i>; Elsevier: Amsterdam, The Netherlands, 2019.</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark105">Muralidharan, A.; Pedarsani, R.; Varaiya, P.  Analysis of fixed-time control.   </a><i>Transp.  Res.  Part B Methodol.  </i><b>2015</b>, <i>73</i><a href="http://dx.doi.org/10.1016/j.trb.2014.12.002" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 81–90. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Liu, Y.; Li, H.; Lu, R.; Zuo, Z.; Li, X.  An overview of finite/fixed-time control and its application in engineering systems.</p><p class="s57" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark106">IEEE/CAA J. Autom. Sin. </a><b>2022</b><span class="s14">, </span>9<a href="http://dx.doi.org/10.1109/JAS.2022.105413" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 2106–2120. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Ferrara, A.; Sacone, S.; Siri, S.; Ferrara, A.; Sacone, S.; Siri, S. An overview of traffic control schemes for freeway systems.  In</p><p class="s57" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark107">Freeway Traffic Modelling and Control</a><span class="s14">; Springer: Berlin/Heidelberg, Germany, 2018; pp. 193–234.</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: left;"><p class="s14" style="display: inline;">Kalašová, A.; Hájnik, A.; Kubal’ák, S.; Benˇ uš, J.; Harantová, V. The impact of actuated control on the environment and the traffic flow. <i>J. Appl. Eng. Sci. </i><b>2022</b>, <i>20</i><a href="http://dx.doi.org/10.5937/jaes0-33043" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 305–314. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-top: 2pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark108">Dabiri, S.; Kompany, K.; Abbas, M. Introducing a cost-effective approach for improving the arterial traffic performance operating under the semi-actuated coordinated signal control. </a><i>Transp. Res. Rec. </i><b>2018</b>, <i>2672</i><a href="http://dx.doi.org/10.1177/0361198118772691" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 71–80. [</a><span style=" color: #0774B7;">CrossRef</span>]<a name="bookmark109">&zwnj;</a></p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark110">Zheng, X.; Recker, W.; Chu, L. Optimization of control parameters for adaptive traffic-actuated signal control. </a><i>J. Intell. Transp. Syst. </i><b>2010</b>, <i>14</i><a href="http://dx.doi.org/10.1080/15472451003719756" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 95–108. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark111">Lin, F.B. Knowledge base on semi-actuated traffic-signal control. </a><i>J. Transp. Eng. </i><b>1991</b>, <i>117</i>, 398–417. [<span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark112">Xu, H.; Zhang, K.; Zhang, D.; Zheng, Q. Traffic-responsive control technique for fully-actuated coordinated signals. </a><i>IEEE Trans. Intell. Transp. Syst. </i><b>2021</b>, <i>23</i><a href="http://dx.doi.org/10.1109/TITS.2021.3054054" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 5460–5469. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark113">Mannion, P.; Duggan, J.; Howley, E. An experimental review of reinforcement learning algorithms for adaptive traffic signal control. In </a><i>Autonomic Road Transport Support Systems</i>; Springer: Berlin/Heidelberg, Germany, 2016; pp. 47–66.</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark114">Van den Berg, M.; Hegyi, A.; De Schutter, B.; Hellendoorn, H. Integrated traffic control for mixed urban and freeway networks: A model predictive control approach. </a><i>Eur. J. Transp. Infrastruct. Res. </i><b>2007</b>, <i>7</i>.</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark115">Mitsakis, E.; Salanova, J.M.; Giannopoulos, G. Combined dynamic traffic assignment and urban traffic control models. </a><i>Procedia- Soc. Behav. Sci. </i><b>2011</b>, <i>20</i><a href="http://dx.doi.org/10.1016/j.sbspro.2011.08.049" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 427–436. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark116">Abdelghany, K.F.; Valdes, D.M.; Abdelfatah, A.S.; Mahmassani, H.S. Real-time dynamic traffic assignment and path-based signal coordination; application to network traffic management. </a><i>Transp. Res. Rec. </i><b>1999</b>, <i>1667</i><a href="http://dx.doi.org/10.3141/1667-09" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 67–76. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark117">Ashtiani, F.; Fayazi, S.A.; Vahidi, A. Multi-intersection traffic management for autonomous vehicles via distributed mixed integer linear programming. In Proceedings of the 2018 Annual American Control Conference (ACC), Milwaukee, WI, USA, 27–29 June 2018; pp. 6341–6346.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Fayazi, S.A.; Vahidi, A. Mixed-integer linear programming for optimal scheduling of autonomous vehicle intersection crossing.</p><p class="s57" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark118">IEEE Trans. Intell. Veh. </a><b>2018</b><span class="s14">, </span>3<a href="http://dx.doi.org/10.1109/TIV.2018.2843163" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 287–299. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Ng, K.M.; Reaz, M.B.I.; Ali, M.A.M. A review on the applications of Petri nets in modeling, analysis, and control of urban traffic.</p><p class="s57" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark119">IEEE Trans. Intell. Transp. Syst. </a><b>2013</b><span class="s14">, </span>14<a href="http://dx.doi.org/10.1109/TITS.2013.2246153" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 858–870. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark120">Papadopoulos, G.; Bastas, A.; Vouros, G.A.; Crook, I.; Andrienko, N.; Andrienko, G.; Cordero, J.M. Deep reinforcement learning in service of air traffic controllers to resolve tactical conflicts. </a><i>Expert Syst. Appl. </i><b>2024</b>, <i>236</i><a href="http://dx.doi.org/10.1016/j.eswa.2023.121234" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 121234. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark121">Shashi, F.I.; Sultan, S.M.; Khatun, A.; Sultana, T.; Alam, T. A study on deep reinforcement learning based traffic signal control for mitigating traffic congestion. In Proceedings of the 2021 IEEE 3rd Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS), Tainan, Taiwan, 28–30 May 2021; pp. 288–291.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Koukol, M.; Zajícˇková, L.; Marek, L.; Tucˇek, P. Fuzzy logic in traffic engineering: A review on signal control. <i>Math. Probl. Eng.</i></p><p class="s14" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark122"><b>2015</b></a>, <i>2015</i><a href="http://dx.doi.org/10.1155/2015/979160" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 979160. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark123">Mohanaselvi, S.; Shanpriya, B. Application of fuzzy logic to control traffic signals. </a><i>AIP Conf. Proc. </i><b>2019</b>, <i>2112</i>, 020045.</p></li><li style="padding-top: 1pt;padding-left: 26pt;text-indent: -20pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark124">Mao, T.; Miha˘ita˘, A.S.; Chen, F.; Vu, H.L. Boosted genetic algorithm using machine learning for traffic control optimization. </a><i>IEEE Trans. Intell. Transp. Syst. </i><b>2021</b>, <i>23</i><a href="http://dx.doi.org/10.1109/TITS.2021.3066958" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 7112–7141. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Lee, J.; Abdulhai, B.; Shalaby, A.; Chung, E.H. Real-time optimization for adaptive traffic signal control using genetic algorithms.</p><p class="s57" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark125">J. Intell. Transp. Syst. </a><b>2005</b><span class="s14">, </span>9<a href="http://dx.doi.org/10.1080/15472450500183649" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 111–122. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Wu, B.; Wang, D. Traffic signal networks control optimize with PSO algorithm. In Proceedings of the 2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD), Changsha, China, 13–15 August 2016; pp. 230–234.</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark126">Zhao, H.; Han, G.; Niu, X. The signal control optimization of road intersections with slow traffic based on improved PSO. </a><i>Mob. Netw. Appl. </i><b>2020</b>, <i>25</i><a href="http://dx.doi.org/10.1007/s11036-019-01225-7" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 623–631. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark127">Garcia-Nieto, J.; Olivera, A.C.; Alba, E. Optimal cycle program of traffic lights with particle swarm optimization. </a><i>IEEE Trans. Evol. Comput. </i><b>2013</b>, <i>17</i><a href="http://dx.doi.org/10.1109/TEVC.2013.2260755" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 823–839. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -20pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark128">Li, D.; De Schutter, B. Distributed model-free adaptive predictive control for urban traffic networks. </a><i>IEEE Trans. Control. Syst. Technol. </i><b>2021</b>, <i>30</i><a href="http://dx.doi.org/10.1109/TCST.2021.3059460" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 180–192. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark129">Michailidis, P.; Michailidis, I.; Kosmatopoulos, E. Reinforcement Learning for Optimizing Renewable Energy Utilization in Buildings: A Review on Applications and Innovations. </a><i>Energies </i><b>2025</b>, <i>18</i><a href="http://dx.doi.org/10.3390/en18071724" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1724. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Wang, Y.; Yang, X.; Liang, H.; Liu, Y. A review of the self-adaptive traffic signal control system based on future traffic environment.</p><p class="s57" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark130">J. Adv. Transp. </a><b>2018</b><span class="s14">, </span>2018<a href="http://dx.doi.org/10.1155/2018/1096123" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1096123. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark131">Mohammadi, P.; Nasiri, A.; Darshi, R.; Shirzad, A.; Abdollahipour, R. Achieving Cost Efficiency in Cloud Data Centers Through Model-Free Q-Learning. In Proceedings of the International Conference on Electrical and Electronics Engineering, Melbourne, Australia, 11–12 September 2024; pp. 457–468.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark132">Watkins, C.J.; Dayan, P. Q-learning. </a><i>Mach. Learn. </i><b>1992</b>, <i>8</i><a href="http://dx.doi.org/10.1007/BF00992698" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 279–292. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark133">Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A.A.; Veness, J.; Bellemare, M.G.; Graves, A.; Riedmiller, M.; Fidjeland, A.K.; Ostrovski, G.; et al. Human-level control through deep reinforcement learning. </a><i>Nature </i><b>2015</b>, <i>518</i><a href="http://dx.doi.org/10.1038/nature14236" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 529–533. [</a><span style=" color: #0774B7;">CrossRef</span><a href="http://www.ncbi.nlm.nih.gov/pubmed/25719670" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">] [</a><span style=" color: #0774B7;">PubMed</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Zeng, J.; Hu, J.; Zhang, Y. Adaptive traffic signal control with deep recurrent Q-learning. In Proceedings of the 2018 IEEE Intelligent Vehicles Symposium (IV), Changshu, China, 26–30 June 2018; pp. 1215–1220.</p></li><li style="padding-top: 2pt;padding-left: 26pt;text-indent: -20pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark134">Ducrocq, R.; Farhi, N. Deep reinforcement Q-learning for intelligent traffic signal control with partial detection. </a><i>Int. J. Intell. Transp. Syst. Res. </i><b>2023</b>, <i>21</i><a href="http://dx.doi.org/10.1007/s13177-023-00346-4" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 192–206. [</a><span style=" color: #0774B7;">CrossRef</span>]<a name="bookmark135">&zwnj;</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark136">Guerrero-Ibanez, A.; Contreras-Castillo, J.; Buenrostro, R.; Marti, A.B.; Muñoz, A.R. A policy-based multi-agent management approach for intelligent traffic-light control. In Proceedings of the 2010 IEEE Intelligent Vehicles Symposium, La Jolla, CA, USA, 21–24 June 2010; pp. 694–699.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark137">Li, Y.; He, J.; Gao, Y. Intelligent traffic signal control with deep reinforcement learning at single intersection. In Proceedings of the 2021 7th International Conference on Computing and Artificial Intelligence, Tianjin, China, 23–26 April 2021; pp. 399–406.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Huang, L.; Qu, X. Improving traffic signal control operations using proximal policy optimization. <i>IET Intell. Transp. Syst. </i><b>2023</b>,</p><p class="s57" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark138">17</a><a href="http://dx.doi.org/10.1049/itr2.12286" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 592–605. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark139">Wei, H.; Liu, X.; Mashayekhy, L.; Decker, K. Mixed-autonomy traffic control with proximal policy optimization. In Proceedings of the 2019 IEEE Vehicular Networking Conference (VNC), Los Angeles, CA, USA, 4–6 December 2019; pp. 1–8.</a></p></li><li style="padding-left: 26pt;text-indent: -20pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark140">Mao, F.; Li, Z.; Lin, Y.; Li, L. Mastering arterial traffic signal control with multi-agent attention-based soft actor-critic model. </a><i>IEEE Trans. Intell. Transp. Syst. </i><b>2022</b>, <i>24</i><a href="http://dx.doi.org/10.1109/TITS.2022.3229477" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 3129–3144. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark141">Lillicrap, T. Continuous control with deep reinforcement learning. </a><i>arXiv </i><b>2015</b>, arXiv:1509.02971.</p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark142">Ding, F.; Ma, G.; Chen, Z.; Gao, J.; Li, P. Averaged Soft Actor-Critic for Deep Reinforcement Learning. </a><i>Complexity </i><b>2021</b>, <i>2021</i><a href="http://dx.doi.org/10.1155/2021/6658724" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 6658724. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark143">Greguric´, M.; Vujic´, M.; Alexopoulos, C.; Miletic´, M. Application of deep reinforcement learning in traffic signal control: An overview and impact of open traffic data. </a><i>Appl. Sci. </i><b>2020</b>, <i>10</i><a href="http://dx.doi.org/10.3390/app10114011" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 4011. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Ayeelyan, J.; Lee, G.H.; Hsu, H.C.; Hsiung, P.A. Advantage actor-critic for autonomous intersection management. <i>Vehicles </i><b>2022</b>,</p><p class="s57" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark144">4</a><a href="http://dx.doi.org/10.3390/vehicles4040073" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1391–1412. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark145">Li, L.; Lv, Y.; Wang, F.Y. Traffic signal timing via deep reinforcement learning. </a><i>IEEE/CAA J. Autom. Sin. </i><b>2016</b>, <i>3</i><a href="http://dx.doi.org/10.1109/JAS.2016.7508798" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 247–254. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark146">Nishi, T.; Otaki, K.; Hayakawa, K.; Yoshimura, T. Traffic signal control based on reinforcement learning with graph convolutional neural nets. In Proceedings of the 2018 21st International Conference on Intelligent Transportation Systems (ITSC), Maui, HI, USA, 4–7 November 2018; pp. 877–883.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark147">Jin, J.; Ma, X. Hierarchical multi-agent control of traffic lights based on collective learning. </a><i>Eng. Appl. Artif. Intell. </i><b>2018</b>, <i>68</i><a href="http://dx.doi.org/10.1016/j.engappai.2017.10.013" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 236–248. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark148">Aziz, H.A.; Zhu, F.; Ukkusuri, S.V. Learning-based traffic signal control algorithms with neighborhood information sharing: An application for sustainable mobility. </a><i>J. Intell. Transp. Syst. </i><b>2018</b>, <i>22</i><a href="http://dx.doi.org/10.1080/15472450.2017.1387546" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 40–52. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark149">Guo, M.; Wang, P.; Chan, C.Y.; Askary, S. A reinforcement learning approach for intelligent traffic signal control at urban intersections. In Proceedings of the 2019 IEEE Intelligent Transportation Systems Conference (ITSC), Auckland, New Zealand, 27–30 October 2019; pp. 4242–4247.</a></p></li><li style="padding-left: 26pt;text-indent: -20pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark150">Liang, X.; Du, X.; Wang, G.; Han, Z. A deep q learning network for traffic lights’ cycle control in vehicular networks. </a><i>IEEE Trans. Veh. Technol. </i><b>2019</b>, <i>68</i><a href="http://dx.doi.org/10.1109/TVT.2018.2890726" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1243–1253. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark151">Tan, T.; Bao, F.; Deng, Y.; Jin, A.; Dai, Q.; Wang, J. Cooperative deep reinforcement learning for large-scale traffic grid signal control. </a><i>IEEE Trans. Cybern. </i><b>2019</b>, <i>50</i><a href="http://dx.doi.org/10.1109/TCYB.2019.2904742" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 2687–2700. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark152">Zhang, R.; Ishikawa, A.; Wang, W.; Striner, B.; Tonguz, O.K. Using reinforcement learning with partial vehicle detection for intelligent traffic signal control. </a><i>IEEE Trans. Intell. Transp. Syst. </i><b>2020</b>, <i>22</i><a href="http://dx.doi.org/10.1109/TITS.2019.2958859" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 404–415. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark153">Zhou, P.; Chen, X.; Liu, Z.; Braud, T.; Hui, P.; Kangasharju, J. DRLE: Decentralized reinforcement learning at the edge for traffic light control in the IoV. </a><i>IEEE Trans. Intell. Transp. Syst. </i><b>2020</b>, <i>22</i><a href="http://dx.doi.org/10.1109/TITS.2020.3035841" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 2262–2273. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark154">Zang, X.; Yao, H.; Zheng, G.; Xu, N.; Xu, K.; Li, Z. Metalight: Value-based meta-reinforcement learning for traffic signal control. In Proceedings of the AAAI Conference on Artificial Intelligence, New York, NY, USA, 7–12 February 2020; Volume 34, pp. 1153–1160.</a></p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark155">Rasheed, F.; Yau, K.L.A.; Noor, R.M.; Wu, C.; Low, Y.C. Deep reinforcement learning for traffic signal control: A review. </a><i>IEEE Access </i><b>2020</b>, <i>8</i><a href="http://dx.doi.org/10.1109/ACCESS.2020.3034141" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 208016–208044. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Padakandla, S.; KJ, P.; Bhatnagar, S.  Reinforcement learning algorithm for non-stationary environments.   <i>Appl.  Intell.  </i><b>2020</b>,</p><p class="s57" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark156">50</a><a href="http://dx.doi.org/10.1007/s10489-020-01758-5" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 3590–3606. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark157">Chen, C.; Wei, H.; Xu, N.; Zheng, G.; Yang, M.; Xiong, Y.; Xu, K.; Li, Z. Toward a thousand lights: Decentralized deep reinforcement learning for large-scale traffic signal control. In Proceedings of the AAAI Conference on Artificial Intelligence, New York, NY, USA, 7–12 February 2020; Volume 34, pp. 3414–3421.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark158">Essa, M.; Sayed, T. Self-learning adaptive traffic signal control for real-time safety optimization. </a><i>Accid. Anal. Prev. </i><b>2020</b>, <i>146</i><a href="http://dx.doi.org/10.1016/j.aap.2020.105713" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 105713. [</a><span style=" color: #0774B7;">CrossRef</span><a href="http://www.ncbi.nlm.nih.gov/pubmed/32823035" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">] [</a><span style=" color: #0774B7;">PubMed</span>]</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Gong, Y.; Abdel-Aty, M.; Yuan, J.; Cai, Q. Multi-objective reinforcement learning approach for improving safety at intersections with adaptive traffic signal control. <i>Accid. Anal. Prev. </i><b>2020</b>, <i>144</i><a href="http://dx.doi.org/10.1016/j.aap.2020.105655" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 105655. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-top: 2pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark159">Joo, H.; Ahmed, S.H.; Lim, Y. Traffic signal control for smart cities using reinforcement learning. </a><i>Comput. Commun. </i><b>2020</b>, <i>154</i><a href="http://dx.doi.org/10.1016/j.comcom.2020.03.005" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 324–330. [</a><span style=" color: #0774B7;">CrossRef</span>]<a name="bookmark160">&zwnj;</a></p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark161">Wang, X.; Ke, L.; Qiao, Z.; Chai, X. Large-scale traffic signal control using a novel multiagent reinforcement learning. </a><i>IEEE Trans. Cybern. </i><b>2020</b>, <i>51</i><a href="http://dx.doi.org/10.1109/TCYB.2020.3015811" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 174–187. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark162">Xu, M.; Wu, J.; Huang, L.; Zhou, R.; Wang, T.; Hu, D. Network-wide traffic signal control based on the discovery of critical nodes and deep reinforcement learning. </a><i>J. Intell. Transp. Syst. </i><b>2020</b>, <i>24</i><a href="http://dx.doi.org/10.1080/15472450.2018.1527694" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1–10. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Devailly, F.X.; Larocque, D.; Charlin, L. IG-RL: Inductive graph reinforcement learning for massive-scale traffic signal control.</p><p class="s57" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark163">IEEE Trans. Intell. Transp. Syst. </a><b>2021</b><span class="s14">, </span>23<a href="http://dx.doi.org/10.1109/TITS.2021.3070835" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 7496–7507. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark164">Mushtaq, A.; Haq, I.U.; Imtiaz, M.U.; Khan, A.; Shafiq, O. Traffic flow management of autonomous vehicles using deep reinforcement learning and smart rerouting. </a><i>IEEE Access </i><b>2021</b>, <i>9</i><a href="http://dx.doi.org/10.1109/ACCESS.2021.3063463" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 51005–51019. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark165">Wang, T.; Cao, J.; Hussain, A. Adaptive Traffic Signal Control for large-scale scenario with Cooperative Group-based Multi-agent reinforcement learning. </a><i>Transp. Res. Part C Emerg. Technol. </i><b>2021</b>, <i>125</i><a href="http://dx.doi.org/10.1016/j.trc.2021.103046" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 103046. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark166">Haddad, T.A.; Hedjazi, D.; Aouag, S. A deep reinforcement learning-based cooperative approach for multi-intersection traffic signal control. </a><i>Eng. Appl. Artif. Intell. </i><b>2022</b>, <i>114</i><a href="http://dx.doi.org/10.1016/j.engappai.2022.105019" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 105019. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Kolat, M.; Ko˝vári, B.; Bécsi, T.; Aradi, S. Multi-agent reinforcement learning for traffic signal control: A cooperative approach.</p><p class="s57" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark167">Sustainability </a><b>2023</b><span class="s14">, </span>15<a href="http://dx.doi.org/10.3390/su15043479" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 3479. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark168">Bouktif, S.; Cheniki, A.; Ouni, A.; El-Sayed, H. Deep reinforcement learning for traffic signal control with consistent state and reward design approach. </a><i>Knowl.-Based Syst. </i><b>2023</b>, <i>267</i><a href="http://dx.doi.org/10.1016/j.knosys.2023.110440" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 110440. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark169">Yazdani, M.; Sarvi, M.; Bagloee, S.A.; Nassir, N.; Price, J.; Parineh, H. Intelligent vehicle pedestrian light (IVPL): A deep reinforcement learning approach for traffic signal control. </a><i>Transp. Res. Part C Emerg. Technol. </i><b>2023</b>, <i>149</i><a href="http://dx.doi.org/10.1016/j.trc.2022.103991" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 103991. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Su, Z.; Chow, A.H.; Fang, C.; Liang, E.; Zhong, R. Hierarchical control for stochastic network traffic with reinforcement learning.</p><p class="s57" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark170">Transp. Res. Part B Methodol. </a><b>2023</b><span class="s14">, </span>167<a href="http://dx.doi.org/10.1016/j.trb.2022.12.001" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 196–216. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark171">Du, W.; Ye, J.; Gu, J.; Li, J.; Wei, H.; Wang, G. Safelight: A reinforcement learning method toward collision-free traffic signal control. In Proceedings of the AAAI Conference on Artificial Intelligence, Washington, DC, USA, 7–14 February 2023; Volume 37, pp. 14801–14810.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark172">Li, D.; Zhu, F.; Wu, J.; Wong, Y.D.; Chen, T. Managing mixed traffic at signalized intersections: An adaptive signal control and CAV coordination system based on deep reinforcement learning. </a><i>Expert Syst. Appl. </i><b>2024</b>, <i>238</i><a href="http://dx.doi.org/10.1016/j.eswa.2023.121959" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 121959. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark173">Vieira, M.A.; Galvão, G.; Vieira, M.; Louro, P.; Vestias, M.; Vieira, P. Enhancing urban intersection efficiency: Visible light communication and learning-based control for traffic signal optimization and vehicle management. </a><i>Symmetry </i><b>2024</b>, <i>16</i><a href="http://dx.doi.org/10.3390/sym16020240" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 240. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark174">Zhang, G.; Chang, F.; Jin, J.; Yang, F.; Huang, H. Multi-objective deep reinforcement learning approach for adaptive traffic signal control system with concurrent optimization of safety, efficiency, and decarbonization at intersections. </a><i>Accid. Anal. Prev. </i><b>2024</b>,  <i>199</i><a href="http://dx.doi.org/10.1016/j.aap.2023.107451" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 107451. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark175">Mousavi, S.S.; Schukat, M.; Howley, E. Traffic light control using deep policy-gradient and value-function-based reinforcement learning. </a><i>IET Intell. Transp. Syst. </i><b>2017</b>, <i>11</i><a href="http://dx.doi.org/10.1049/iet-its.2017.0153" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 417–423. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark176">Garg, D.; Chli, M.; Vogiatzis, G. Deep reinforcement learning for autonomous traffic light control. In Proceedings of the 2018 3rd IEEE International Conference on Intelligent Transportation Engineering (ICITE), Singapore, 3–5 September 2018; pp. 214–218.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark177">Oroojlooy, A.; Nazari, M.; Hajinezhad, D.; Silva, J. Attendlight: Universal attention-based reinforcement learning model for traffic signal control. </a><i>Adv. Neural Inf. Process. Syst. </i><b>2020</b>, <i>33</i>, 4079–4090.</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark178">Guo, J.; Cheng, L.; Wang, S. CoTV: Cooperative control for traffic light signals and connected autonomous vehicles using deep reinforcement learning. </a><i>IEEE Trans. Intell. Transp. Syst. </i><b>2023</b>, <i>24</i><a href="http://dx.doi.org/10.1109/TITS.2023.3276416" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 10501–10512. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark179">Aslani, M.; Mesgari, M.S.; Wiering, M. Adaptive traffic signal control with actor-critic methods in a real-world traffic network with different traffic disruption events. </a><i>Transp. Res. Part C Emerg. Technol. </i><b>2017</b>, <i>85</i><a href="http://dx.doi.org/10.1016/j.trc.2017.09.020" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 732–752. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark180">Chu, T.; Wang, J.; Codecà, L.; Li, Z. Multi-agent deep reinforcement learning for large-scale traffic signal control. </a><i>IEEE Trans. Intell. Transp. Syst. </i><b>2019</b>, <i>21</i><a href="http://dx.doi.org/10.1109/TITS.2019.2901791" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1086–1095. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark181">Li, Z.; Yu, H.; Zhang, G.; Dong, S.; Xu, C.Z. Network-wide traffic signal control optimization using a multi-agent deep reinforcement learning. </a><i>Transp. Res. Part C Emerg. Technol. </i><b>2021</b>, <i>125</i><a href="http://dx.doi.org/10.1016/j.trc.2021.103059" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 103059. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark182">Li, Z.; Xu, C.; Zhang, G. A deep reinforcement learning approach for traffic signal control optimization. </a><i>arXiv </i><b>2021</b>, arXiv:2107.06115.</p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Guo, Y.; Ma, J. DRL-TP3: A learning and control framework for signalized intersections with mixed connected automated traffic.</p><p class="s57" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark183">Transp. Res. Part C Emerg. Technol. </a><b>2021</b><span class="s14">, </span>132<a href="http://dx.doi.org/10.1016/j.trc.2021.103416" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 103416. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark184">Yang, S.; Yang, B.; Kang, Z.; Deng, L. IHG-MA: Inductive heterogeneous graph multi-agent reinforcement learning for multi- intersection traffic signal control. </a><i>Neural Netw. </i><b>2021</b>, <i>139</i><a href="http://dx.doi.org/10.1016/j.neunet.2021.03.015" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 265–277. [</a><span style=" color: #0774B7;">CrossRef</span><a href="http://www.ncbi.nlm.nih.gov/pubmed/33838602" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">] [</a><span style=" color: #0774B7;">PubMed</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Damadam, S.; Zourbakhsh, M.; Javidan, R.; Faroughi, A. An intelligent IoT based traffic light management system: Deep reinforcement learning. <i>Smart Cities </i><b>2022</b>, <i>5</i><a href="http://dx.doi.org/10.3390/smartcities5040066" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 1293–1311. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-top: 2pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark185">Wu, Q.; Wu, J.; Shen, J.; Du, B.; Telikani, A.; Fahmideh, M.; Liang, C. Distributed agent-based deep reinforcement learning for large scale traffic signal control. </a><i>Knowl.-Based Syst. </i><b>2022</b>, <i>241</i><a href="http://dx.doi.org/10.1016/j.knosys.2022.108304" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 108304. [</a><span style=" color: #0774B7;">CrossRef</span>]<a name="bookmark186">&zwnj;</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Mo, Z.; Li, W.; Fu, Y.; Ruan, K.; Di, X. CVLight: Decentralized learning for adaptive traffic signal control with connected vehicles.</p><p class="s57" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark187">Transp. Res. Part C Emerg. Technol. </a><b>2022</b><span class="s14">, </span>141<a href="http://dx.doi.org/10.1016/j.trc.2022.103728" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 103728. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark188">Su, H.; Zhong, Y.D.; Chow, J.Y.; Dey, B.; Jin, L. EMVLight: A multi-agent reinforcement learning framework for an emergency vehicle decentralized routing and traffic signal control system. </a><i>Transp. Res. Part C Emerg. Technol. </i><b>2023</b>, <i>146</i><a href="http://dx.doi.org/10.1016/j.trc.2022.103955" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 103955. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark189">Darmoul, S.; Elkosantini, S.; Louati, A.; Said, L.B. Multi-agent immune networks to control interrupted flow at signalized intersections. </a><i>Transp. Res. Part C Emerg. Technol. </i><b>2017</b>, <i>82</i><a href="http://dx.doi.org/10.1016/j.trc.2017.07.003" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 290–313. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark190">Wei, H.; Xu, N.; Zhang, H.; Zheng, G.; Zang, X.; Chen, C.; Zhang, W.; Zhu, Y.; Xu, K.; Li, Z. Colight: Learning network-level cooperation for traffic signal control. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, Beijing, China, 3–7 November 2019; pp. 1913–1922.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark191">Kim, D.; Jeong, O. Cooperative traffic signal control with traffic flow prediction in multi-intersection. </a><i>Sensors </i><b>2019</b>, <i>20</i><a href="http://dx.doi.org/10.3390/s20010137" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 137. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark192">Wu, T.; Zhou, P.; Liu, K.; Yuan, Y.; Wang, X.; Huang, H.; Wu, D.O. Multi-agent deep reinforcement learning for urban traffic light control in vehicular networks. </a><i>IEEE Trans. Veh. Technol. </i><b>2020</b>, <i>69</i><a href="http://dx.doi.org/10.1109/TVT.2020.2997896" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 8243–8256. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark193">Xu, B.; Wang, Y.; Wang, Z.; Jia, H.; Lu, Z. Hierarchically and cooperatively learning traffic signal control. In Proceedings of the AAAI Conference on Artificial Intelligence, Virtually, 2–9 February 2021; Volume 35, pp. 669–677.</a></p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark194">Kumar, N.; Rahman, S.S.; Dhakad, N. Fuzzy inference enabled deep reinforcement learning-based traffic light control for intelligent transportation system. </a><i>IEEE Trans. Intell. Transp. Syst. </i><b>2020</b>, <i>22</i><a href="http://dx.doi.org/10.1109/TITS.2020.2984033" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 4919–4928. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;">Wang, M.; Wu, L.; Li, J.; He, L. Traffic signal control with reinforcement learning based on region-aware cooperative strategy.</p><p class="s57" style="padding-top: 1pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark195">IEEE Trans. Intell. Transp. Syst. </a><b>2021</b><span class="s14">, </span>23<a href="http://dx.doi.org/10.1109/TITS.2021.3062072" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 6774–6785. [</a><span class="s61">CrossRef</span><span class="s14">]</span></p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark196">Abdoos, M.; Bazzan, A.L. Hierarchical traffic signal optimization using reinforcement learning and traffic prediction with long-short term memory. </a><i>Expert Syst. Appl. </i><b>2021</b>, <i>171</i><a href="http://dx.doi.org/10.1016/j.eswa.2021.114580" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 114580. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark197">Guo, G.; Wang, Y. An integrated MPC and deep reinforcement learning approach to trams-priority active signal control. </a><i>Control Eng. Pract. </i><b>2021</b>, <i>110</i><a href="http://dx.doi.org/10.1016/j.conengprac.2021.104758" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 104758. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark198">Wang, M.; Wu, L.; Li, M.; Wu, D.; Shi, X.; Ma, C. Meta-learning based spatial-temporal graph attention network for traffic signal control. </a><i>Knowl.-Based Syst. </i><b>2022</b>, <i>250</i><a href="http://dx.doi.org/10.1016/j.knosys.2022.109166" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 109166. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s14" style="display: inline;"><a name="bookmark199">Tunc, I.; Soylemez, M.T. Fuzzy logic and deep Q learning based control for traffic lights. </a><i>Alex. Eng. J. </i><b>2023</b>, <i>67</i><a href="http://dx.doi.org/10.1016/j.aej.2022.12.028" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 343–359. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-top: 1pt;padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark200">Wang, T.; Zhu, Z.; Zhang, J.; Tian, J.; Zhang, W. A large-scale traffic signal control algorithm based on multi-layer graph deep reinforcement learning. </a><i>Transp. Res. Part C Emerg. Technol. </i><b>2024</b>, <i>162</i><a href="http://dx.doi.org/10.1016/j.trc.2024.104582" style=" color: black; font-family:&quot;Book Antiqua&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">, 104582. [</a><span style=" color: #0774B7;">CrossRef</span>]</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark201">Krajzewicz, D. Traffic simulation with SUMO–simulation of urban mobility. In </a><i>Fundamentals of Traffic Simulation</i>; Springer: Berlin/Heidelberg, Germany, 2010; pp. 269–293.</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark202">Fellendorf, M.; Vortisch, P. Microscopic traffic flow simulator VISSIM. In </a><i>Fundamentals of Traffic Simulation</i>; Springer: Berlin/Heidelberg, Germany, 2010; pp. 63–93.</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark203">Casas, J.; Ferrer, J.L.; Garcia, D.; Perarnau, J.; Torday, A. Traffic simulation with aimsun. In </a><i>Fundamentals of Traffic Simulation</i>; Springer: Berlin/Heidelberg, Germany, 2010; pp. 173–232.</p></li><li style="padding-left: 27pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark204">Zhang, H.; Feng, S.; Liu, C.; Ding, Y.; Zhu, Y.; Zhou, Z.; Zhang, W.; Yu, Y.; Jin, H.; Li, Z. Cityflow: A multi-agent reinforcement learning environment for large scale city traffic scenario. In Proceedings of the World Wide Web Conference, San Francisco, CA, USA, 13–17 May 2019; pp. 3620–3624.</a></p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;"><a name="bookmark205">Sykes, P. Traffic simulation with paramics. In </a><i>Fundamentals of Traffic Simulation</i>; Springer: Berlin/Heidelberg, Germany, 2010; pp. 131–171.</p></li><li style="padding-left: 26pt;text-indent: -21pt;line-height: 114%;text-align: justify;"><p class="s14" style="display: inline;">Messaoudi, F.; Simon, G.; Ksentini, A. Dissecting games engines: The case of Unity3D. In Proceedings of the 2015 International Workshop on Network and Systems Support for Games (NetGames), Zagreb, Croatia, 3–4 December 2015; pp. 1–6.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 5pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Disclaimer/Publisher’s Note: <span class="s14">The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</span></h4></body></html>
