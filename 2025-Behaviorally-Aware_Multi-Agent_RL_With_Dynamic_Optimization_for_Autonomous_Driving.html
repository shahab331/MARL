<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Behaviorally-Aware Multi-Agent RL With Dynamic Optimization for Autonomous Driving</title><meta name="author" content="Hamid Taghavifar"/><meta name="description" content="IEEE Transactions on Automation Science and Engineering;2025;22; ;10.1109/TASE.2025.3527327"/><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 24pt; }
 .s2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s4 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .h3, h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s5 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s6 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 29pt; }
 .a { color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s7 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s8 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s9 { color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s10 { color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s11 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s12 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s13 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s14 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s15 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s16 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s17 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 8pt; }
 .s18 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -2pt; }
 .s19 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s20 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -2pt; }
 .s21 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s22 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s23 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s24 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -3pt; }
 .s25 { color: black; font-family:Calibri, sans-serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s26 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 6pt; }
 .s27 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -1pt; }
 .s28 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s29 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s30 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 9pt; }
 .s31 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s32 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -3pt; }
 .s33 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -7pt; }
 .s34 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s35 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 3pt; }
 .s36 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -10pt; }
 .s37 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 7pt; }
 .h2, h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s38 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s39 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s40 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -1pt; }
 .s41 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s42 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s43 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -5pt; }
 .s44 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s45 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 12pt; }
 .s46 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 9pt; }
 .s47 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 5pt; }
 .s48 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 10pt; }
 .s49 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s50 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 6pt; }
 .s51 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 9pt; }
 .s52 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s53 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s54 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s55 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s56 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -4pt; }
 .s57 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s58 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s59 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s60 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 3pt; }
 .s61 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s62 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s63 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -3pt; }
 .s64 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 7pt; }
 .s65 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s66 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s67 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -3pt; }
 .s68 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s69 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s70 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 7pt; }
 .s71 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 7pt; }
 .s72 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 2pt; }
 .s73 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s74 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -2pt; }
 .s75 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 7pt; }
 .s76 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; }
 .s77 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s78 { color: #004392; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 h4 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 22; }
 #l1> li:before {counter-increment: c1; content: counter(c1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l2 {padding-left: 0pt;counter-reset: c2 0; }
 #l2> li:before {counter-increment: c2; content: counter(c2, upper-roman)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l3 {padding-left: 0pt;counter-reset: d1 0; }
 #l3> li:before {counter-increment: d1; content: counter(d1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l4 {padding-left: 0pt;counter-reset: e1 0; }
 #l4> li:before {counter-increment: e1; content: counter(e1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l5 {padding-left: 0pt;counter-reset: e2 0; }
 #l5> li:before {counter-increment: e2; content: counter(e2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l6 {padding-left: 0pt;counter-reset: f1 0; }
 #l6> li:before {counter-increment: f1; content: counter(f1, lower-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l7 {padding-left: 0pt;counter-reset: g1 0; }
 #l7> li:before {counter-increment: g1; content: counter(g1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
</style></head><body><p class="s1" style="padding-left: 74pt;text-indent: -66pt;text-align: left;">Behaviorally-Aware Multi-Agent RL With Dynamic Optimization for Autonomous Driving</p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_001.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_002.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_003.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_004.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_005.png"/></span></p><p class="s2" style="padding-top: 13pt;padding-left: 145pt;text-indent: -89pt;line-height: 106%;text-align: left;">Hamid Taghavifar  , <i>Senior Member, IEEE</i>, Chuan Hu  , Chongfeng Wei  , <i>Member, IEEE</i>, Ardashir Mohammadzadeh  , and Chunwei Zhang</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: justify;">Abstract<span class="h3">— This study presents a novel Multi-Agent Reinforce- ment Learning (MURL) architecture for autonomous vehicle (AV) navigation in complex urban traffic environments. By integrating a Social Value Orientation (SVO) model into a model-free SARSA reinforcement learning framework, our approach effectively balances individual agents’ social preferences with safety and per- formance objectives. A logistic regression-based risk assessment module evaluates collision probabilities in real time by analyz- ing spatiotemporal dynamics such as distances and velocities. Additionally, a dynamic optimizer adapts the learning rate and exploration strategies of the SARSA algorithm to provide efficient convergence to optimal policies. Extensive simulation experiments demonstrate that the proposed method significantly enhances safety and efficiency, achieving a 55.6% reduction in collision risk and increasing average rewards per episode by 2.1 compared to traditional SARSA without SVO. Furthermore, the optimized policy reduces average episode length, indicating the framework’s effectiveness in providing robust decision-making and adaptabil- ity across various traffic scenarios.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: right;">Note to Practitioners<span class="h3">—The proposed framework in this paper is driven by the demand for comprehensive navigation systems in the rapidly evolving field of connected and autonomous vehicles (CAVs),  especially  within  complex  and  unpredictable  urban environments and mixed traffic scenarios. As AVs are getting more and  more attention,  the  capacity to  navigate  effectively among many road users, including other AVs, pedestrians, and human-driven vehicles, is essential. Our framework builds upon the SARSA algorithm to produce an optimal policy for the AV and integrates a dynamic optimization method that represents the concept of risk as the inverse logistic of potential collisions. Distinctive to our proposed model is a finely-tuned Social Value Orientation (SVO) that captures the nuanced social dynamics</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Received 14 October 2024; revised 3 December 2024; accepted 3 January</p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">2025. Date of publication 8 January 2025; date of current version 8 April 2025. This article was recommended for publication by Associate Editor</p><ol id="l1"><li style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><p class="s5" style="display: inline;">Zhang and Editor P. Rocco upon evaluation of the reviewers’ comments. This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN-2023-04816. <i>(Corresponding authors: Hamid Taghavifar; Ardashir Mohammadzadeh; Chunwei Zhang.)</i></p><p class="s5" style="padding-left: 5pt;text-indent: 6pt;line-height: 9pt;text-align: justify;">Hamid Taghavifar is with the Department of Mechanical, Industrial and Aerospace Engineering, Concordia University, Montreal, QC H3G 1M8, Canada (e-mail: hamid.taghavifar@concordia.ca).</p><p class="s5" style="padding-left: 5pt;text-indent: 6pt;line-height: 9pt;text-align: justify;">Chuan Hu is with the School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai 200240, China.</p><p class="s5" style="padding-left: 5pt;text-indent: 6pt;line-height: 9pt;text-align: justify;">Chongfeng Wei is with the James Watt School of Engineering, University of Glasgow, G12 8QQ Glasgow, U.K.</p><p class="s5" style="padding-left: 5pt;text-indent: 6pt;line-height: 9pt;text-align: justify;">Ardashir Mohammadzadeh is with the Department of Electrical and Elec- tronics Engineering, Sakarya University, 54050 Sakarya, Türkiye (e-mail: ardashir@skarya.edu.tr).</p><p class="s5" style="padding-left: 5pt;text-indent: 6pt;line-height: 9pt;text-align: justify;">Chunwei Zhang is with the Multidisciplinary Center for Infrastructure Engineering, Shenyang University of Technology, Shenyang 110870, China (e-mail: zhangchunwei@sut.edu.cn).</p><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Digital Object Identifier 10.1109/TASE.2025.3527327</p><h3 style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">between multiple autonomous agents, spanning a continuum from self-interested to entirely cooperative behaviors. This allows AVs to make decisions socially and cooperatively. This framework significantly influences the AV navigation sector by contributing to the development of secure, human-centric, and reliable trans- portation systems. Its multi-agent focus and the incorporation of dynamic  optimization  emphasize  its  potential  to  facilitate a network of AVs that interact with diverse road users, thus improving scalability. The robustness and adaptability of  this machine learning-powered solution are crucial for navigating the varied scenarios that characterize urban driving, ensuring that AVs can adapt to changing conditions and make decisions that benefit all road users.</h3><p class="s4" style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: justify;">Index Terms<span class="h3">— Automated vehicles, collision avoidance, rein- forcement learning, path planning.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l2"><li style="padding-left: 15pt;text-indent: 76pt;text-align: left;"><p style="display: inline;">I<span class="s5">NTRODUCTION</span></p><h1 style="text-indent: 0pt;line-height: 28pt;text-align: left;">A</h1><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark0">CCORDING to the World Health Organization (WHO), there are approximately 1.19 million casualties per year</a></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark69" class="s7" name="bookmark1">due to road traffic accidents </a><a href="#bookmark69" class="a">[1]</a><a href="#bookmark70" class="s7">. The primary cause of fatalities for individuals between 5 and 29 years of age is road traffic injuries. Furthermore, 92% of road fatalities worldwide happen in low-income and middle-income countries, despite having only around 60% of the vehicles. In addition, road traffic acci- dents result in a 3% loss of GDP for most countries. AVs are expected to make a transformative change to the existing trans- portation system by improving traffic efficiency </a><a href="#bookmark70" class="a">[2]</a><a href="#bookmark70" class="s7">, </a><a href="#bookmark71" class="s7">increasing driving safety </a><a href="#bookmark71" class="a">[3]</a><a href="#bookmark72" class="s7">, reducing environmental impact </a><a href="#bookmark72" class="a">[4]</a><a href="#bookmark73" class="s7">,  and facilitating driving for the elderly and vulnerable road users or people with disabilities </a><a href="#bookmark73" class="a">[5]</a><a href="#bookmark74" class="s7">. Therefore, there has been much attention in the last couple of decades on improving the performance of AVs and their reliability for mass com- mercialization. Despite improved automotive technologies and driver assistance systems (ADAS), road accidents still occur frequently, resulting in injuries and fatalities </a><a href="#bookmark74" class="a">[6]</a><a href="#bookmark74" class="s7">.</a><a name="bookmark2">&zwnj;</a><a name="bookmark3">&zwnj;</a><a name="bookmark4">&zwnj;</a><a name="bookmark5">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark75" class="s7" name="bookmark6">A range of complex challenges has emerged since trans- forming into intelligent transportation systems in which AVs are an integral part of its ecosystem. These challenges include the technical complexities of autonomous navigation and the intricate interplay of social and ethical considerations that influence every driving decision </a><a href="#bookmark75" class="a">[7]</a><a href="#bookmark76" class="s7">, </a><a href="#bookmark76" class="a">[8]</a><a href="#bookmark77" class="s7">. Indeed, a primary concern in creating trustworthiness in AVs is the ability to make socially interpretable and intuitive judgments  in  the face of complex obstacles </a><a href="#bookmark77" class="a">[9]</a>. However, it is known that AV navigation relies on pre-defined decision-making algorithms for safe and efficient travel, while human drivers make intrin- sically intuitive decisions based on their instincts and intuitive<a name="bookmark7">&zwnj;</a><a name="bookmark8">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 59pt;text-indent: 0pt;line-height: 9pt;text-align: center;">1558-3783 © 2025 IEEE. All rights reserved, including rights for text and data mining, and training of artificial intelligence and similar technologies. Personal use is permitted, but republication/redistribution requires IEEE permission.</p><p class="s5" style="text-indent: 0pt;line-height: 9pt;text-align: center;"><a href="http://www.ieee.org/publications/rights/index.html" class="s8" target="_blank">See https://www.ieee.or</a>g/publications/rights/index.html for more information.</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark78" class="s7" name="bookmark9">adaptability </a><a href="#bookmark78" class="a">[10]</a><a href="#bookmark79" class="s7">. Consequently, designing AVs that coexist with human-driven vehicles, as well as with other cars with multi-level autonomy, cyclists, and pedestrians, has become increasingly essential and challenging </a><a href="#bookmark79" class="a">[11]</a><a href="#bookmark80" class="s7">. In this complex system of interactions, the capacity to compute the ideal route from the current position to the final destination while adhering to kinetic dynamics and physical limitations of the vehicle needs to be considered for safe autonomous driving </a><a href="#bookmark80" class="a">[12]</a><a href="#bookmark80" class="s7">.</a><a name="bookmark10">&zwnj;</a><a name="bookmark11">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark81" class="s7" name="bookmark12">Traditional navigation schemes for AVs have two main limitations. They tend to be overly cautious in design, leading to unpredictable driving that contrasts with human drivers’ intuitive behavior </a><a href="#bookmark81" class="a">[13]</a><a href="#bookmark82" class="s7">, </a><a href="#bookmark82" class="a">[14]</a><a href="#bookmark78" class="s7">. This over-cautious approach can adversely impact traffic flow and trigger confusion among other road users. Secondly, these systems encounter challenges in adapting to unforeseen scenes, which is a critical issue con- sidering the number of potential scenarios encountered on the road </a><a href="#bookmark78" class="a">[10]</a><a href="#bookmark83" class="s7">. This limitation can result in hesitation or inappropri- ate responses in complex or novel traffic conditions, potentially compromising safety and efficiency </a><a href="#bookmark83" class="a">[15]</a><a href="#bookmark82" class="s7">. A spectrum of decision-making methods has emerged to address these limi- tations, from model-based approaches to advanced data-driven tools </a><a href="#bookmark82" class="a">[14]</a><a href="#bookmark84" class="s7">. Model-based approaches develop according to predefined relationships and heuristics that provide trans- parency and simplicity </a><a href="#bookmark84" class="a">[16]</a><a href="#bookmark83" class="s7">. However, such  methods  can face problems in the events of the dynamic nature of traffic environments </a><a href="#bookmark83" class="a">[15]</a>. On the other hand, AI-enabled decision- making approaches, such as reinforcement learning (RL), have demonstrated great promise for decision-making tasks of AVs <a href="#bookmark85" class="a">[17]</a><a href="#bookmark86" class="s7">. These algorithms can comprehend interactions with the environment and adapt to changing scenarios. However, one limitation is that significant data and training may only sometimes be generalized to other environments or those deci- sions that are humanly interpretable </a><a href="#bookmark86" class="a">[18]</a><a href="#bookmark86" class="s7">. RL approaches may also encounter the credit assignment issue, which challenges learning long-term temporal correlations between decisions and the agent’s overall performance </a><a href="#bookmark86" class="a">[18]</a><a href="#bookmark87" class="s7">, </a><a href="#bookmark87" class="a">[19]</a><a href="#bookmark87" class="s7">.</a><a name="bookmark13">&zwnj;</a><a name="bookmark14">&zwnj;</a><a name="bookmark15">&zwnj;</a><a name="bookmark16">&zwnj;</a><a name="bookmark17">&zwnj;</a><a name="bookmark18">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;line-height: 11pt;text-align: justify;">The deep reinforcement learning (DRL) approach in AV</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark88" class="s7" name="bookmark19">decision-making effectively handles complex state and action spaces using neural networks for Q-value function estima- tion in RL frameworks </a><a href="#bookmark88" class="a">[20]</a><a href="#bookmark89" class="s7">. However, challenges, such as overfitting and training instability, can affect performance in real-world scenarios </a><a href="#bookmark89" class="a">[21]</a><a href="#bookmark90" class="s7">. Other recent advancements in this field include combining game theory for strategic multi-agent interactions and the development of human-centered strate- gies </a><a href="#bookmark90" class="a">[22]</a><a href="#bookmark91" class="s7">, </a><a href="#bookmark91" class="a">[23]</a><a href="#bookmark92" class="s7">. These strategies are designed to emulate human decision-making processes in AVs, thereby enhancing safety and predictability in various traffic conditions. Inverse Reinforcement Learning (IRL) has also been proven as a capable method for enhancing the navigation systems  for AVs by learning from the behavior of human drivers </a><a href="#bookmark92" class="a">[24]</a><a href="#bookmark93" class="s7">, </a><a href="#bookmark93" class="a">[25]</a><a href="#bookmark94" class="s7">. IRL aims to infer the underlying reward function that a human driver optimizes, allowing AVs to replicate human- like decision-making in complex driving environments </a><a href="#bookmark94" class="a">[26]</a><a href="#bookmark95" class="s7">. By leveraging data from human drivers, IRL can provide more intuitive and acceptable driving strategies. However, IRL also has significant drawbacks, including the complexity of accu- rately modeling human driving behavior and the computational intensity of the learning process </a><a href="#bookmark95" class="a">[27]</a>. SARSA (State-Action-<a name="bookmark20">&zwnj;</a><a name="bookmark21">&zwnj;</a><a name="bookmark22">&zwnj;</a><a name="bookmark23">&zwnj;</a><a name="bookmark24">&zwnj;</a><a name="bookmark25">&zwnj;</a><a name="bookmark26">&zwnj;</a></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark83" class="s7" name="bookmark27">Reward-State-Action), a method of RL, can be effectively utilized due to its balance of exploration and exploitation, enabling real-time decision-making  while  addressing  some of the limitations inherent in IRL </a><a href="#bookmark83" class="a">[15]</a><a href="#bookmark96" class="s7">. SARSA is an RL algorithm that can significantly enhance the decision-making task for AVs by being trained for optimal policies and updating Q-values considering the rewards as a result of the actions taken </a><a href="#bookmark96" class="a">[28]</a><a href="#bookmark97" class="s7">. SARSA is less  prone  to  overfitting  and  train- ing instability than deep learning models, and its ability to manage high-order state-action spaces is useful in navigating the complexities of AV decision-making </a><a href="#bookmark97" class="a">[29]</a><a href="#bookmark96" class="s7">. Despite their ability to optimize the reward function, RL algorithms cannot inherently ensure safety and ethical behavior as well as the social interactions between the agents and, thus, often can overlook the potential risks and broader consequences </a><a href="#bookmark96" class="a">[28]</a><a href="#bookmark98" class="s7">. To achieve this purpose, the incorporation of advanced frame- works that are behaviorally and socially aware is required. Such a framework should evaluate and mitigate risks by considering social behaviors and contextual cues within the driving environment </a><a href="#bookmark98" class="a">[30]</a><a href="#bookmark98" class="s7">.</a><a name="bookmark28">&zwnj;</a><a name="bookmark29">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark99" class="s7" name="bookmark30">Social and behavioral-aware planning and decision-making algorithms mixed with DRL and multi-agent reinforcement learning (MURL) have been studied in various urban driving conditions </a><a href="#bookmark99" class="a">[31]</a><a href="#bookmark100" class="s7">, </a><a href="#bookmark100" class="a">[32]</a><a href="#bookmark101" class="s7">, </a><a href="#bookmark101" class="a">[33]</a><a href="#bookmark102" class="s7">, </a><a href="#bookmark102" class="a">[34]</a><a href="#bookmark100" class="s7">. In </a><a href="#bookmark100" class="a">[32]</a><a href="#bookmark101" class="s7">, Valiente et al. proposed a decentralized MARL framework for cooperative AVs in mixed-autonomy traffic. For this purpose, a Double Deep Q-Network (DDQN) was utilized to enable AVs to learn from experience and optimize social utility and, accordingly, to improve safety in the presence of human-driven vehicles (HVs) with varying behaviors. In </a><a href="#bookmark101" class="a">[33]</a><a href="#bookmark102" class="s7">, a DRL-based decision- making approach was proposed for AVs at unsignalized intersections, in which the Twin Delayed Deep  Determin- istic Policy Gradient (TD3) method with an ego-attention mechanism was used to enable AVs to interact with social vehicles and make efficient right-turn decisions. Also, in </a><a href="#bookmark102" class="a">[34]</a>, a prediction-aware and RL-based altruistic, cooperative driving framework for AVs in mixed-autonomy traffic was studied. A Hybrid Predictive Network (HPN) and a value function network (VFN) were used to anticipate future states and to optimize for social utility, respectively.<a name="bookmark31">&zwnj;</a><a name="bookmark32">&zwnj;</a><a name="bookmark33">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">However, existing social and behavioral-aware navigation techniques often fail to dynamically adapt to real-time inter- actions and effectively balance individual objectives with collective safety. Our BSI framework addresses this by inte- grating dynamic Social Value Orientation (SVO) with RL. The proposed BSI framework in our paper includes a com- prehensive analysis of the interactions between AVs and their surrounding environment. It focuses on human behavior, social dynamics, and contextual factors to identify and assess poten- tial hazards. By incorporating this BSI algorithm into RL, AVs can enhance their decision-making processes, ensuring that they act safely, ethically, and socially compliant by adhering to identified risks and constraints. Additionally, our model includes an advanced SVO framework to assess individuals’ preferences for balancing personal well-being with the welfare of others. These preferences are categorized as altruistic and egocentric. By combining the SVO architecture and learning- based model-free RL, our goal is to enhance all road users’</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">safety and comfort and prioritize AVs’ operational objectives. This integration helps create trustworthy and human-like nav- igation for AVs, ultimately improving autonomous driving technologies’ overall reliability and acceptance. Hence, this study is conducted based on the following considerations and objectives:</p><ol id="l3"><li style="padding-top: 6pt;padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">Our proposed architecture prioritizes promoting trust and acceptance among road users in shared spaces and multi- agent environments. It ensures that potential risks and constraints are considered during decision-making and autonomous navigation for the ego vehicle.</p></li><li style="padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">This architecture attempts to effectively interpret and react to social and behavioral cues from other road users. The existing navigation structures usually need more adaptability to deal effectively with unpredictable or unexpected circumstances. Our presented architecture enables AVs to navigate intuitively and reliably by considering other traffic participants’ social cues and actions.</p></li></ol><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">A primary contribution of this study lies in integrating an advanced social intelligence scheme in a MURL environment, thereby refining the reward mechanisms of the Q-learning scheme to promote more human-like and ethically logical vehicle behaviors. Hence, a reward structure is proposed that discourages actions detrimental to other traffic participants while incentivizing those that enhance the safety of other agents in the shared spaces. This reward system ensures AVs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 10pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="323" height="170" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_006.jpg"/></span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark103" class="s8" name="bookmark34">Fig. 1. Urban Traffic Scenario with AV and Multiple Pedestrians and Cyclists, Highlighting Interaction Points for Behavioral Analysis </a><a href="#bookmark103" class="s10">[35]</a><a href="#bookmark103" class="s8">.</a><a name="bookmark35">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 85pt;text-indent: -15pt;text-align: left;"><p style="display: inline;"><a name="bookmark36">P</a><span class="s5">ROBLEM </span>F<span class="s5">ORMULATION</span></p><ol id="l4"><li style="padding-top: 4pt;padding-left: 18pt;text-indent: -12pt;text-align: justify;"><p class="s11" style="display: inline;">State Representation</p><p class="s9" style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark34" class="s7">In our multi-agent system, we have  a  number  of  cars and pedestrians interacting in a complex and shared urban environment. Fig. </a>1 <span style=" color: #000;">shows an urban traffic scenario involving an ego car, multiple pedestrians, and a cyclist, illustrating the critical interaction points that are essential for analyzing the behavioral dynamics in the proposed reinforcement learning</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 83%;text-align: justify;">framework. Each car is denoted as <i>C</i><span class="s12">i</span><span class="s13"> </span>(for <i>i </i><span class="s14">= </span>1<span class="s15">, </span>2<span class="s15">, . . . , </span><i>n</i>) and each pedestrian as <i>P</i><span class="s12">j</span><span class="s13"> </span>(for <i>j </i><span class="s14">= </span>1<span class="s15">, </span>2<span class="s15">, . . . , </span><i>m </i>). The state of the system at any given time <i>t </i>is described by a vector <i><b>x</b></i><span class="s15">(</span><i>t</i><span class="s15">) </span>:</p><p class="s15" style="padding-top: 8pt;padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s16">x</span>(<span class="s11">t</span>)</p><p class="s13" style="text-indent: 0pt;line-height: 7pt;text-align: left;">T</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-left: 13pt;text-indent: 0pt;line-height: 20pt;text-align: left;"><span class="s14">= </span><span class="s17">r</span><span class="s16">x</span><span class="s12">C</span><span class="s18">1</span><span class="s19"> </span>(<span class="s11">t</span>), <span class="s16">x</span><span class="s12">C</span><span class="s18">2</span><span class="s19"> </span>(<span class="s11">t</span>), . . . , <span class="s16">x</span><span class="s12">C</span><span class="s20">n</span><span class="s21"> </span>(<span class="s11">t</span>), <span class="s16">x </span><span class="s12">P</span><span class="s18">1 </span>(<span class="s11">t</span>), <span class="s16">x </span><span class="s12">P</span><span class="s18">2</span><span class="s19"> </span>(<span class="s11">t</span>), . . . , <span class="s16">x </span><span class="s12">P</span><span class="s20">m</span><span class="s21"> </span>(<span class="s11">t</span>)<span class="s17">l</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">navigate complex traffic efficiently and responsibly according</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">to human logic and intuitions. Given the above motivations, the present study makes these contributions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 5pt;text-indent: 0pt;line-height: 22pt;text-align: left;"><span class="p">where </span><span class="s16">x</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">= </span><span class="s17">r</span><span class="s22"> </span><span class="s16">p</span><span class="s23">C</span><span class="s24">i</span><span class="s21"> </span>(<span class="s11">t</span>), <span class="s25">v</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>)<span class="s17">l</span></p><p style="padding-top: 1pt;text-indent: 0pt;text-align: right;">(1)</p><p class="s26" style="padding-top: 6pt;text-indent: 0pt;text-align: right;">T  <span class="s13"> </span><span class="p">represents the state of car</span></p><ol id="l5"><li style="padding-top: 3pt;padding-left: 30pt;text-indent: -14pt;line-height: 7pt;text-align: left;"><p style="display: inline;">In this study, a new hybrid socially and behaviorally</p><p class="s15" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s11">i </span><span class="p">, and  </span><span class="s16">p</span><span class="s23">C</span><span class="s24">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="p">and </span><span class="s25">v</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="p">are its position and velocity vectors,</span></p><p style="padding-top: 5pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">intelligent model-free RL scheme is presented, which</p><p class="s15" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"><span class="p">respectively. Similarly, </span><span class="s16">x </span><span class="s12">P</span><span class="s20">j </span>(<span class="s11">t</span>) <span class="s14">=</span></p><p class="s21" style="text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s13">P</span><span class="s27">j                 </span> <span class="s28">P</span>j       <span class="s29">)</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-left: 7pt;text-indent: 0pt;line-height: 17pt;text-align: left;"><span class="s16">p   </span>(<span class="s11">t</span>), <span class="s25">v   </span>(<span class="s11">t   </span><span class="s30">T</span></p><p style="padding-top: 4pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">represents</p><p style="padding-left: 30pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">considers the interplay dynamics among the AVs and</p><p style="padding-left: 30pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">other agents in complex traffic by combining the SARSA scheme to discover the best policy for the AV. In addi- tion,  we  also  design  and  include  a  risk  cost  as  an</p><p style="padding-top: 1pt;padding-left: 9pt;text-indent: 0pt;text-align: justify;">the state of pedestrian <i>j </i>. The dynamics of each car and pedestrian are expressed through their positional and velocity changes over time<span class="s14">:</span></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;">indicator of the likelihood of collisions.</p></li><li style="padding-left: 30pt;text-indent: -14pt;line-height: 1pt;text-align: left;"><p style="display: inline;">A new and modified social psychology model according</p><p class="s15" style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s16">p</span><span class="s23">C</span><span class="s13"> </span>(<span class="s11">t </span><span class="s14">+ </span><span class="p">1</span>) <span class="s14">= </span><span class="s16">p</span><span class="s23">C</span><span class="s13"> </span>(<span class="s11">t</span>) <span class="s14">+ </span><span class="s25">v</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>)f:,<span class="s11">t </span><span class="s14">+</span></p><p class="s31" style="padding-left: 1pt;text-indent: 0pt;line-height: 6pt;text-align: left;">1              <span class="s32">2</span></p><p class="s15" style="padding-left: 1pt;text-indent: 0pt;line-height: 6pt;text-align: left;"><span class="s33">2</span><span class="p"> </span><span class="s16">a</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>)f:,<span class="s11">t</span></p><p class="s21" style="padding-left: 311pt;text-indent: 0pt;line-height: 7pt;text-align: left;">i                                           i</p><p style="padding-left: 30pt;text-indent: 0pt;line-height: 12pt;text-align: left;">to the principle of value orientation is presented. This architecture continuously quantifies various cues based</p><p class="s15" style="padding-left: 30pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s25">v</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t </span><span class="s14">+ </span><span class="p">1</span>) <span class="s14">= </span><span class="s25">v</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">+ </span><span class="s16">a</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>)f:,<span class="s11">t</span></p><p style="text-indent: 0pt;line-height: 8pt;text-align: right;">1</p><p class="s15" style="padding-left: 30pt;text-indent: 0pt;line-height: 0pt;text-align: left;"><span class="s16">p</span><span class="s23">P</span><span class="s13"> </span>(<span class="s11">t </span><span class="s14">+ </span><span class="p">1</span>) <span class="s14">= </span><span class="s16">p</span><span class="s23">P</span><span class="s13"> </span>(<span class="s11">t</span>) <span class="s14">+ </span><span class="s25">v </span><span class="s12">P                            </span><span class="s13"> P              </span><span class="s34">2</span></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;">on  the  system’s  states  and  outputs  a  value  of  social                     <span class="s35">j</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="1" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_007.png"/></span></p><p class="s21" style="padding-left: 30pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s24">j                           </span> <span class="s20">j</span> <span class="s15">(</span><span class="s11">t</span><span class="s15">)f:,</span><span class="s11">t </span><span class="s14">+ </span><span class="s33">2</span><span class="p"> </span><span class="s16">a</span></p><p class="s15" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><span class="s20">j</span><span class="s21"> </span>(<span class="s11">t</span>)f:,<span class="s11">t</span></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;">preferences ranging from altruistic to egocentric to the agents in the shared traffic space.</p></li><li style="padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">Our presented architecture employs a dynamic optimiza- tion method to iteratively refine the learning rate and exploration strategies of the SARSA algorithm. This advanced policy derivation process ensures the conver- gence to an optimal strategy that balances all agents’ safety, efficiency, and social preferences.</p><p class="s9" style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark36" class="s7">The rest of this paper is structured as follows: Section </a>II <a href="#bookmark46" class="s7">outlines the problem formulation, Section </a>III <a href="#bookmark49" class="s7">describes the integration of the SARSA Algorithm with SVO, Section </a>IV <a href="#bookmark50" class="s7">details the Optimal Policy Derivation framework, Section </a>V <a href="#bookmark53" class="s7">explains the Dynamic SVO in the multi-agent complex envi- ronment, Section </a>VI <a href="#bookmark68" class="s7">presents and analyzes the results, and Section </a>VII <span style=" color: #000;">provides the conclusions.</span></p><p class="s15" style="padding-left: 5pt;text-indent: 31pt;line-height: 14pt;text-align: left;"><a name="bookmark37"><span class="s25">v </span></a><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">t </span><span class="s14">+ </span><span class="p">1</span>) <span class="s14">= </span><span class="s25">v </span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">+ </span><span class="s16">a </span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">t</span>)f:,<span class="s11">t                                </span><span class="p">(2)</span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where <i><b>a</b></i><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span><span class="s15">(</span><i>t</i><span class="s15">) </span>and <i><b>a </b></i><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span><span class="s15">(</span><i>t</i><span class="s15">) </span>represent the  acceleration  vectors for car <i>i </i>and pedestrian <i>j </i>at time <i>t </i>, respectively, and <span class="s15">f:,</span><i>t </i>represents the time step between states. Additionally, the position vectors of car <i>i </i>and pedestrian <i>j </i>at time <i>t </i>are typically in a two-dimensional plane representing a map or urban area, and the velocity vectors of car <i>i </i>and pedestrian <i>j </i>indicate their speed and direction of movement. These vectors can be influenced by factors like the agent’s decision-making, road conditions, and interactions with other agents. The accelera- tions of cars and pedestrians are influenced by their control</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">inputs and external factors, modeled as disturbances as<span class="s14">:</span></p><p class="s15" style="padding-top: 4pt;padding-left: 79pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><span class="s16">a</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">= </span><span class="s16">u</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">+ </span><span class="s25">w</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>),</p><p class="s15" style="padding-left: 79pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a name="bookmark38"><span class="s16">a </span></a><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">= </span><span class="s16">u</span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">+ </span><span class="s25">w </span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">t</span>),                      <span class="p">(3)</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="333" height="183" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_008.jpg"/></span></p><p class="s22" style="text-indent: 0pt;line-height: 16pt;text-align: right;"><a name="bookmark39">I        </a><span class="s36">1                </span><span class="p"> </span>\</p><p style="text-indent: 0pt;text-align: left;"><span><img width="148" height="1" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_009.png"/></span></p><p class="s15" style="text-indent: 0pt;line-height: 14pt;text-align: right;"><span class="s37">× </span><span class="s14"> </span><span class="p">1 </span><span class="s14">+ </span><span class="p">exp</span><span class="s17">(</span><span class="s22"> </span><span class="h2">v</span><span class="s12">C</span><span class="s13"> </span>(<span class="s11">t</span>) <span class="s14">− </span><span class="h2">v</span><span class="s12">P</span><span class="s13"> </span>(<span class="s11">t</span>) <span class="s17">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;"><a name="bookmark40">(4)</a></p><p class="s21" style="text-indent: 0pt;line-height: 5pt;text-align: right;">i                           j</p><p style="padding-top: 9pt;text-indent: 0pt;line-height: 8pt;text-align: right;">where    <i><b>p</b></i></p><p class="s22" style="text-indent: 0pt;line-height: 5pt;text-align: right;">   <span class="s23">C</span><span class="s24">i</span></p><p class="s15" style="padding-top: 7pt;text-indent: 0pt;line-height: 15pt;text-align: left;">(<span class="s11">t</span>) <span class="s14">− </span><span class="s16">p</span><span class="s23">P</span><span class="s24">j</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="text-indent: 0pt;line-height: 6pt;text-align: left;">(<span class="s11">t</span>)   <span class="p">is the Euclidean distance between</span></p><p class="s22" style="padding-left: 10pt;text-indent: 0pt;line-height: 6pt;text-align: left;"> </p><p class="s11" style="padding-left: 268pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="p">car </span>i  <span class="p">and pedestrian  </span>j <span class="p">, and   </span><span class="s25">v</span><span class="s12">C</span><span class="s13"> </span><span class="s15">(</span>t<span class="s15">) </span><span class="s14">− </span><span class="s25">v </span><span class="s12">P</span><span class="s13"> </span><span class="s15">(</span>t<span class="s15">)   </span><span class="p">is the relative</span></p><p class="s21" style="text-indent: 0pt;line-height: 5pt;text-align: right;">i                            j</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><span class="s5">Fig. 2. Illustration of the multi-agent scenario in the SVO-reshaped SARSA environment, which depicts the interaction between two ego cars and two pedestrians. Each agent (car or pedestrian) is characterized by its position </span>( <span class="s6">p</span>)<span class="s5">, velocity </span>(v)<span class="s5">, and acceleration (a). The predefined path of ego car 1 indicates</span></p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><span class="s5">the decision-making process influenced by the actions </span>A <span class="s39">= {</span>a<span class="s40">1</span><span class="s38">, </span>a<span class="s40">2</span><span class="s38">, </span>a<span class="s40">3</span><span class="s39">}</span><span class="s5">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">where <i><b>u</b></i><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span><span class="s15">(</span><i>t</i><span class="s15">) </span>and <i><b>u</b></i><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span><span class="s15">(</span><i>t</i><span class="s15">) </span>represent the control inputs for car <i>i </i>and pedestrian <i>j </i>at time <i>t </i>, respectively. These inputs are decisions the agents (or the driver/pedestrian in a real-world scenario) make based on their current state and objectives. Additionally, the terms <span class="s25">w</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span><span class="s15">(</span><i>t</i><span class="s15">) </span>and <span class="s25">w </span><span class="s12">P</span><span class="s20">j </span><span class="s15">(</span><i>t</i><span class="s15">) </span>are disturbance vectors for car <i>i </i>and pedestrian <i>j </i>, accounting for unexpected motion influences. These disturbances can include environ- mental conditions (e.g., slippery roads, wind), unmodeled dynamics, or random fluctuations in the agent’s behavior.</p><p class="s15" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark39" class="s7">Fig. </a><span class="s9">2 </span><span class="p">illustrates the MURL in the SVO-reshaped SARSA environment and shows the interaction between two ego cars and two pedestrians. Each agent (car or pedestrian) is charac- terized by its position </span>( <span class="s11">p</span>)<span class="p">, velocity </span>(v)<span class="p">, and acceleration </span>(<span class="s11">a</span>)<span class="p">. The predefined path of ego car 1 indicates the decision-making</span></p><p class="s11" style="padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><span class="p">process influenced by the actions </span>A <span class="s14">= {</span>a<span class="s41">1</span><span class="s15">, </span>a<span class="s41">2</span><span class="s15">, </span>a<span class="s41">3</span><span class="s14">}</span><span class="p">, where </span>a<span class="s41">1</span><span class="p">,</span></p><p class="s11" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">a<span class="s41">2</span><span class="p">, and </span>a<span class="s41">3</span><span class="s34"> </span><span class="p">represent acceleration, deceleration, and maintain speed  based  on  if  the  path  is  clear  if  needed  to  yield  to</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">pedestrians or avoid collisions, and to keep the agent’s velocity constant in stable traffic conditions.</p><p class="s15" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark37" class="s7">These actions directly influence the agent’s state transitions as described by Eqs. </a><span class="s9">2 </span><a href="#bookmark38" class="s7">and </a><span class="s9">3</span><span class="p">. For example, selecting Action </span><span class="s11">a</span><span class="s41">1 </span><span class="s34"> </span><span class="p">results in an increase in velocity (</span><span class="s25">v</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t </span><span class="s14">+ </span><span class="p">1</span>) <span class="s14">= </span><span class="s25">v</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">+</span></p><p class="s15" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><span class="s16">a</span><span class="s12">C</span><span class="s20">i </span>(<span class="s11">t</span>)f:,<span class="s11">t </span><span class="p">), affecting the agent’s position and interactions with</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark39" class="s7">other agents. Fig. </a><span style=" color: #00F;">2 </span>illustrates how these actions guide ego cars’ paths and interactions with pedestrians. For instance, Action <i>a</i><span class="s41">2</span><span class="s34"> </span>(Decelerate) allows an AV to yield to a crossing pedestrian, enhancing safety, while Action <i>a</i><span class="s41">1</span><span class="s34"> </span>(Accelerate) enables the AV to proceed when clear to optimize traffic flow.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-top: 6pt;padding-left: 18pt;text-indent: -12pt;text-align: justify;"><p class="s11" style="display: inline;">Logistic Regression-Based Collision Risk Assessment</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark105" class="s7" name="bookmark41">The logistic regression model offers a direct and computa- tionally efficient method for estimating collision probabilities in our MURL architecture. Unlike the  Bayesian approach, which requires prior knowledge and Bayesian updating, this logistic regression can provide a fast estimation based on observable features, such as distances and velocities </a><a href="#bookmark105" class="a">[37]</a>. As a result, this approach can be suitable for real-time applications. The collision risk model <i>P</i><span class="s12">c</span><span class="s42">,</span><span class="s13">i j  </span>between cars <i>i </i>and pedestrians</p><p class="s11" style="padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">j <span class="p">in the MURL can be defined as</span><span class="s14">:</span></p><p class="s22" style="padding-left: 99pt;text-indent: 0pt;line-height: 12pt;text-align: left;">(                   <span class="s43">2</span> </p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">velocity that indicates how fast the distance between the enti-</p><p class="s9" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark40" class="s7">ties is closing. The coefficients in Eq. </a>4 <a href="#bookmark40" class="s7">are determined through logistic regression using this historical interaction data, which enables the model to accurately reflect the influence of distance and relative velocity on collision probability. Additionally, the logistic regression model in Eq. </a>4 <span style=" color: #000;">computes collision risk by leveraging both distance and relative velocity as key indicators of potential collision. The exponential decay based on distance reflects how quickly collision risk decreases as the car and pedestrian move further apart, while the logistic function on velocity captures how faster relative speeds increase collision likelihood. This combination provides a well-rounded measure of risk.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark105" class="s7">Logistic regression has several advantages over the Bayesian approach </a><a href="#bookmark105" class="a">[37]</a>. Firstly, it is straightforward to implement and computationally less demanding, making it ideal for systems that require real-time decision-making. Secondly, it does not require extensive prior data or a complex process of updating beliefs based on new evidence, making it less data-dependent. Lastly, the model coefficients directly reflect the influence of each feature on the collision probability by providing direct interpretability.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s11" style="display: inline;">Experience-Driven SVO Dynamics</p></li></ol><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">In the MURL context of urban traffic systems, the SVO for car <i>i </i>and pedestrian <i>j </i>are defined as <i>SV O</i><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>and <i>SV O</i><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>, respectively. These are modeled as continuous variables in the <span class="s14">[−</span>1<span class="s15">, </span>1<span class="s14">] </span>range, where <span class="s14">−</span>1 indicates completely competitive and</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">1 indicates fully cooperative behavior. SVO is modeled as a</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">continuous spectrum where each agent’s preference for coop- erative or competitive behavior is quantified. The SVO range is limited to [-1,1] to ensure that all agents’ behaviors lie on a spectrum from fully competitive to fully cooperative, which provides a controlled and interpretable behavior adjustment mechanism. This range allows for clear transitions between different social behaviors, where -1 represents selfish behavior, and 1 indicates altruistic behavior. For cars <i>C</i><span class="s12">i</span><span class="s13"> </span>, <i>SV O</i><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>might be influenced by the programmed driving style (e.g., cautious, aggressive), and for pedestrians <i>P</i><span class="s12">j</span><span class="s13"> </span>, <i>SV O</i><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>could reflect their current behavior, like hurried crossing or attentive walking. The SVO of each agent can dynamically change based on their experiences. For car <i>i </i>:</p><p class="s15" style="padding-top: 4pt;padding-left: 45pt;text-indent: 0pt;text-align: left;"><span class="p">SVO</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t </span><span class="s14">+ </span><span class="p">1</span>) <span class="s14">= </span><span class="p">SVO</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">+ </span>ηf:,<span class="p">SVO</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>)          <span class="p">(5)</span></p><p class="s15" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><span class="p">where </span>η <span class="p">is a learning rate and </span>f:,<span class="p">SVO</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="p">is the change in SVO based on interactions and outcomes at time </span><span class="s11">t </span><span class="p">. </span>f:,<span class="p">SVO</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="p">is calculated by evaluating the difference in utility gained from cooperative versus competitive actions during interactions. This adjustment allows the agent to adapt its social preferences dynamically based on the effectiveness and outcomes of its previous decisions.</span></p><p class="s11" style="padding-left: 9pt;text-indent: 0pt;text-align: center;">Remark 1: <span class="p">Dynamic SVO allows agents to adapt their social</span></p><p class="s11" style="padding-left: 43pt;text-indent: 0pt;line-height: 12pt;text-align: left;">P<span class="s12">c</span><span class="s42">,</span><span class="s13">i j </span><span class="s15">(</span>t<span class="s15">) </span><span class="s14">= </span><span class="p">exp</span></p><p class="s15" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s14">− </span><span class="h2">p</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">− </span><span class="h2">p</span><span class="s12">P</span><span class="s20">j </span>(<span class="s11">t</span>)<span class="s44"> </span></p><p style="padding-left: 43pt;text-indent: 0pt;line-height: 11pt;text-align: left;">preferences based on real-time experiences, which is crucial</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">for achieving optimal performance in complex and dynamic environments. This adaptability enables agents to become more cooperative or competitive depending on the observed outcomes of their interactions. Additionally, dynamic SVOs enhance social learning, allowing agents to learn from their interactions with others and adjust their behavior to improve collective outcomes. This  social  learning  process  can  lead to emergent cooperative behaviors that static SVOs cannot</p><p class="s11" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><span class="p">where the differences in SVO values between the interacting agents and their interaction potential are considered, and there- fore, both behavioral and positional aspects are incorporated into the risk assessment. Furthermore, interaction potential function </span>V<span class="s12">i</span><span class="s13"> j </span><span class="s15">(</span>t<span class="s15">) </span><span class="p">models the influence between car </span>i  <span class="p">and pedestrian </span>j <span class="p">:</span></p><p class="s15" style="padding-left: 104pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><span class="s45">    </span><span class="s22"> </span><span class="s16">p  </span>(<span class="s11">t</span>)    <span class="s16">p   </span>(<span class="s11">t   </span><span class="s46">2</span><span class="s34"> </span><span class="s45"></span></p><p class="s23" style="padding-left: 133pt;text-indent: 0pt;line-height: 3pt;text-align: left;">C<span class="s24">i</span><span class="s21">           </span><span class="s14">−   </span>P<span class="s24">j</span><span class="s21">       </span><span class="s15">)</span><span class="s47"> </span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">achieve.</p><p class="s11" style="padding-left: 5pt;text-indent: 9pt;text-align: left;"><a name="bookmark42">Remark 2: </a><span class="p">The concept of dynamic SVOs reflects the idea of agents evolving their strategies over time to maximize their</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="104" height="1" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_010.png"/></span></p><p class="s22" style="text-indent: 0pt;line-height: 4pt;text-align: left;"></p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">V<span class="s12">i</span><span class="s13"> j </span><span class="s15">(</span>t<span class="s15">) </span><span class="s14">= </span><span class="p">exp</span><span class="s47"></span><span class="s14">−</span><span class="s48"> </span></p><p class="s22" style="padding-top: 11pt;text-indent: 0pt;text-align: right;"> </p><p class="s22" style="text-indent: 0pt;line-height: 4pt;text-align: left;"></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 5pt;text-indent: 0pt;line-height: 16pt;text-align: left;">2<span class="s15">σ </span><span class="s49">2                </span><span class="s34"> </span><span class="s45"></span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">(9)</p><p class="s15" style="text-indent: 0pt;line-height: 1pt;text-align: right;"><span class="s22">  </span><span class="s16">p   </span>(<span class="s11">t</span>) <span class="s14">− </span><span class="s16">p</span><span class="s23">P</span><span class="s13"> </span>(<span class="s11">t</span>)<span class="s50"> </span></p><p style="padding-left: 4pt;text-indent: 0pt;line-height: 1pt;text-align: left;">measures  the  Euclidean  distance</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 0pt;text-align: justify;"><a href="#bookmark106" class="s7">utility, similar to evolutionary processes </a><a href="#bookmark106" class="a">[38]</a>. This approach</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark107" class="s7" name="bookmark43">could help identify new, more efficient strategies for managing traffic and avoiding collisions. Research in social psychol- ogy and behavioral economics suggests that human social preferences are not fixed but evolve through interactions and feedback </a><a href="#bookmark107" class="a">[39]</a>. Furthermore, dynamic SVOs improve robust- ness and flexibility, allowing agents to handle varying traffic conditions and unforeseen events better. By continuously updating their SVOs, agents can respond more effectively to unexpected scenarios, such as sudden pedestrian crossings.</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 0pt;text-align: justify;">where       <span class="s23">C</span><span class="s24">i</span><span class="s21">                             j</span></p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">between the ego car and the other agents, indicating their prox- imity. The parameter <span class="s15">σ </span>scales the influence of this distance on the interaction potential, with a larger <span class="s15">σ </span>leading to a more gradual change in the potential over distance.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">E. Modified Utility Functions Under SVO</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;line-height: 89%;text-align: left;">The utility functions in the decision-making process can be modeled as<span class="s14">:</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: left;"><i>Remark 3: </i>The mathematical modeling of dynamic SVOs involves the learning rate (<span class="s15">η</span>) and the change in SVO (<span class="s15">f:,</span>SVO).</p><p class="s15" style="padding-left: 5pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><span class="s11">U</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">x</span>) <span class="s14">= </span>w<span class="s41">1</span><span class="s13">i </span><span class="s25">v</span><span class="s12">C</span><span class="s20">i</span><span class="s21">  </span><span class="s14">+ </span>w<span class="s41">2</span><span class="s13">i </span><span class="s16">a</span><span class="s12">C</span><span class="s20">i</span><span class="s21">  </span><span class="s14">− </span>w<span class="s41">3</span><span class="s13">i</span></p><p class="s11" style="text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s51">�</span><span class="s22"> </span>R <span class="s12">f</span><span class="s13"> </span>P<span class="s12">j</span><span class="s13"> </span><span class="s15">(</span>x<span class="s15">),</span></p><p class="s13" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">j</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">The learning rate controls the speed of adaptation, where a higher learning rate leads to faster adaptation but may cause</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">instability, while a lower learning rate ensures stability but</p><p class="s15" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><span class="s11">U</span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">x</span>) <span class="s14">= </span>w<span class="s41">4</span><span class="s34"> </span><span class="s13">j</span></p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">v <span class="s12">P</span><span class="s20">j </span><span class="s21"> </span><span class="s14">+ </span><span class="s15">w</span><span class="s41">5</span><span class="s34"> </span><span class="s13">j</span></p><p class="s22" style="text-indent: 0pt;line-height: 4pt;text-align: left;">�</p><p style="text-indent: 0pt;text-align: left;"/><p class="s16" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">a <span class="s12">P</span><span class="s20">j </span><span class="s21"> </span><span class="s14">− </span><span class="s15">w</span><span class="s41">6</span><span class="s34"> </span><span class="s13">j</span></p><p class="s13" style="text-indent: 0pt;text-align: right;">i</p><p class="s11" style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><a name="bookmark44">R </a><span class="s12">f</span><span class="s13"> </span>C<span class="s12">i</span><span class="s13"> </span><span class="s15">(</span>x<span class="s15">).       </span><span class="p">(10)</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">slower adaptation. The change in SVO captures the influence of recent interactions on the agent’s social preferences and can be modeled based on rewards, penalties, and observed behaviors of other agents. The goal is to balance individual utility and collective welfare, fostering a harmonious and effi- cient traffic environment. Dynamic SVOs provide a promising framework for achieving this balance by considering uncer- tainties in the representation of social cues for other agents in the environment.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">D. Enhanced Risk Functions and SVO Formulations</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">where the velocity and acceleration components <span class="s15">(</span><span class="s25">v  </span>and  <i><b>a</b></i>)</p><p class="s13" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark37" class="s7">reflect the dynamic state of the agents, according to Eqs. </a><span class="s9">2 </span><a href="#bookmark38" class="s7">and </a><span class="s9">3</span><span class="p">. The weighting factors </span><span class="s15">w</span><span class="s41">1</span>i <span class="s15">, w</span><span class="s41">2</span>i <span class="s15">, . . . , w</span><span class="s41">6</span><span class="s34"> </span>j <span class="p">are scaling parameters that balance the importance of velocity, accel- eration, and risk in the utility function for each agent. Additionally, the risk function components </span><span class="s11">R </span><span class="s12">f</span> C<span class="s20">i</span><span class="s21"> </span><span class="p">and </span><span class="s11">R </span><span class="s12">f</span> P<span class="s20">j</span><span class="s21"> </span><a href="#bookmark45" class="s7">for cars and pedestrians are based on Eq. </a><span class="s9">6 </span><span class="p">and represent the impact of each agent’s actions on others, adjusted by their respective SVOs. We modify the utility function for the ego</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">car to incorporate their SVO and risk functions directly<span class="s14">:</span></p><p class="s15" style="padding-left: 10pt;text-indent: 0pt;line-height: 20pt;text-align: justify;"><span class="s11">U</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span><span class="s17">(</span><span class="h2">x</span>, <span class="s11">SV O</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span><span class="s17">)</span><span class="s22"> </span><span class="s14">= </span>β<span class="s41">1</span><span class="s34"> </span><span class="s11">SV O</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span><span class="s11">R</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="h2">x</span>) <span class="s14">+ </span>β<span class="s41">2</span><span class="s17">(</span><span class="p">1 </span><span class="s14">− </span><span class="s11">SV O</span><span class="s12">C</span><span class="s20">i</span><span class="s21">   </span><span class="s17">)</span><span class="s22"> </span><span class="p">(11)</span></p><p style="padding-left: 5pt;text-indent: 9pt;line-height: 83%;text-align: left;">The risk function <i>R </i><span class="s12">f </span><span class="s13"> </span>is a composite measure that accounts for both physical and social risks<span class="s14">:</span></p><p class="s11" style="padding-top: 4pt;padding-left: 85pt;text-indent: 0pt;text-align: left;"><a name="bookmark45">R </a><span class="s12">f </span><span class="s13"> </span><span class="s14">= </span>R<span class="s41">SVO</span><span class="s42">,</span><span class="s13">i   j </span><span class="s15">(</span>t<span class="s15">) </span><span class="s14">+ </span><span class="s15">λ</span>S                           <span class="p">(6)</span></p><p class="s11" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><span class="p">where </span>R<span class="s41">SVO</span><span class="s42">,</span><span class="s13">i j </span><span class="s15">(</span>t<span class="s15">) </span><span class="p">represents the physical risk, calculated based on proximity to collision, speed, and other safety metrics. Furthermore, </span>S <span class="p">denotes the social risk, reflecting the potential negative impact of an agent’s actions on others, calculated</span></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">where <span class="s15">β</span><span class="s41">1 </span><span class="s34"> </span>and <span class="s15">β</span><span class="s41">2 </span><span class="s34"> </span>are the scaling factors obtained by trial and error.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 46pt;text-indent: -19pt;text-align: left;"><p style="display: inline;"><a name="bookmark46">O</a><span class="s5">PTIMAL </span>P<span class="s5">OLICY </span>D<span class="s5">ERIVATION </span>U<span class="s5">NDER </span>SVO</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">For obtaining the optimal policy <span class="s15">π </span><span class="s52">∗</span><span class="s53"> </span>using SARSA in a dynamic environment, we start with the definition of the value function under policy <span class="s15">π </span>:</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">from the SVOs of the involved agents, and <span class="s15">λ </span>is a weighting factor, determining the balance between physical and social risks. On this basis, the risk function, incorporating the SVO</p><p class="s15" style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><span class="s11">V </span><span class="s54">π</span><span class="s42"> </span>(<span class="s16">x</span>, <span class="s11">S V O</span>) <span class="s14">= </span><span class="s22">E</span><span class="s55">π</span></p><p class="s22" style="text-indent: 0pt;line-height: 6pt;text-align: center;">  <span class="s56">∞                                                 </span><span class="s53"> </span>l</p><p class="s15" style="padding-left: 5pt;text-indent: 0pt;line-height: 19pt;text-align: left;"><span class="s51">�</span><span class="s22"> </span>γ <span class="s57">k</span><span class="s13"> </span><span class="s11">U</span>(<span class="s16">x</span><span class="s12">k</span>, π(<span class="s16">x</span><span class="s12">k</span>), <span class="s11">S V O</span>) <span class="s14">| </span><span class="s16">x</span><span class="s41">0</span><span class="s34">  </span><span class="s14">= </span><span class="s16">x</span></p><p class="s13" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">k<span class="s53">=</span><span class="s34">0</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: left;">of agents, is given by<span class="s14">:</span></p><p class="s15" style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s11">R</span><span class="s41">SVO</span><span class="s42">,</span><span class="s13">i   j </span>(<span class="s11">t</span>) <span class="s14">= </span>λ<span class="s41">1</span><span class="s34"> </span><span class="s11">P</span><span class="s12">c</span><span class="s42">,</span><span class="s13">i j </span>(<span class="s11">t</span>) <span class="s14">+ </span>λ<span class="s41">2</span><span class="s34">  </span><span class="s11">SV O</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">− </span><span class="s11">SV O</span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">t</span>)<span class="s17"> </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">(7)</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;">(12)</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: right;">where <span class="s22">E</span><span class="s55">π </span><span class="s42"> </span>denotes the expectation with respect to policy <span class="s15">π </span>.</p><p class="s15" style="padding-left: 268pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="p">Additionally,  </span><span class="s11">U</span>(<span class="s16">x</span><span class="s12">k</span>, π(<span class="s16">x</span><span class="s12">k</span>), <span class="s11">S V O</span>)  <span class="p">is  the  utility  at  step  </span><span class="s11">k</span><span class="p">,</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">where <span class="s15">λ</span><span class="s41">1</span><span class="s34"> </span>and <span class="s15">λ</span><span class="s41">2</span><span class="s34"> </span>are weighting factors for collision probability</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">and SVO difference, respectively. Additionally, the social risk function can be further detailed based on the interaction potential and SVO differences<span class="s14">:</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark44" class="s7">based on Eq. </a><span style=" color: #00F;">10</span>, given state <i><b>x</b></i><span class="s12">k</span><span class="s13">  </span>and SVO. The optimal policy</p><p class="s15" style="padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: left;">π <span class="s52">∗</span><span class="s53"> </span><span class="p">maximizes this value function for all states</span><span class="s14">:</span></p><p class="s42" style="text-indent: 0pt;line-height: 7pt;text-align: left;">π</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-top: 2pt;padding-left: 53pt;text-indent: 0pt;text-align: left;"><a name="bookmark47">π </a><span class="s58">∗</span><span class="s53"> </span><span class="s14">= </span><span class="p">arg max </span><span class="s11">V </span><span class="s54">π</span><span class="s42"> </span>(<span class="s16">x</span>, <span class="s11">S V O</span>)   <span class="s14">∀</span><span class="s16">x </span><span class="s14">∈ </span>X          <span class="p">(13)</span></p><p class="s22" style="text-indent: 0pt;line-height: 4pt;text-align: left;"> </p><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="text-indent: 0pt;line-height: 5pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="padding-left: 64pt;text-indent: 0pt;line-height: 19pt;text-align: left;">S <span class="s14">= </span><span class="s51">�</span><span class="s22"> </span>SV O<span class="s12">C</span></p><p class="s13" style="text-indent: 0pt;text-align: right;">i<span class="s42">, </span>j</p><p class="s22" style="text-indent: 0pt;line-height: 4pt;text-align: left;"> </p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="padding-top: 4pt;padding-left: 1pt;text-indent: 0pt;text-align: left;"><span class="s14">— </span>SV O<span class="s12">P</span><span class="s20">j</span><span class="s21">   </span>V<span class="s12">i</span><span class="s13"> j </span><span class="s15">(</span>t<span class="s15">)                </span><span class="p">(8)</span></p><p style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"><a href="#bookmark47" class="s7">where Eq. </a><span style=" color: #00F;">13 </span>identifies the policy <span class="s15">π </span><span class="s52">∗</span><span class="s53"> </span>that yields the highest expected utility, considering the influence of SVO in each</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 89%;text-align: justify;">state. Subsequently, the Bellman equation for SARSA, incor- porating SVO, is given by<span class="s14">:</span></p><p class="s15" style="padding-left: 11pt;text-indent: 0pt;line-height: 21pt;text-align: left;"><span class="s11">Q</span>(<span class="s16">x</span>, <span class="s11">a</span>, <span class="s11">S V O</span>) <span class="s14">= </span><span class="s11">U</span>(<span class="s16">x</span>, <span class="s11">a</span>, <span class="s11">S V O</span>) <span class="s14">+ </span>γ <span class="s22">E</span><span class="s17">r</span><span class="s11">Q</span><span class="s17">(</span><span class="s16">x</span><span class="s58">′</span>, <span class="s11">a</span><span class="s58">′</span>, <span class="s11">S V O</span><span class="s17">)l</span></p><p style="padding-top: 1pt;text-indent: 0pt;text-align: right;">(14)</p><p class="s15" style="padding-top: 8pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><span class="p">where </span><span class="s11">Q</span>(<span class="s16">x</span>, <span class="s11">a</span>, <span class="s11">S V O</span>) <span class="p">is the action-value function, giving the expected utility of taking action </span><span class="s11">a </span><span class="p">in state </span><span class="s16">x </span><span class="p">and following policy </span>π <span class="p">thereafter. The convergence for SARSA with SVO can be assessed in the following steps:</span></p><p style="padding-left: 5pt;text-indent: 9pt;line-height: 89%;text-align: left;">The Q-value is updated using this the temporal-difference (TD) error<span class="s14">:</span></p><p class="s15" style="padding-top: 5pt;padding-left: 5pt;text-indent: 36pt;line-height: 131%;text-align: left;"><span class="s11">Q</span>(<span class="s16">x</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">S V O</span>) <span class="s14">← </span><span class="s11">Q</span>(<span class="s16">x</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">S V O</span>) <span class="s14">+ </span>αδ<span class="s12">t</span><span class="s13">            </span><span class="p">(15) Then, the TD error for SARSA is defined as</span><span class="s14">:</span></p><p class="s15" style="padding-left: 38pt;text-indent: 0pt;line-height: 15pt;text-align: left;">δ<span class="s12">t </span><span class="s13"> </span><span class="s14">= </span><span class="s11">U</span>(<span class="h2">x</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">SV O</span>) <span class="s14">+ </span>γ <span class="s11">Q</span>(<span class="h2">x</span><span class="s12">t</span><span class="s13"> </span><span class="s53">+</span><span class="s34">1</span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span><span class="s53">+</span><span class="s34">1</span>, <span class="s11">SV O</span>)</p><p class="s15" style="text-indent: 0pt;line-height: 15pt;text-align: right;"><span class="s14">— </span><span class="s11">Q</span>(<span class="h2">x</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">SV O</span>)                                      <span class="p">(16)</span></p><p class="s9" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark104" class="s7" name="bookmark48">Finally, this update rule converges to the optimal Q-values under standard learning rate decay and policy improvement conditions. The Robbins-Monro conditions for stochastic approximation </a>[36] <span style=" color: #000;">guarantee this convergence.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 39pt;text-indent: -18pt;text-align: left;"><p style="display: inline;"><a name="bookmark49">SARSA A</a><span class="s5">LGORITHM </span>W<span class="s5">ITH </span>SVO I<span class="s5">NTEGRATION</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;line-height: 88%;text-align: justify;">If policy improvement stops, then <span class="s15">π</span><span class="s41">new</span><span class="s34"> </span><span class="s14">= </span><span class="s15">π </span>. Hence, for all <i>a</i>:</p><p class="s15" style="padding-top: 6pt;padding-left: 84pt;text-indent: 0pt;text-align: left;"><span class="s11">Q</span>(<span class="s16">x</span>, π(<span class="s16">x</span>)) <span class="s14">= </span><span class="s11">Q</span>(<span class="s16">x</span>, <span class="s11">a</span>).                      <span class="p">(20)</span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">This implies <span class="s15">π </span>is an optimal policy since no other policy can achieve higher expected utility.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The policy improvement process guarantees monotonic improvement and converges to an optimal policy after  a finite number of iterations. In practice, soft policy improve- ment methods like epsilon-greedy are often used to handle exploration-exploitation trade-offs in continuous state spaces. This section provides a mathematical foundation for under- standing how SARSA refines the policy towards optimality and ensures convergence to the optimal policy in a dynamic optimization context.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 15pt;text-indent: 5pt;line-height: 135%;text-align: left;"><p style="display: inline;"><a name="bookmark50">D</a><span class="s5">YNAMIC </span>SVO <span class="s5">IN THE </span>M<span class="s5">ULTI</span>-A<span class="s5">GENT </span>C<span class="s5">ONTEXT  </span>The SARSA algorithm is applied to a multi-agent system</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: justify;">comprising cars and pedestrians. This approach is enhanced</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">by integrating SVO, reflecting each agent’s preference for collaboration or competition. In our urban environment model, each agent (car or pedestrian) at each time step <i>t </i>observes the current state <i>s</i><span class="s12">t</span><span class="s13"> </span>, takes action <i>a</i><span class="s12">t</span><span class="s13"> </span>, and receives a reward <i>r</i><span class="s12">t</span><span class="s13"> </span>. The</p><p class="s14" style="padding-left: 5pt;text-indent: 0pt;line-height: 77%;text-align: justify;"><span class="p">objective is to learn a policy </span><span class="s15">π </span>: <span class="s11">S </span>→ <span class="s11">A </span><span class="p">maximizing expected cumulative reward</span>:</p><p style="padding-left: 5pt;text-indent: 9pt;line-height: 8pt;text-align: left;">The SARSA learning algorithm, considering SVO, is for-</p><p class="s15" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 89%;text-align: justify;"><span class="p">malized as follows: First, we initialize </span><span class="s11">Q</span>(<span class="s16">x</span>, <span class="s11">a</span>, <span class="s11">SV O</span>) <span class="p">arbi- trarily for all </span><span class="s16">x </span><span class="s14">∈ </span>X , <span class="s11">a </span><span class="s14">∈ </span>A<span class="p">, and SVO values. Then, for each episode we:</span></p><p class="s11" style="padding-top: 10pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">J<span class="s15">(π) </span><span class="s14">= </span><span class="s22">E</span><span class="s55">π</span></p><p class="s22" style="text-indent: 0pt;line-height: 7pt;text-align: center;">  <span class="s56">∞      </span><span class="s53"> </span>l</p><p class="s51" style="padding-left: 5pt;text-indent: 0pt;line-height: 17pt;text-align: left;">�<span class="s22"> </span><i>γ </i><span class="s57">t</span><span class="s13"> </span><span class="s11">r</span><span class="s12">t</span></p><p class="s13" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">t <span class="s53">=</span><span class="s34">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">(21)</p><ol id="l6"><li style="padding-left: 5pt;text-indent: 9pt;line-height: 12pt;text-align: justify;"><p style="display: inline;">Initialize <i><b>x</b></i><span class="s41">0</span><span class="s34"> </span>and choose <i>a</i><span class="s41">0</span><span class="s34"> </span>from <i><b>x</b></i><span class="s41">0</span><span class="s34"> </span>using policy <span class="s15">π </span>derived from <i>Q </i>(e.g., <span class="s15">ϵ</span>-greedy).</p></li><li style="padding-left: 26pt;text-indent: -10pt;line-height: 14pt;text-align: left;"><p style="display: inline;">For each step of the episode<span class="s14">: </span>i.</p></li></ol><ol id="l7"><li style="padding-left: 30pt;text-indent: -14pt;line-height: 12pt;text-align: left;"><p style="display: inline;">Take action <i>a</i><span class="s12">t</span><span class="s13"> </span>, observe the reward <i>U </i>, and transition to the next state <i><b>x</b></i><span class="s12">t</span><span class="s13"> </span><span class="s53">+</span><span class="s34">1</span>.</p></li><li style="padding-left: 30pt;text-indent: -14pt;line-height: 12pt;text-align: left;"><p style="display: inline;">Choose the next action <i>a</i><span class="s12">t</span><span class="s13"> </span><span class="s53">+</span><span class="s34">1 </span>from <i><b>x</b></i><span class="s12">t</span><span class="s13"> </span><span class="s53">+</span><span class="s34">1 </span>using the current policy <span class="s15">π </span>.</p></li><li style="padding-left: 30pt;text-indent: -14pt;line-height: 12pt;text-align: left;"><p class="s15" style="display: inline;"><span class="p">Update the Q-value </span><span class="s11">Q</span>(<span class="s16">x</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">SV O</span>) <span class="p">using the SARSA update rule based on the observed transition.</span></p></li><li style="padding-top: 1pt;padding-left: 30pt;text-indent: -14pt;line-height: 89%;text-align: left;"><p class="s12" style="display: inline;"><span class="p">Set the current state and action to the next state and action: </span><span class="s16">x</span>t <span class="s13"> </span><span class="s14">← </span><span class="s16">x</span>t<span class="s13"> </span><span class="s53">+</span><span class="s34">1</span><span class="s14">; </span><span class="s11">a</span>t <span class="s13"> </span><span class="s14">← </span><span class="s11">a</span>t<span class="s13"> </span><span class="s53">+</span><span class="s34">1</span><span class="p">.</span></p></li></ol><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Then we perform policy improvement in SARSA iteratively with the policy <span class="s15">π </span>refined based on the updated action-value function <i>Q</i>. The policy <span class="s15">π </span>is updated using the action-values <i>Q </i>by selecting actions that maximize the expected utility<span class="s14">:</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: justify;">where  <span class="s15">γ  </span>is  the  discount  factor.  The  SARSA  algorithm</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 89%;text-align: justify;">updates Q-values, representing expected cumulative rewards for actions in given states, using the rule<span class="s14">:</span></p><p class="s15" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><span class="s11">Q</span>(<span class="s11">s</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span>) <span class="s14">← </span><span class="s11">Q</span>(<span class="s11">s</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span>) <span class="s14">+ </span>α(<span class="s11">r</span><span class="s12">t</span><span class="s13"> </span><span class="s14">+ </span>γ <span class="s11">Q</span>(<span class="s11">s</span><span class="s12">t</span><span class="s13"> </span><span class="s53">+</span><span class="s34">1</span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span><span class="s53">+</span><span class="s34">1</span>) <span class="s14">− </span><span class="s11">Q</span>(<span class="s11">s</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span>))</p><p style="padding-top: 1pt;text-indent: 0pt;text-align: right;">(22)</p><p class="s15" style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><span class="p">where </span>α <span class="p">is the learning rate. The algorithm utilizes an </span>ϵ<span class="p">-greedy policy for action selection. Then, SVO is integrated into SARSA to model agents’ cooperative or competitive preferences. We modify the reward function </span>R(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s52">′</span>) <span class="p">to include SVO terms by considering the utilities for both cars and pedestrians as follows</span><span class="s14">:</span></p><p class="s15" style="padding-top: 3pt;text-indent: 0pt;text-align: center;">R(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s58">′</span>) <span class="s14">= </span>θ<span class="s12">C</span><span class="s20">i </span><span class="s21"> </span><span class="s14">· </span>U<span class="s12">C</span><span class="s20">i </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s58">′</span>) <span class="s14">+ </span>(<span class="p">1 </span><span class="s14">− </span>θ<span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>) <span class="s14">· </span>U<span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s58">′</span>)</p><p style="padding-top: 1pt;text-indent: 0pt;text-align: right;">(23)</p><p class="s15" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><span class="p">where </span>U<span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s52">′</span>) <span class="p">and </span>U<span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s52">′</span>) <span class="p">are the utilities for car </span><span class="s11">i</span></p><p class="s15" style="padding-top: 8pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">π<span class="s41">new</span><span class="s34"> </span>(<span class="s11">a </span><span class="s14">| </span><span class="s16">x</span>) <span class="s14">=</span></p><p class="s14" style="text-indent: 0pt;line-height: 9pt;text-align: left;">=</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 11pt;text-align: center;"><span class="s17">r</span><span class="s22"> </span><span class="p">1    if </span>a    <span class="p">arg max </span>Q<span class="s15">(</span><b>x</b><span class="s15">, </span>a<span class="s58">′</span><span class="s15">, </span>SV O<span class="s15">),</span></p><p class="s23" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">a<span class="s59">′</span></p><p style="padding-left: 9pt;text-indent: 0pt;text-align: left;">0     otherwise.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">(17)</p><p style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">and pedestrian  <i>j </i>respectively. The SVO parameters <span class="s15">θ</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span><span class="s15">θ</span><span class="s12">P</span><span class="s20">j</span><span class="s21">   </span>balance the importance of each agent’s utility.</p><p style="padding-left: 19pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Furthermore, the utility functions are defined as<span class="s14">:</span></p><p style="padding-top: 5pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">and</p><p style="padding-top: 8pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">By definition, for all <i>a</i>:</p><p class="s15" style="padding-top: 7pt;padding-left: 15pt;text-indent: 63pt;text-align: left;"><span class="s11">Q</span>(<span class="s16">x</span>, π<span class="s41">new</span>(<span class="s16">x</span>)) <span class="s14">≥ </span><span class="s11">Q</span>(<span class="s16">x</span>, <span class="s11">a</span>).                    <span class="p">(18)</span></p><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;line-height: 89%;text-align: left;">Therefore, the expected utility of the new policy is greater than or equal to that of the old policy<span class="s14">:</span></p><p class="s15" style="padding-left: 39pt;text-indent: 0pt;line-height: 21pt;text-align: left;"><span class="s11">V </span><span class="s54">π</span><span class="s60">new</span><span class="s19"> </span>(<span class="s16">x</span>) <span class="s14">= </span><span class="s51">�</span><span class="s22"> </span>π<span class="s41">new</span>(<span class="s11">a </span><span class="s14">| </span><span class="s16">x</span>)<span class="s11">Q</span>(<span class="s16">x</span>, <span class="s11">a</span>) <span class="s14">≥ </span><span class="s11">V </span><span class="s54">π</span><span class="s42"> </span>(<span class="s16">x</span>).      <span class="p">(19)</span></p><p class="s13" style="text-indent: 0pt;text-align: center;">a</p><p class="s21" style="text-indent: 0pt;line-height: 5pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-left: 6pt;text-indent: 0pt;line-height: 16pt;text-align: left;">U<span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s58">′</span>) <span class="s14">= </span>α<span class="s41">1</span><span class="p">Safety</span><span class="s23">C</span><span class="s13"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s58">′</span>)</p><p class="s15" style="padding-left: 66pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span class="s14">+ </span>α<span class="s41">2</span><span class="p">Efficiency</span><span class="s23">C</span><span class="s24">i</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s </span>) <span class="s14">− </span>α<span class="s41">3</span><span class="p">Risk</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s </span>)</p><p class="s53" style="padding-left: 162pt;text-indent: 0pt;line-height: 5pt;text-align: left;">′                                       ′</p><p class="s21" style="text-indent: 0pt;line-height: 5pt;text-align: left;">j</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 16pt;text-align: left;">U<span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s58">′</span>) <span class="s14">= </span>β<span class="s41">1</span><span class="p">Safety</span><span class="s23">P</span><span class="s13"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s58">′</span>)</p><p class="s15" style="text-indent: 0pt;line-height: 16pt;text-align: right;"><span class="s14">+ </span>β<span class="s41">2</span><span class="p">Convenience</span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s58">′</span>)<span class="s14">−</span>β<span class="s41">3</span><span class="p">Risk</span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s58">′</span>)</p><p style="padding-top: 1pt;text-indent: 0pt;text-align: right;">(24)</p><p class="s15" style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;line-height: 12pt;text-align: right;">α<span class="s41">1</span><span class="p">,  </span>α<span class="s41">2</span><span class="p">,  </span>α<span class="s41">3   </span><span class="s34"> </span><span class="p">are  user-defined  weighting  factors  for  the car’s utility components and </span>β<span class="s41">1</span><span class="p">, </span>β<span class="s41">2</span><span class="p">, </span>β<span class="s41">3</span><span class="s34">  </span><span class="p">are scaling factors</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="325" height="376" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_011.png"/></span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">for the pedestrian’s utility. The selection of <span class="s15">α</span><span class="s12">i</span><span class="s13">  </span>values  is based on empirical  tuning  and  domain-specific  knowledge to reflect the priorities of the  AVs  in  navigation  strat- egy.  For  instance,  higher  values  of  <span class="s15">α</span><span class="s41">1  </span><span class="s34"> </span>emphasize  safety,</p><p class="s15" style="padding-left: 5pt;text-indent: 0pt;line-height: 84%;text-align: justify;"><span class="p">while higher values of </span>α<span class="s41">2</span><span class="s34"> </span><span class="p">and </span>α<span class="s41">3</span><span class="s34"> </span><span class="p">prioritize efficiency and risk minimization respectively. Furthermore, Safety</span><span class="s23">C</span><span class="s24">i</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s52">′</span>) <span class="p">and Safety</span><span class="s23">P</span><span class="s24">j</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s52">′</span>) <span class="p">represent the safety component of the utility for the car and pedestrian, respectively. Moreover, Efficiency</span><span class="s23">C</span><span class="s24">i</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s52">′</span>) <span class="p">represents the efficiency aspect of  the car, such as minimizing travel time or adhering to optimal routes.  Convenience</span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s52">′</span>) <span class="p">could  represent  factors  such</span></p><p class="s15" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><span class="p">as minimizing waiting time at crosswalks for pedestrians. Risk</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s52">′</span>) <span class="p">and Risk</span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">s</span>, <span class="s11">a</span>, <span class="s11">s</span><span class="s52">′</span>) <span class="p">quantify the risk or cost associated with a particular state-action transition for cars and pedestrians, such as the risk of an accident or violating traffic rules.</span></p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 9pt;line-height: 89%;text-align: left;">The Q-value function, considering the state-action pairs, becomes<span class="s14">:</span></p><p class="s15" style="padding-left: 15pt;text-indent: 38pt;line-height: 20pt;text-align: left;"><span class="s11">Q</span>(<span class="s11">s</span>, <span class="s11">a</span>) <span class="s14">= </span><span class="s22">E</span><span class="s17">r</span>R<span class="s12">t</span><span class="s13"> </span><span class="s53">+</span><span class="s34">1 </span><span class="s14">+ </span>γ <span class="s11">Q</span>(<span class="s11">s</span><span class="s58">′</span>, <span class="s11">a</span><span class="s58">′</span>) <span class="s14">| </span><span class="s11">s</span>, <span class="s11">a</span><span class="s17">l</span><span class="s22">           </span><span class="p">(25) Updated using</span><span class="s14">:</span></p><p class="s11" style="padding-top: 6pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Q<span class="s15">(</span>s<span class="s12">t</span><span class="s13"> </span><span class="s15">, </span>a<span class="s12">t</span><span class="s13"> </span><span class="s15">)</span></p><p class="s15" style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><span class="s14">← </span><span class="s11">Q</span>(<span class="s11">s</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span>) <span class="s14">+ </span>α(R<span class="s12">t</span><span class="s13"> </span><span class="s53">+</span><span class="s34">1 </span><span class="s14">+ </span>γ <span class="s11">Q</span>(<span class="s11">s</span><span class="s12">t</span><span class="s13"> </span><span class="s53">+</span><span class="s34">1</span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span><span class="s53">+</span><span class="s34">1</span>) <span class="s14">− </span><span class="s11">Q</span>(<span class="s11">s</span><span class="s12">t</span><span class="s13"> </span>, <span class="s11">a</span><span class="s12">t</span><span class="s13"> </span>))   <span class="p">(26)</span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;line-height: 89%;text-align: left;">The overall objective is to find the policy that maximizes the expected cumulative reward, defined as<span class="s14">:</span></p><p class="s53" style="padding-top: 2pt;padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: center;">∞</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark51">Fig. 3. The outline of an SVO-based SARSA-RL  framework  for  the socially intelligent interaction between ego vehicle and a dynamic multi-agent environment.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 102pt;text-indent: 0pt;line-height: 7pt;text-align: center;"><a name="bookmark52">TABLE I</a></p><p class="s14" style="padding-left: 83pt;text-indent: 0pt;line-height: 16pt;text-align: left;"><span class="s11">J</span><span class="s15">(π) </span>= <span class="s22">E</span><span class="s55">π</span><span class="s42"> </span>[<span class="s51">�</span><span class="s22"> </span><span class="s15">γ </span><span class="s57">t</span><span class="s13"> </span><span class="s11">R</span><span class="s12">t</span></p><p class="s13" style="text-indent: 0pt;line-height: 11pt;text-align: right;">t <span class="s53">=</span><span class="s34">0</span></p><p class="s61" style="text-indent: 0pt;text-align: left;">+<span class="s34">1</span><span class="s14">]                  </span><span class="p">(27)</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="266" height="4" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_012.png"/></span></p><p class="s5" style="padding-top: 4pt;padding-left: 46pt;text-indent: 0pt;text-align: left;">H<span class="s62">YPERPARAMETERS </span>U<span class="s62">SED IN </span>SVO-B<span class="s62">ASED </span>SARSA</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">where <span class="s15">π </span>represents the  policy,  mapping  states  to  actions. By employing the SARSA algorithm, the Q-function of a given policy <span class="s15">π </span><a href="#bookmark51" class="s7">can be assessed, and the action associated with the highest Q-value in each state is chosen to deter- mine the optimal policy. Fig. </a><span style=" color: #00F;">3 </span><a href="#bookmark54" class="s7">gives an overview of the presented SARSA-RL framework incorporating SVO. This architecture enables considering the interplay between AVs and other agents in the environment through SVO. Further- more, Algorithm </a><span style=" color: #00F;">1 </span>outlines the detailed steps of the framework using pseudo-code.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 83pt;text-indent: -19pt;text-align: left;"><p style="display: inline;"><a name="bookmark53">R</a><span class="s5">ESULTS AND </span>D<span class="s5">ISCUSSION</span></p><p class="s9" style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark52" class="s7">The earlier sections of the paper detailed a MURL decision-making model designed for dynamic urban traffic contexts, focusing on interactions between AVs and pedestri- ans at intersections. Central to our model is using spatial data, such as the distances between cars and pedestrians, combined with pedestrians’ SVO and vehicles’ collision risk assessments to facilitate effective decision-making in urban traffic settings. Table </a>I <span style=" color: #000;">lists the hyperparameters used in our SVO-integrated SARSA algorithm, tailored for the urban multi-agent envi- ronment. These parameters include separate  learning  rates for vehicles and pedestrians, a discount factor for evaluating future states, a weighting parameter for the SVO model, and diverse reward values for different actions. Additionally, the hyperparameters account for the range of pedestrian velocities, Gaussian noise standard deviation, simulation time horizon,</span></p><p style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span><img width="266" height="12" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_013.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="266" height="159" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_014.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">time step size, and the total number of simulations. Our simu- lation results validate the model’s capability to navigate urban traffic efficiently and reduce collision risks with pedestrians while optimizing vehicular travel efficiency. Comparative anal- ysis demonstrates our model’s enhanced ability to balance safety and efficiency.</p><p class="s9" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark56" class="s7">Fig. </a>4 <a href="#bookmark56" class="s7">shows the collision risk surface as a function of rel- ative velocity and distance between the ego vehicle and other objects. The horizontal axis represents the relative velocity, while the vertical axis shows the distance norm to the other objects. The vertical axis illustrates the collision risk, which is highest when the distance is zero and the relative velocity decreases (indicating approaching agents). Fig. </a>4 <span style=" color: #000;">demonstrates the critical  points  where  collision  risk  is  most  significant. It also illustrates how both speed and proximity impact safety.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="334" height="1" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_015.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="333" height="260" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_016.jpg"/></span></p><p class="s15" style="padding-left: 5pt;text-indent: -2pt;text-align: center;"><a name="bookmark54"><span class="h2">Algorithm 1 </span></a><span class="p">Multi-Agent System Interaction and Learning in </span><span class="s31">Urban Environments                                                                    </span><span class="p"> </span><span class="s5">1:  </span><span class="h2">Input: </span><span class="p">Number of cars </span><span class="s11">n</span><span class="p">, number of pedestrians </span><span class="s11">m</span><span class="p">, time step </span>f:,<span class="s11">t </span><span class="p">, parameters </span>α<span class="s41">1</span>, α<span class="s41">2</span>, α<span class="s41">3</span>, β<span class="s41">1</span>, β<span class="s41">2</span>, γ, λ, η, σ</p><p style="padding-left: 11pt;text-indent: 0pt;line-height: 11pt;text-align: center;"><span class="s5">2:  </span><b>Initialize: </b>State vectors <i><b>x</b></i><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span><span class="s15">(</span>0<span class="s15">) </span>and <i><b>x </b></i><span class="s12">P</span><span class="s20">j </span><span class="s15">(</span>0<span class="s15">)</span>, SVO values</p><p class="s11" style="padding-left: 38pt;text-indent: 0pt;line-height: 12pt;text-align: left;">SV O<span class="s12">C</span><span class="s20">i</span><span class="s21"> </span><span class="p">, </span>SV O<span class="s12">P</span><span class="s20">j</span><span class="s21"> </span><span class="p">, Q-values </span>Q<span class="s15">(</span><b>x</b><span class="s15">, </span>a<span class="s15">, </span>SV O<span class="s15">) </span><span class="p">for all </span>i <span class="s14">∈</span></p><p class="s14" style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">{<span class="p">1</span><span class="s15">, . . . , </span><span class="s11">n</span>} <span class="p">and  </span><span class="s11">j </span>∈ {<span class="p">1</span><span class="s15">, . . . , </span><span class="s11">m</span>}</p><h2 style="padding-left: 11pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s5">3:  </span>for <span class="p">each time step </span><i>t </i>do</h2><p class="s11" style="padding-left: 11pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s5">4:       </span><b>for </b>i <span class="s14">= </span><span class="p">1 to </span>n <b>do </b><span class="s63">I</span></p><p class="s14" style="padding-left: 11pt;text-indent: 0pt;line-height: 11pt;text-align: left;">▷ <span class="p">Update for each car</span></p><p class="s34" style="text-indent: 0pt;line-height: 7pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s34" style="text-indent: 0pt;line-height: 7pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-left: 118pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s16">p</span><span class="s23">C</span><span class="s24">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">+ </span><span class="s25">v</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>)f:,<span class="s11">t </span><span class="s14">+</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="1" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_017.png"/></span></p><p class="s15" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s16">a</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>)f:,<span class="s11">t </span><span class="s17">l</span></p><p class="s15" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s5">5:                </span><span class="s16">x</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t </span><span class="s14">+ </span><span class="p">1</span>) <span class="s14">←</span></p><p class="s5" style="padding-top: 2pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">6:         <span class="h2">end for</span></p><p class="s25" style="padding-top: 4pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">v<span class="s12">C</span><span class="s20">i</span></p><p class="s15" style="padding-top: 2pt;text-indent: 0pt;text-align: left;">(<span class="s11">t</span>) <span class="s14">+</span></p><p class="s16" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">a<span class="s12">C</span><span class="s20">i</span></p><p class="s34" style="text-indent: 0pt;line-height: 6pt;text-align: center;">2</p><p class="s15" style="text-indent: 0pt;line-height: 10pt;text-align: center;">(<span class="s11">t</span>)f:,<span class="s11">t</span></p><p class="s11" style="padding-left: 11pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s5">7:         </span><b>for </b>j <span class="s14">= </span><span class="p">1 to </span>m <b>do         </b><span class="s14">▷ </span><span class="p">Update for each pedestrian</span></p><p class="s15" style="padding-left: 111pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s64">I</span><span class="s22"> </span><span class="s16">p</span><span class="s23">P</span><span class="s24">j</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">+ </span><span class="s25">v </span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">t</span>)f:,<span class="s11">t </span><span class="s14">+</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="1" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_018.png"/></span></p><p class="s21" style="text-indent: 0pt;line-height: 5pt;text-align: left;">j</p><p style="text-indent: 0pt;text-align: left;"/><p class="s34" style="text-indent: 0pt;line-height: 7pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-left: 1pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s65">1</span><span class="s34"> </span><span class="s16">a </span><span class="s12">P</span><span class="s13"> </span>(<span class="s11">t</span>)f:,<span class="s11">t </span><span class="s49">2</span><span class="s64">l</span></p><p class="s15" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s5">8:                </span><span class="s16">x </span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">t </span><span class="s14">+ </span><span class="p">1</span>) <span class="s14">←</span></p><p class="s5" style="padding-top: 3pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">9:         <span class="h2">end for</span></p><p class="s25" style="padding-top: 4pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">v <span class="s12">P</span><span class="s20">j</span></p><p class="s15" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">(<span class="s11">t</span>) <span class="s14">+ </span><span class="s16">a </span><span class="s12">P</span><span class="s20">j</span></p><p class="s15" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">(<span class="s11">t</span>)f:,<span class="s11">t</span></p><p class="s15" style="padding-left: 7pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s5">10:         </span><span class="h2">for all </span><span class="p">pairs </span>(<span class="s11">C</span><span class="s12">i</span><span class="s13"> </span>, <span class="s11">P</span><span class="s12">j</span><span class="s13"> </span>) <span class="h2">do          </span><span class="s14">▷ </span><span class="p">Interaction and Risk</span></p><p style="padding-left: 52pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Assessment <span class="s22">I</span></p><p class="s66" style="text-indent: 0pt;line-height: 12pt;text-align: right;"> <span class="s67">2</span><span class="s19"> </span><span class="s22">\</span></p><p class="s5" style="padding-top: 2pt;padding-left: 52pt;text-indent: 0pt;text-align: left;"><a name="bookmark55">Fig. 5.    Interaction potential function visualizing the influence of distance</a></p><p class="s42" style="text-indent: 0pt;line-height: 4pt;text-align: right;"><span class="s68"> </span><span class="s69"> </span><span class="s70">p</span><span class="s27">C</span><span class="s24">i</span><span class="s21"> </span>(<span class="s13">t</span>)<span class="s53">− </span><span class="s70">p</span><span class="s27">P</span><span class="s24">j</span><span class="s21"> </span>(<span class="s13">t</span>)<span class="s68"> </span></p><p class="s5" style="padding-left: 93pt;text-indent: 0pt;line-height: 4pt;text-align: left;">and relative velocity on interaction dynamics. Higher interaction potential is</p><p style="text-indent: 0pt;text-align: left;"><span><img width="73" height="1" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_019.png"/></span></p><p class="s11" style="padding-left: 7pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s5">11:                 </span>V<span class="s12">i</span><span class="s13"> j </span><span class="s15">(</span>t<span class="s15">) </span><span class="s14">← </span><span class="p">exp  </span><span class="s14">−</span><span class="s71"> </span></p><p class="s5" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">12:         <span class="h2">end for</span></p><p class="s34" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">2<span class="s42">σ </span><span class="s72">2</span></p><p class="s5" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">observed at closer distances and higher relative velocities.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="237" height="196" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_020.png"/></span></p><p class="s15" style="padding-left: 7pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s5">13:         </span><span class="h2">for all </span><span class="p">pairs </span>(<span class="s11">C</span><span class="s12">i</span><span class="s13"> </span>, <span class="s11">P</span><span class="s12">j</span><span class="s13"> </span>) <span class="h2">do          </span><span class="s14">▷ </span><span class="p">Interaction and Risk</span></p><p class="s73" style="padding-left: 52pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Assessment            <span class="p"> </span><span class="s22">(</span></p><p class="s74" style="padding-top: 7pt;padding-left: 52pt;text-indent: 0pt;line-height: 5pt;text-align: left;"> <span class="s43">2</span><span class="s22"> </span></p><p class="s11" style="padding-left: 7pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="s5">14:                </span>P<span class="s12">c</span><span class="s42">,</span><span class="s13">i j </span><span class="s15">(</span>t<span class="s15">)      </span><span class="s14">←     </span><span class="p">exp</span></p><p class="s22" style="text-indent: 0pt;line-height: 9pt;text-align: center;">I</p><p class="s15" style="padding-left: 3pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="s14">− </span><span class="h2">p</span><span class="s12">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s11">t</span>) <span class="s14">− </span><span class="h2">p</span><span class="s12">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s11">t</span>)       <span class="s14">×</span></p><p class="s22" style="padding-left: 3pt;text-indent: 0pt;line-height: 9pt;text-align: left;">\</p><p class="s75" style="padding-top: 1pt;padding-left: 75pt;text-indent: 0pt;line-height: 7pt;text-align: left;">                    1                     </p><p class="s69" style="padding-left: 94pt;text-indent: 0pt;line-height: 3pt;text-align: left;">( </p><p class="s42" style="padding-left: 75pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s34">1</span><span class="s53">+</span><span class="s34">exp    </span><span class="s76">v</span><span class="s27">C</span><span class="s20">i</span><span class="s21"> </span>(<span class="s13">t</span>)<span class="s53">−</span><span class="s76">v</span><span class="s27">P</span><span class="s20">j</span><span class="s21"> </span>(<span class="s13">t</span>)<span class="s68"> </span></p><p class="s5" style="padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;">15:         <span class="h2">end for</span></p><h2 style="padding-left: 7pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s5">16:         </span>for <span class="p">each agent </span><i>a </i>do       <span class="s14">▷ </span><span class="p">Policy Learning with SVO</span></h2><p class="s11" style="padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s5">17:                   </span><span class="p">Update SVO based on interactions:  </span>SV O<span class="s12">a</span><span class="s15">(</span>t <span class="s14">+</span></p><p class="s15" style="padding-left: 67pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="p">1</span>) <span class="s14">= </span><span class="s11">SV O</span><span class="s12">a</span>(<span class="s11">t</span>) <span class="s14">+ </span>ηf:,<span class="s11">SV O</span><span class="s12">a</span>(<span class="s11">t</span>)</p><p class="s15" style="padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s5">18:                   </span><span class="p">Compute utility </span><span class="s11">U</span><span class="s12">a</span>(<span class="s16">x</span>, <span class="s11">SV O</span><span class="s12">a</span>)</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s5">19:                     </span>Update   policy   <span class="s15">π   </span>using   SARSA:   <span class="s15">π </span><span class="s52">∗    </span><span class="s53"> </span><span class="s14">=</span></p><p class="s15" style="padding-left: 67pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="p">arg max</span><span class="s55">π</span><span class="s42"> </span><span class="s11">V </span><span class="s77">π</span><span class="s42"> </span>(<span class="s16">x</span>, <span class="s11">SV O</span><span class="s12">a</span>)</p><p class="s5" style="padding-left: 67pt;text-indent: -60pt;text-align: left;">20:                    <span class="p">Update  Q-values  using  Bellman  equation  for SARSA</span></p><p class="s5" style="padding-left: 7pt;text-indent: 0pt;text-align: left;">21:         <span class="h2">end for</span></p><p class="s5" style="padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: left;">22:  <span class="h2">end for</span></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s5">23:  </span><b>Output: </b>Optimized policies <span class="s15">π </span><span class="s52">∗</span><span class="s53"> </span>for all agents</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="336" height="1" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_021.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="333" height="228" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_022.png"/></span></p><p class="s5" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark56">Fig. 4.   Collision risk surface as a function of relative velocity and distance between the ego vehicle and other agents.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">The symmetry in the results, with both positive and negative values for relative velocity, highlights the risk in head-on and</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark57">Fig. 6. Dynamic adaptation of the SVO of the ego vehicle over time. The SVO changes based on interactions and outcomes, balancing egoistic and prosocial behaviors.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s9" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: right;"><a href="#bookmark55" class="s7">rear-end scenarios, which also emphasizes how maintaining safe distances and appropriate speeds minimize collision risks. In Fig. </a>5<a href="#bookmark55" class="s7">, the interaction potential function is depicted to illustrate the influence of distance and relative velocity on the interaction dynamics between the ego vehicle and pedestrians or other vehicles. The horizontal axis represents the distance, the vertical axis represents the relative velocity, and the color gradient represents the interaction potential. Fig. </a>5 <a href="#bookmark57" class="s7">emphasizes the  decrease  in  interaction  potential  as  distance  increases, especially at closer distances and higher relative velocities. Fig.  </a>6  <a href="#bookmark57" class="s7">shows  the  dynamic  adaptation  of  the  SVO  of  the ego vehicle over time, indicating how the SVO dynamically changes based on the interactions and outcomes experienced by the ego vehicle. This adaptation allows the ego vehicle to adjust its social preferences, balancing egoistic and prosocial behaviors to optimize performance in varying traffic scenarios. Fig. </a>6 <a href="#bookmark58" class="s7">shows how the SVO stabilizes after initial fluctuations, indicating the learning process and eventual convergence to a stable behavior pattern. Fig. </a>7 <a href="#bookmark58" class="s7">illustrates the average reward per episode for different SVO values spanning egoistical to proso- cial in terms of the number of episodes. Fig. </a>7 <span style=" color: #000;">also compares</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 42pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="237" height="196" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_023.png"/></span>	<span><img width="234" height="195" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_024.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 78%;text-align: justify;"><a name="bookmark58">Fig. 7. Average Reward per Episode for Different SVOs. comparing the aver- age reward progression for egoistic (</a><i>SV O </i><span class="s39">= </span>0), intermediate (<i>SV O </i><span class="s39">= </span>0<span class="s38">.</span>7), and prosocial (<i>SV O </i><span class="s39">= </span>1) orientations over time.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="271" height="90" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_025.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="267" height="100" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_026.png"/></span></p><p class="s5" style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark59">Fig. 8. Exploration Rate Decay and Policy Change Frequency over the number of episodes.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 83%;text-align: justify;">the performance of three SVO settings: egoistic (<i>SV O </i><span class="s14">= </span>0), intermediate (<i>SV O </i><span class="s14">= </span>0<span class="s15">.</span>7), and prosocial (<i>SV O </i><span class="s14">= </span>1), and it is shown that intermediate agents achieve the highest average</p><p class="s9" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark58" class="s7">rewards over time, followed by the prosocial and egoistic agents. The shaded regions represent the confidence intervals, indicating the variability in rewards across different runs. Additionally, it is demonstrated how varying the SVO impacts the agents’ learning efficiency and reward optimization in a MURL environment. The overall trend in Fig. </a>7 <span style=" color: #000;">demonstrates that agents with higher SVO values learn more effective strategies by considering the welfare of others.</span></p><p class="s9" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark59" class="s7">Fig. </a>8 <a href="#bookmark60" class="s7">shows the changes in our MURL framework’s strat- egy exploration behavior and policy over time. Initially, a high exploration and frequent policy adjustment  is  pronounced, but as the policy improves, exploration decreases, and policy changes slow down. This indicates a move towards policy sta- bilization and convergence. Fig. </a>9 <span style=" color: #000;">shows the Q-values heatmap generated from the SVO-reshaped SARSA algorithm with dynamic optimization. This heatmap illustrates the learned Q-values for each state-action pair, where states are defined by specific positions (1-6), and actions are defined by specific maneuvers (1-6). The intensity of the colors represents the magnitude of the Q-values, with red representing lower values and blue showing higher values. The high Q-values (red regions) in certain state-action pairs indicate that these actions</span></p><p class="s5" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark60">Fig. 9.    Heatmap of Q-values for State-Action Pairs in the SVO-Reshaped SARSA Algorithm.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="267" height="95" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_027.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="273" height="96" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_028.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark61">Fig. 10.     Learning curve and state visitation frequency over time using SVO-reshaped SARSA with proposed dynamic optimization algorithm.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: right;"><a href="#bookmark61" class="s7">are highly valued in those states. For instance, the highest Q-values are observed for states 3 and 4 with actions 2 and 3, respectively, suggesting that yielding and stopping lanes are the  most  rewarding  when  the  vehicle  is  in  those  states. Fig.  </a><span style=" color: #00F;">10  </span><a href="#bookmark61" class="s7">presents  the  multi-agent  system’s  learning  process and state visitation dynamics using the SVO-reshaped SARSA algorithm with dynamic optimization. Fig. </a><span style=" color: #00F;">10 </span><a href="#bookmark61" class="s7">a graph shows the  learning  curve,  represented  by  the  average  reward  per episode, which indicates a rapid initial increase in reward, fol- lowed by a plateau as the system approaches an optimal policy. This trend suggests that the ego AV quickly learns effective pedestrian interaction strategies and improves its performance in the simulated environment. Fig. </a><span style=" color: #00F;">10b </span><a href="#bookmark62" class="s7">demonstrates the state visitation frequency over time for three distinct states, with each line tracking how often a particular state is visited. The convergence of state visitation frequencies over time reflects a balanced exploration of state space, critical for ensuring robust policy learning. The parallel increase of all curves indicates that the algorithm does not overly focus on any single state, which can prevent overfitting and ensure a generalized policy. Fig.  </a><span style=" color: #00F;">11  </span><a href="#bookmark62" class="s7">illustrates  the  adaptability  and  stability  of  the learning process in the SVO-reshaped SARSA algorithm with dynamic  optimization.  Fig.  </a><span style=" color: #00F;">11a  </span>depicts  the  fluctuations  in Q-value  changes  over  time  for  different  state-action  pairs, represented as  <i>Q</i><span class="s15">(</span><i>s</i>1<span class="s15">, </span><i>a</i>1<span class="s15">)</span>,  <i>Q</i><span class="s15">(</span><i>s</i>1<span class="s15">, </span><i>a</i>2<span class="s15">)</span>, etc. A high degree of fluctuation indicates the algorithm’s exploration mechanism,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="8" height="57" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_029.png"/></span>	<span><img width="258" height="85" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_030.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="262" height="96" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_031.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="85" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_032.png"/></span></p><p class="s5" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark62">Fig. 11. Temporal analysis of Q-value adjustments and convergence in the SVO-reshaped SARSA with proposed dynamic optimization algorithm.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 32pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="264" height="72" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_033.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 33pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="263" height="81" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_034.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 132pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span><img width="24" height="6" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_035.png"/></span></p><p class="s5" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark63">Fig.  12.   Distribution  of  states  visited  and  actions  taken  by  the  AV  in the SVO-reshaped SARSA environment with proposed dynamic optimization algorithm.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark62" class="s7">while the gradual decrease in fluctuation suggests the stabiliza- tion of the learning process as the policy converges. Fig. </a><span style=" color: #00F;">11b </span><a href="#bookmark63" class="s7">presents the sum of squared Q-value differences.  Initially, large variances are observed, reflecting the system’s learning and adaptation phase. Over time, these differences diminish, signifying that the algorithm is approaching an optimal set of Q-values. This convergence is a fundamental attribute of the SVO-reshaped SARSA algorithm, demonstrating its capacity to systematically reduce uncertainty and enhance precision. Fig. </a><span style=" color: #00F;">12 </span><a href="#bookmark63" class="s7">illustrates the distribution of states visited and actions taken by the AV in the SVO-reshaped SARSA environment with the proposed dynamic optimization algorithm. Fig.</a><span style=" color: #00F;">12a </span><a href="#bookmark63" class="s7">shows the frequency of visits to each state, while the bottom histogram displays the frequency of each action taken. Fig.</a><a href="#bookmark63" class="a">12 </a><span style=" color: #00F;">b </span>shows that ‘Stop’ is the most frequently taken action, followed by ’<i>Accelerate</i>’ and ’<i>Yield</i>’. This distribution reflects the strate- gic decision-making process influenced by the SVO-reshaped SARSA algorithm. The higher frequency of ’<i>Stop</i>’ actions indicates a strong emphasis on safety, likely to avoid collisions in complex traffic scenarios. The frequencies of ’<i>Accelerate</i>’ and ’<i>Yield</i>’ suggest that the vehicle also maintains efficient traffic flow and responds to dynamic changes.</p><p class="s9" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark64" class="s7">Fig. </a>13 <a href="#bookmark64" class="s7">illustrates the 3D surface plots mapping out Q-values as a function of state and action indices within our SVO-reshaped SARSA learning framework. Fig. </a>13a <span style=" color: #000;">shows a</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="279" height="217" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_036.jpg"/></span></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark64">Fig. 13. 3D Surface Plots of Q-Values across State-Action Space: (a) Contour plot of the action-state space and (b) 3D surface plot representing the learned Q-values in the SVO-reshaped SARSA algorithm.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 102pt;text-indent: 0pt;text-align: center;"><a name="bookmark65">TABLE II</a></p><p class="s5" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">E<span class="s62">VALUATING  </span>P<span class="s62">ERFORMANCE  </span>W<span class="s62">ITH  AND  </span>W<span class="s62">ITHOUT  </span>SVO C<span class="s62">ONSIDERING</span></p><p class="s5" style="text-indent: 0pt;line-height: 9pt;text-align: center;">C<span class="s62">OLLISION </span>R<span class="s62">ISK AND </span>A<span class="s62">VERAGE </span>E<span class="s62">PISODE </span>L<span class="s62">ENGTH</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 3pt;text-align: left;"><span><img width="336" height="4" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_037.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="334" height="51" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_038.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s9" style="padding-left: 5pt;text-indent: 0pt;text-align: right;"><a href="#bookmark64" class="s7">contour plot of the action-state space, revealing the geometric relationship  between  state-action  pairs  and  their  Q-values. Fig. </a>13b <a href="#bookmark64" class="s7">provides a 3D representation of the learned Q-values and shows the policy’s behavior, indicating the algorithm’s preferences for state-action combinations. Higher Q-values are represented by warmer colors, suggesting more favorable state- action pairs. Fig. </a><a href="#bookmark64" class="a">13 </a>b <a href="#bookmark65" class="s7">implies that the algorithm has learned to assign higher Q-values to certain actions in specific states. Table </a>II <span style=" color: #000;">presents the integration of SVO into the SARSA algorithm  with  a  significant  improvement  in  system  per- formance  compared  to  traditional  reinforcement  learning approaches. The proposed method substantially reduces the collision risk to 0.08, compared to 0.18 for SARSA without SVO and 0.11 for the Deep Q-Network. This reduction in collision risk by 55.6% from the traditional SARSA and by 27.3% from the Deep Q-Network highlights the efficacy of the SVO framework in enhancing the safety of autonomous systems. Furthermore, the average reward per episode for the proposed method stands at 2.1, significantly higher than the</span></p><p class="s9" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark66" class="s7">1.1 achieved by SARSA without SVO and the 1.6 achieved by the Deep Q-Network. This  enhancement  indicates  that the suggested approach improves safety without sacrificing the reward, which  often  represents  system  objectives  such as speed, efficiency, or compliance with traffic regulations. The most evident disparity is in the average episode length, where the proposed method delivers an episode length of 435, significantly lower than the 754 and 954 steps required for SARSA without SVO and the Deep Q-Network, respectively. This substantial reduction signifies a more efficient policy as the system reaches its goal state more swiftly. Figure </a>14 <span style=" color: #000;">shows the speed profiles of the ego vehicle, pedestrian, and another vehicle across different SVO values. For an SVO of 0, the ego</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="294" height="162" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_039.png"/></span></p><p class="s5" style="padding-top: 8pt;padding-left: 5pt;text-indent: 0pt;line-height: 78%;text-align: justify;"><a name="bookmark66">Fig. 14. Tempo-spatial analysis of the speed profile for the ego car, pedestrian, and other vehicle under different SVO values from </a><i>SV O </i><span class="s39">= </span>0 to <i>SV O </i><span class="s39">= </span>1 representing ego-centric to altruistic.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="291" height="161" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_040.png"/></span></p><p class="s5" style="padding-top: 8pt;padding-left: 5pt;text-indent: 0pt;line-height: 83%;text-align: justify;"><a name="bookmark67">Fig. 15. Spatial analysis of the ego car, pedestrian, and other vehicle trajec- tories under different SVO values from </a><i>SV O </i><span class="s39">= </span>0 to <i>SV O </i><span class="s39">= </span>1 representing ego-centric to altruistic.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s9" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark58" class="s7">vehicle keeps higher speeds with infrequent reductions, which prioritizes its own progress and shows a self-centered driving behavior. As the SVO value increases to 0.7, the ego vehicle reduces its speed earlier, adopting a more cautious approach to ensure safer interactions with the pedestrian and the other vehicle. With an SVO of 1, the ego vehicle maintains its speed longer before braking, relying on precise risk assessments to decide when to reduce speed. This behavior is suggestive of a fully prosocial strategy that can aim to optimize overall traffic flow and minimize unnecessary interventions. In this scenario, the SVO 0.7 agent demonstrates a safer driving strategy by initiating braking sooner. This suggests that an intermediate SVO value can lead to more cautious behavior, allowing the agent to proactively mitigate potential risks, as also seen in Fig. </a>7<a href="#bookmark67" class="s7">. Finally, Figure </a>15 <span style=" color: #000;">shows the spatial trajectories of the ego vehicle, pedestrian, and another vehicle under different</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 88%;text-align: justify;">SVO values (SVO <span class="s14">= </span>0, 0.7, and 1). These trajectories show the paths each agent takes to reach their intended destinations.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">While the spatial paths are similar across all SVO valuessince all agents ultimately reach their goalsthe key differences lie in the timing of actions along these trajectories, such as when braking or acceleration occurs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 114pt;text-indent: -23pt;text-align: left;"><p style="display: inline;"><a name="bookmark68">C</a><span class="s5">ONCLUSION</span></p></li></ol></li></ol><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">In this paper, we proposed an innovative MURL archi- tecture for AV navigation in urban environments using a SARSA learning algorithm enriched with a dynamic opti- mization technique and integrating SVO. Our approach incorporates individual agents’ social preferences and logistic</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">regression-based risk assessment to evaluate collision proba- bilities in real-time. The results of our simulation experiments showed a significant reduction in collision risk, an increase in average rewards per episode, and an efficient convergence to an optimal policy. Looking ahead, future work could focus on the integration of comprehensive psychological models, such as the Theory of Planned Behavior, to better comprehend the diversity of human behaviors in urban settings. Additionally, it is possible to extend the proposed framework to other complex traffic scenarios, such as multi-lane interactions and varying pedestrian dynamics, which would further improve its generalization.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 102pt;text-indent: 0pt;text-align: center;">R<span class="s5">EFERENCES</span></p><p class="s5" style="padding-top: 8pt;padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark69">[1] World Health Org. (Dec. 2023). </a><i>Road Traffic Injuries</i><a href="http://www.who.int/news-room/fact-sheets/detail/road-traffic-" class="s8" target="_blank">. [Online]. Avail- able: </a>https://www.who.int/news-room/fact-sheets/detail/road-traffic- injuries#:~: text=Approximately</p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark70">[2] J. Zhu, L. Wang, I. Tasic, and X. Qu, “Improving freeway merging effi- ciency via flow-level coordination of connected and autonomous vehi- cles,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 25, no. 7, pp. 6703–6715, Jul. 2024.<a name="bookmark71">&zwnj;</a></p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[3] M. Garg and M. Bouroche, “Can connected autonomous vehicles improve mixed traffic safety without compromising efficiency in real- istic scenarios?” <i>IEEE Trans. Intell.  Transp.  Syst.</i>,  vol.  25,  no.  6, pp. 6674–6689, Jun. 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark72">[4] S. Zhong et al., “Energy and environmental impacts of shared autonomous vehicles under different pricing strategies,” </a><i>NPJ Urban Sustainability</i>, vol. 3, no. 1, p. 8, Feb. 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark73">[5] J. Hwang and S. Kim, “Autonomous vehicle transportation service for people with disabilities: Policy recommendations based on the evidence from hybrid choice model,” </a><i>J. Transp. Geography</i>, vol. 106, Jan. 2023, Art. no. 103499.<a name="bookmark74">&zwnj;</a></p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[6] J. Schoner, R. Sanders, and T. Goddard, “Effects of advanced driver assistance systems on impact velocity and injury severity: An exploration of data from the crash investigation sampling system,” <i>Transp. Res. Rec.,</i></p><p class="s6" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">J. Transp. Res. Board<span class="s5">, vol. 2678, no. 5, pp. 451–462, May 2024.</span></p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark75">[7] V. Dubljevic et al., “Toward a rational and ethical sociotechnical system of autonomous vehicles: A novel application of multi-criteria decision analysis,” </a><i>PLoS ONE</i>, vol. 16, no. 8, Aug. 2021, Art. no. e0256224.</p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;"><a name="bookmark76">[8] H. Taghavifar and A. Mohammadzadeh, “Integrating deep reinforce- ment learning and social-behavioral cues: A new human-centric cyber-physical approach in automated vehicle decision-making,” </a><i>Proc. Inst. Mech. Eng., D, J. Automobile  Eng.</i>,  vol.  2024,  Feb.  2024, Art. no. 09544070241230126.<a name="bookmark77">&zwnj;</a></p><p class="s5" style="padding-left: 24pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">[9] B. Toghi, R. Valiente, D. Sadigh, R. Pedarsani, and Y. P. Fallah, “Social coordination and altruism in autonomous driving,” <i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 23, no. 12, pp. 24791–24804, Dec. 2022.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark78">[10] L. Crosato, H. P. H. Shum, E. S. L. Ho, and C. Wei, “Interaction-aware decision-making for automated vehicles using social value orientation,” </a><i>IEEE Trans. Intell. Vehicles</i>, vol. 8, no. 2, pp. 1339–1349, Feb. 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark79">[11] A. Balachandran et al., “Human-centric intelligent driving: Collaborat- ing with the driver to improve safety,” </a><i>Road Vehicle Autom.</i>, vol. 9, pp. 85–109, Jul. 2022.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark80">[12] K. Tian et al., “Impacts of visual and cognitive distractions and time pressure on pedestrian crossing behaviour: A simulator study,” </a><i>Accident Anal. Prevention</i>, vol. 174, Sep. 2022, Art. no. 106770.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark81">[13] W. Liu et al., “A systematic survey of control techniques and applications in connected and automated vehicles,” 2023, </a><i>arXiv:2303.05665</i>.<a name="bookmark82">&zwnj;</a></p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[14] S. A. Bagloee, M. Tavana, M. Asadi, and T. Oliver, “Autonomous vehi- cles: Challenges, opportunities, and future implications for transportation policies,” <i>J. Mod. Transp.</i>, vol. 24, no. 4, pp. 284–303, Aug. 2016.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark83">[15] H. Taghavifar, C. Wei, and L. Taghavifar, “Socially intelligent rein- forcement learning for optimal automated vehicle control in traffic scenarios,” </a><i>IEEE Trans. Autom. Sci. Eng.</i><a href="http://dx.doi.org/10.1109/TASE.2023.3347264" class="s8" target="_blank">, early access, Jan. 29, 2024, doi: </a><a href="http://dx.doi.org/10.1109/TASE.2023.3347264" class="s78" target="_blank">10.1109/TASE.2023.3347264.</a><a name="bookmark84">&zwnj;</a></p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">[16] T. Liu et al., “Heuristics-oriented overtaking decision making for autonomous vehicles using reinforcement learning,” <i>IET Electr. Syst. Transp.</i>, vol. 10, no. 4, pp. 417–424, Dec. 2020.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark85">[17] Z. Cao et al., “Reinforcement learning based control of imitative policies for near-accident driving,” 2020, </a><i>arXiv:2007.00178</i>.</p><p class="s5" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark86">[18]  C.-J.    Hoel,    K.    Driggs-Campbell,    K.    Wolff,    L.    Laine,    and</a></p><p class="s5" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">M. J. Kochenderfer, “Combining planning and deep reinforcement learning in tactical decision making  for autonomous driving,” <i>IEEE Trans. Intell. Vehicles</i>, vol. 5, no. 2, pp. 294–305, Jun. 2020.</p><p class="s5" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark87">[19]  B. Andrew and R. S. Sutton, </a><i>Reinforcement Learning: An Introduction</i>.</p><p class="s5" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">MIT Press, 2018.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark88">[20] J. Wu, Z. Song, and C. Lv, “Deep reinforcement learning based energy- efficient decision-making for autonomous electric vehicle in dynamic traffic environments,” </a><i>IEEE Trans. Transp. Electrific.</i>, vol. 10, no. 1, pp. 875–887, Mar. 2024.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark89">[21] C. Zhang, O. Vinyals, R. Munos, and S. Bengio, “A study on overfitting in deep reinforcement learning,” 2018, </a><i>arXiv:1804.06893</i>.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark90">[22] P. Hang, C. Lv, Y. Xing, C. Huang, and Z. Hu, “Human-like decision making for autonomous driving: A noncooperative game the- oretic approach,” </a><i>IEEE Trans. Intell.  Transp.  Syst.</i>,  vol.  22,  no.  4, pp. 2076–2087, Apr. 2021.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark91">[23] P. Hang, C. Lv, C. Huang, Y. Xing, and Z. Hu, “Cooperative decision making of connected automated vehicles at multi-lane merging zone: A coalitional game approach,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 23, no. 4, pp. 3829–3841, Apr. 2022.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark92">[24] Z. Huang, J. Wu, and C. Lv, “Driving behavior modeling using natu- ralistic human driving data with inverse reinforcement learning,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 23, no. 8, pp. 10239–10251, Aug. 2022.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark93">[25] Z. Huang, H. Liu, J. Wu, and C. Lv, “Conditional predictive behavior planning with inverse reinforcement learning for human-like autonomous driving,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 24, no. 7, pp. 7244–7258, Jul. 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark94">[26] Z. Wu, F. Qu, L. Yang, and J. Gong, “Human-like decision making for autonomous vehicles at the intersection using inverse reinforcement learning,” </a><i>Sensors</i>, vol. 22, no. 12, p. 4500, Jun. 2022.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark95">[27] S. Adams, T. Cody, and P. A. Beling, “A survey of inverse reinforcement learning,” </a><i>Artif. Intell. Rev.</i>, vol. 55, no. 6, pp. 4307–4346, 2022.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark96">[28] M. S. Rais, R. Boudour, K. Zouaidia, and L. Bougueroua, “Decision making for autonomous vehicles in highway scenarios using harmonic SK deep SARSA,” </a><i>Appl. Intell.</i>, vol. 53,  no.  3,  pp.  2488–2505, Feb. 2023.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark97">[29] C.-C. Lin, K.-Y. Chen, and L.-T. Hsieh, “Real-time charging scheduling of automated guided vehicles in cyber-physical smart factories using feature-based reinforcement learning,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 24, no. 4, pp. 4016–4026, Apr. 2023.</p><p class="s5" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark98">[30]  L.  Crosato,  K.  Tian,  H.  P.  H.  Shum,  E.  S.  L.  Ho,  Y.  Wang,  and</a></p><p class="s5" style="padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">C. Wei, “Social interaction-aware dynamical models and decision- making for autonomous vehicles,” <i>Adv. Intell. Syst.</i>,  vol.  6,  no.  3, Mar. 2024, Art. no. 2300575.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark99">[31] B. Toghi, R. Valiente, D. Sadigh, R. Pedarsani, and Y. P. Fallah, “Altruistic maneuver planning for cooperative autonomous vehicles using multi-agent advantage actor-critic,” 2021, </a><i>arXiv:2107.05664</i>.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark100">[32] R. Valiente, B. Toghi, R. Pedarsani, and Y. P. Fallah, “Robustness and adaptability of reinforcement learning-based cooperative autonomous driving in mixed-autonomy traffic,” </a><i>IEEE Open J. Intell. Transp. Syst.</i>, vol. 3, pp. 397–410, 2022.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark101">[33] S. Li, K. Peng, F. Hui, Z. Li, C. Wei, and W. Wang, “A decision-making approach for complex unsignalized intersection by deep reinforcement learning,” </a><i>IEEE Trans. Veh. Technol.</i>, vol. 73, no. 11, pp. 16134–16147, Nov. 2024.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark102">[34] R. Valiente, M. Razzaghpour, B.  Toghi, G. Shah, and Y. P. Fallah, “Prediction-aware and reinforcement learning-based altruistic coop- erative driving,” </a><i>IEEE Trans. Intell.  Transp.  Syst.</i>,  vol.  25,  no.  3, pp. 2450–2465, Mar. 2024.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark103">[35] NVIDIA Metropolis. </a><i>Transforming the Future of Mobility at ITS America With NVIDIA Metropolis Partners</i>. Accessed: Jun. 13, 2024. [Online]. Available: https://developer.nvidia.com/blog/transforming-the- future-of-mobility-at-its-america-with-nvidia-metropolis-partners/</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark104">[36] S. Dereich and T. Müller-Gronbach, “General  multilevel  adapta- tions for stochastic approximation algorithms of Robbins–Monro and Polyak–Ruppert type,” </a><i>Numerische Math.</i>, vol. 142, no. 2, pp. 279–328, Jun. 2019.</p><p class="s5" style="padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark105">[37] X. Kavelaars, J. Mulder, and M. Kaptein, “Bayesian multilevel multivari- ate logistic regression for superiority decision-making under observable treatment heterogeneity,” </a><i>BMC Med. Res. Methodol.</i>, vol. 23, no. 1, p. 220, Oct. 2023.</p><p class="s5" style="padding-top: 1pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark106">[38] M. Zhang, D. Chu, Z. Deng, and C. Zhao, “Game theory-based lane change decision-making considering vehicle’s social value orientation,” SAE, Warrendale, PA, USA, Tech. Rep. 2023-01-7109, 2023.</a></p><p class="s5" style="padding-top: 2pt;padding-left: 24pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark107">[39] J. Liu, X. Qi, P. Hang, and J. Sun, “Enhancing social decision-making of autonomous vehicles: A mixed-strategy game approach with interaction orientation identification,” </a><i>IEEE Trans. Intell. Transp. Syst.</i><a href="http://dx.doi.org/10.1109/TITS.2023.1234567" class="s8" target="_blank">, vol. 25, no. 3, pp. 1234–1245, Mar. 2024, doi: </a><a href="http://dx.doi.org/10.1109/TITS.2023.1234567" class="s78" target="_blank">10.1109/TITS.2023.1234567.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="96" height="120" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_041.jpg"/></span></p><h4 style="padding-left: 88pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Hamid Taghavifar <span class="s5">(Senior Member, IEEE) is cur- rently an Assistant Professor with the Department of Mechanical, Industrial and Aerospace Engineer- ing, Concordia University, Montreal, Canada. His research interests include control theory and applica- tions, automated driving, mechatronics, and robotics.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="96" height="120" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_042.jpg"/></span></p><h4 style="padding-left: 88pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Chuan Hu <span class="s5">received the B.S. degree  in  automo- tive engineering from Tsinghua University, Beijing, China, in 2010, and the Ph.D. degree in mechanical engineering from McMaster University, Hamilton, Canada, in 2017. He is currently an Associate Pro- fessor at the School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai,  China. His research interests include perception, predic- tion, decision-making, path planning, motion control of intelligent vehicles, eco-driving, human–machine interaction, and shared control. He has published</span></h4><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">more than 80 articles in these research areas. He is currently an associate editor of several leading IEEE T<span class="s62">RANSACTIONS</span>/journals.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="96" height="120" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_043.jpg"/></span></p><h4 style="padding-left: 88pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Chongfeng Wei <span class="s5">(Member, IEEE) received the B.S. degree in computational and applied mathematics and the M.S. degree in vehicle engineering from Southwest Jiaotong University, Chengdu, China, in 2009 and 2011, respectively, and the Ph.D. degree in mechanical engineering from the University of Birmingham, Birmingham, U.K., in 2015. He is currently a Senior Lecturer (an Associate Professor) at the University of Glasgow, U.K. His current research interests include decision-making and con- trol of intelligent vehicles, cooperative autonomy,</span></h4><p class="s5" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">and robotic system dynamics and control. He is serving as an associate editor for several peer-reviewed IEEE journals.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="96" height="120" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_044.jpg"/></span></p><h4 style="padding-left: 88pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Ardashir Mohammadzadeh <span class="s5">received the B.Sc. degree from the Sahand University of Technology, Iran, the M.Sc. degree from the K. N. Toosi Univer- sity of Technology, Iran, and the Ph.D. degree from the University of Tabriz. He is currently a Professor with the Department of Electrical and Electronics Engineering, Sakarya University, Sakarya, Türkiye. His research interests include control theory, fuzzy logic systems, machine learning, and electric vehicles.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="96" height="120" alt="image" src="2025-Behaviorally-Aware_Multi-Agent_RL_With_Dynamic_Optimization_for_Autonomous_Driving/Image_045.jpg"/></span></p><h4 style="padding-left: 88pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Chunwei Zhang <span class="s5">received the degree from Harbin Institute of Technology in 2005.  He  is  currently the Chair Distinguished Professor and a Ph.D. Supervisor with Shenyang University of Technology (SUT), China. His research interests include struc- tural control, structural health monitoring, AI, smart materials, and structures.</span></h4></body></html>
