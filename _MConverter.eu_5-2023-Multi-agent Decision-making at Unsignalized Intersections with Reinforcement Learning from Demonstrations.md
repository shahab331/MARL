Downloaded from https://iranpaper.ir https://www.tarjomano.com

https://www.tarjomano.com

Multi-agent Decision-making at Unsignalized Intersections with Reinforcement Learning from Demonstrations Chang Huang, Junqiao Zhao\*, Member, IEEE, Hongtu Zhou, Hai Zhang, Xiao Zhang, Chen Ye Abstract— Intersections are key nodes and also bottlenecks of which is a complex problem even for humans. If colli-urban road networks, so improving the traffic efficiency at inter-sions occur frequently, vehicles may become too cautious sections is beneficial to improving overall traffic throughput and to avoid interactions. Furthermore, the global state-action mitigating traffic congestion. Previous methods such as rule-space dimension of MARL grows exponentially with the based, planning-based, and single-agent reinforcement learning usually oversimplify the policies of the surrounding vehicles and number of agents, which requires a local-global exploration thus have difficulty modeling the complex interaction behaviors trade-off \[7\]. Exploring only from a global perspective is between vehicles, which limits the performance of these methods inefficient while exploring only from a local perspective to some extent. Instead, we adopt a multi-agent reinforcement lacks clear guidance for more valuable spaces. Therefore, learning \(MARL\) approach to train and coordinate the policies the complex interactions and large global state-action space of all vehicles to handle unsignalized intersection scenarios. 

Nevertheless, due to complex interactions between multiple make it extremely difficult to successfully pass through the agents, it is challenging to efficiently explore the environment intersection, and the algorithm may converge to a suboptimal and obtain high-reward samples. We therefore propose to pre-policy early in training due to insufficient exploration. 

train the policy using demonstration data consisting of expert So it is of great importance to use an effective exploration data and interaction data to improve the initial performance method that can make full use of the interaction informa-of agents and improve exploration, as well as to reduce the distributional shift between the demonstration data and the tion. Many such methods model the interaction process by environmental interaction data. We experimentally prove that computing mutual information \[8\], causal influence \[9\], and using interaction data generated by the algorithm in the social influence \[10\] among agents, and use this interaction demonstration data improves training stability. The proposed information to encourage agents to explore those spaces method enables effective exploration and greatly speeds up the where interactions between agents occur. These methods training process. 

have achieved remarkable progress in the field of games. 

However, the MARL methods applied in the field of au-I. INTRODUCTION

tonomous driving \[4\] \[5\] \[6\] do not pay much attention to The policy learning at intersections is one of the most the exploration problem. This motivates us to investigate challenging problems and has been an important topic of better exploration methods to deal with complex interaction research. Many previous methods such as rule-based \[1\], scenarios like unsignalized intersections more effectively. 

planning-based \[2\] and reinforcement learning \(RL\) \[3\]

In this paper, we propose to add a pre-training stage methods usually treat it as a single-agent problem and focus to the original training process of the value decomposition on the policy learning of ego-vehicle. These methods con-MARL method QMIX \[11\] to improve initial performance sider other vehicles as part of the environment and assume and exploration ability, which helps to obtain high-reward that they act according to fixed strategies. Therefore, the samples in the early stage of subsequent online training. 

complex interactions between vehicles can not be modeled. 

The demonstration data used for pre-training consists of Recent studies are starting to apply multi-agent reinforce-expert data and interaction data, where the interaction data ment learning \(MARL\) to policy learning at intersections is generated by the algorithm itself through interacting with

\[4\] \[5\] \[6\], as it can be expected that in the future vehicles the environment, rather than a fixed dataset like other similar can be connected and a unified strategy can be adopted to methods \[12\] \[13\] \[14\]. This is important because using coordinate and control all vehicles. We therefore consider self-generated interaction data helps to reduce the mismatch policy learning at unsignalized intersections as a multi-agent between the data distribution induced by the algorithm and 2023 IEEE Intelligent Vehicles Symposium \(IV\) | 979-8-3503-4691-6/23/$31.00 ©2023 IEEE | DOI: 10.1109/IV55152.2023.10186792

problem and employ MARL method to efficiently learn the data distribution contained in the demonstration data \[15\], cooperative policies. 

which makes the transition between pre-training and online However, interactions between vehicles at unsignalized training smoother and more stable. 

intersections are very complex. Each vehicle must decide In pre-training, supervised margin loss, T D\(λ\) \[16\], and when to cross the intersection or slow down to give way, regularization term are used to imitate the demonstrator while avoiding overfitting. The experimental results show that the

\*This work is supported by the National Key Research and Development number of interaction samples required by our proposed Program of China \(No. 2021YFB2501104, No. 2020YFA0711402\) method QMIXwD is much smaller than that of QMIX, All the authors are with the Department of Computer Science and which confirms the effectiveness of our method in improving Technology, Tongji University, China and the MOE Key Lab of Embedded System and Service Computing, Tongji University, China, e-mail: \(zhaojun-exploration and sample efficiency. 

qiao@tongji.edu.cn\)

One of the contributions of this paper is that we use Authorized licensed use limited to: Universitaetsbibliothek Regensburg. Downloaded on January 21,2025 at 15:15:14 UTC from IEEE Xplore. Restrictions apply. 





Downloaded from https://iranpaper.ir https://www.tarjomano.com

https://www.tarjomano.com

a MARL method based on value decomposition to learn C. RL from demonstrations

control policies for multiple vehicles at intersections and combine it with learning from demonstrations to efficiently Reinforcement learning from demonstrations \(RLfD\) is improve exploration. Another contribution is that we expera combination of RL and imitation learning and was first imentally prove that using self-generated interaction data in introduced in \[20\], which shows how learning from demon-the demonstration data improves training stability. 

strations is beneficial for the RL process. Deep Q-learning from demonstrations \(DQfD\) \[12\] and DDPG from demonstrations \(DDPGfD\) \[13\] respectively combine DQN and II. RELATED WORKS

DDPG with learning from demonstrations and significantly A. Policy learning at intersections accelerate the training process. \[21\] applies a similar method in soft actor-critic structure and has validated its effectiveness A lot of work has focused on policy learning of one in a simulated urban roundabout scenario under a single-single vehicle at intersections. \[1\] calculates time-to-collision agent setting. \[14\] extends RLfD in a multi-agent actor-critic \(TTC \[17\]\) with the assumption that surrounding vehicles method for flocking control. Instead, we adopt RLfD on a move at a constant speed to judge whether it is safe to value decomposition MARL method and propose to use self-cross the intersection. In \[2\], a partially observable markov generated interaction data in pre-training. 

decision process \(POMDP\) model is constructed to estimate the intention of other vehicles, and the model is combined with a planning method to generate an optimal trajectory. 

III. METHODOLOGY

\[3\] proposed a driving policy based on spatial and temporal attention to handle unprotected intersections using RL. 

A. Problem definition and revisiting value decomposition In order to achieve more complex interactive behaviors and 1\) Dec-POMDP: The multi-agent decision process at an further improve traffic efficiency at intersections, other re-intersection can be modeled as a decentralized partially searchers adopt MARL methods. \[4\] developed Coordinated observable markov decision process \(Dec-POMDP \[22\]\) Policy Optimization \(CoPO\) to coordinate agents’ behaviors consisting of a tuple G =< S, A, O, Ω, P, r, n, γ >. s ∈ S

while still maximizing individual objectives. Each agent is the global state. After performing the joint action of n receives its own reward and can become competitive with agents a ∈ A, a = \{a

others, which is not adaptable to our fully-cooperative task. 

1, ..., an\}, a transition from s to the state at the next moment s′ occurs according to the state

\[5\] incorporates a vehicle selection method into MADDPG

transition function P \(s′|s, a\) and all agents get a shared

\[18\] to cooperate between different numbers of vehicles. 

reward r\(s, a\) ∈

But they conducted experiments on a simplified intersection R. In partially observable scenarios, each agent can only obtain the observation o scenario where vehicles only go straight. \[6\] applies QMIX

i ∈ Ω of part of the

environment according to the observation function O\(o to control various autonomous vehicles in mixed-autonomy i|s, a\)

and has an individual policy π

traffic of different densities. The method in \[6\] is most i\(ai|τi\) where τi is the action-observation history. The objective is to find an optimal joint relevant to ours but it only applies some implementation policy π = \{π

techniques including Q\(λ\), reward clipping, and network-1, ..., πn\} to maximize the joint value function V π\(s\) =

γtr

level improvements on QMIX. In contrast, our method effec-E\[P∞

t=0

t\]. 

tively encourages exploration and greatly improves sample 2\) Value decomposition: Value decomposition based on efficiency. 

centralized training and decentralized execution \(CTDE\) framework is one of the most commonly used methods in MARL. Global state information is provided during training B. Multi-agent exploration

to get a more accurate joint action value function Qjt\(s, a\) Many methods considering interaction information have while individual action value function Qi\(τi, ai\) only re-been proposed to encourage exploration. EMC \[19\] encour-ceives its own observation to achieve full decentralization. 

ages collaborative exploration with intrinsic reward calcu-Qjt can be decomposed into Qi and is trained by the TD

lated by the prediction error of individual action values and error. 

introduces episodic memory to efficiently utilize explored b

informative experience to speed up policy training. MAVEN

X

LT D =

\[\(y − Qjt\(s, a\)\)2\]

\(1\)

\[8\] uses mutual information between the trajectories of i=1

agents to capture the interaction relationship to achieve coordinated exploration. Other methods such as causal influence where y = r \+ γmaxa′ Qjt\(s′, a′\) and b is the mini-batch

\[9\] and social influence \[10\] are proposed to model the size. However, the complexity of calculating target y is too interaction process. Rather than modeling the interactions high for practical application since the joint action space explicitly or implicitly, our approach directly imitates the grows exponentially with the number of agents. In order to demonstrator to perform high-quality interactive behaviors. 

achieve decentralized execution while reducing the computa-Exploration is encouraged by improving the initial perfor-tional complexity, individual-global-max \(IGM\) condition is mance of the algorithm to avoid premature convergence due proposed, where a global argmax performed on Qjt yields to insufficient exploration in the early stage of training. 

the same result as a set of individual argmax operations Authorized licensed use limited to: Universitaetsbibliothek Regensburg. Downloaded on January 21,2025 at 15:15:14 UTC from IEEE Xplore. Restrictions apply. 





Downloaded from https://iranpaper.ir https://www.tarjomano.com

https://www.tarjomano.com

performed on each Qi:

algorithm. We use T D\(λ\) to balance short-term and long-term rewards:

arg max Qjt\(s, a\) =

b

a∈A

\(2\)

X

\(arg max Q

LT D\(λ\) =

\[\(Gλ − Qjt\(s, a\)\)2\]

\(6\)

1\(τ1, a1\), ..., arg max Qn\(τn, an\)\) t

a1∈A

an∈A

i=1

3\) QMIX: QMIX is a state-of-the-art algorithm in value where Gλ

t is T D\(λ\) at time t. 

decomposition MARL. It adds monotonicity constraints Finally, a regularization item is added to prevent the through a mixing network generated based on global state algorithm from overfitting in the pre-training stage, making through a hypernet to meet the IGM condition. 

it easier to adapt to subsequent online training stage. This regularization term is applied to both the individual action QQMIX = f

jt

mixing \(Q1, ..., Qn; s\)

\(3\)

value network and the mixing network: b

∂QQMIX \(s, a\)

X

jt

L

\(||θ||2\)

\(7\)

≥ 0, i = 1, ..., n

\(4\)

L2 =

2

∂Qi\(τi, ai\)

i=1

where θ is the network parameters of QMIX. 

B. MARL from demonstrations

The overall loss for updating the network parameters is: At present, QMIX and other value decomposition MARL

methods most commonly use individual ϵ-greedy exploration Ltot = λ1Ldemo \+ λ2LT D\(λ\) \+ λ3LL2

\(8\)

techniques, which cannot effectively deal with complex Only LT D\(λ\) is considered during online training and the interactive scenarios \[23\]. We therefore propose to add a demonstration data is no longer used. The online training pre-training stage to learn from demonstrations to improve process is exactly the same as the original QMIX. Individual exploration and name our method QMIXwD. 

action value function shares parameters across all agents. 

Unlike previous methods where the demonstration data The whole algorithm is presented in Algorithm 1. 

used for pre-training was a fixed dataset completely generated by experts, we propose to use both expert data and Algorithm 1 QMIXwD

self-generated interaction data to improve the stability of the 1: Inputs: θ: weights for individual action value network, algorithm. This will, on the one hand, avoid the algorithm ϕ: weights for mixing network, η: expert data ratio in overfitting to the expert policy and thus save time in the demonstration data, E: demonstration data buffer size, subsequent online training stage for correcting the Q-values. 

k: number of pre-training gradient updates, m: number On the other hand, the interaction data generated by the of online-training gradient updates currently trained algorithm can reduce the distributional shift 2: collect E episodes using pre-trained expert and initial-between the current policy and the policy in demonstration ized θ according to η and store them in buffer Ddemo. 

data, which further stabilizes the transition from pre-training 3: for steps ← 1, k do

to subsequent online training. 

4:

Sample a mini-batch from Ddemo

The pre-training loss function consists of three parts, 5:

Update θ and ϕ using loss Ltot

supervised margin loss, TD error and regularization term. 

6: end for

Supervised margin loss makes the action of policy execution 7: for steps ← 1, m do

as consistent as possible with the demonstration data and 8:

episode data ← \[\]

is a key component to improve the initial performance of 9:

reset environment status

the algorithm. It will only be applied to the individual 10:

while episode not terminated do

action value function. Keeping max Qi\(τi, a\) constant after 11:

each agent i selects action a

a∈A

i based on ϵ-greedy

gradient update by supervised margin loss helps improve 12:

play actions a and observe \(s′, r\) training stability as max Qi\(τi, a\) is used when calculating 13:

append \(s, o, a, s′, o′, r\) into episode data a∈A

TD target y. Therefore, in order to imitate the demonstrator 14:

end while

while trying to stabilize training, we design the supervised 15:

store episode data in buffer D

margin loss to ensure that the maximum individual action 16:

Sample a mini-batch from D

value remains as constant as possible: 17:

Update θ and ϕ using loss LT D\(λ\) 18:

Update target networks

Ldemo = \[Qi\(τi, ai\) − \(max Qi\(τi, a\) − l\(ademo, ai\)\]2 \(5\) 19: end for

a∈A

where ademo is the actual action in demonstration data and l\(a

IV. EXPERIMENTS AND ANALYSIS

demo, ai\) is a margin function that is 0 when ai = ademo and a positive constant otherwise. 

In this section, we test the effectiveness of the proposed TD error is designed to satisfy the Bellman equation to method. We designed an intersection scenario where careful achieve a better transition between the pre-training stage and interactions are required to successfully cross the intersec-the online training stage and improve the stability of the tion. 

Authorized licensed use limited to: Universitaetsbibliothek Regensburg. Downloaded on January 21,2025 at 15:15:14 UTC from IEEE Xplore. Restrictions apply. 





Downloaded from https://iranpaper.ir https://www.tarjomano.com

https://www.tarjomano.com

Therefore, we sought to enhance the exploration ability of the algorithm, with the intention of learning complex interaction skills. 

B. Baseline

1\) TTC: We design a rule-based strategy based on TTC

for multiple agents. For agent i, we calculate the predicted trajectory for a period of time along the predetermined route, while for other agents, we still assume that the intention is Fig. 1. 

An intersection scenario in highway-env. 

unknowable and predict the future trajectory with constant speed and heading angle. If it can be inferred that agent i will collide with other agents following the predicted trajectories A. Scenario setting

and TTC is less than a certain threshold, agent i will perform deceleratee action, otherwise, it will perform accelerate 1\) Task: We conduct our experiments in the highway-action. 

env environment \[24\]. The scenario is a crossroad with one 2\) PPO: PPO is a state-of-the-art reinforcement learning vehicle coming from each direction as shown in Figure 1. 

method and it has excellent adaptability in a multi-agent To meet the condition of decentralized execution, each environment \[26\]. Each vehicle is controlled by a PPO agent vehicle is controlled individually by a policy. Each vehicle is and receives the same global reward. 

initialized with a predetermined route with a random choice 3\) QMIX: All agents share the parameters of individual of turning left, going straight, or turning right. Vehicles action value network but receive their own partial observation will travel along the predetermined routes, and the policies to make decisions. 

need to control the speed. The goal of this task is that all vehicles will cross the intersection and reach their respective C. Experiment and algorithm setup destinations as quickly and safely as possible. 

At the beginning of each episode, a vehicle is generated 2\) State: The global state consists of the position \(x, y\), at a distance of \(60 \+ 5 × N \(0, 1\)\)m from the intersection velocity \(vx, vy\), heading angle \(ψ\), and the future trajectory on each entry road and the travel direction is randomly set. 

\{x0, y0, ..., xT , yT \} within a period of time calculated with a An episode has a maximum of 100 time steps. When all constant speed of every vehicle following the predetermined vehicles leave the intersection 25m away without collision, routes. 

the episode is marked as done and is successfully completed. 

3\) Observation: The observation contains the information If there is a collision, the episode terminates immediately. 

of ego-vehicle including position, velocity, heading angle, The policy execution frequency is 5Hz. For each run of a and the future trajectory which are identical to their coun-method, we test for 20 episodes with each agent performing terparts in the global state. At the same time, the current greedy decentralized action selection every 20000 steps. 

relative position, relative velocity, and relative heading angle Each result is averaged under 3 different seeds with 95%

of surrounding vehicles can be observed. Under the partially confidence intervals. 

observable setting, we assume that the intention and future We design our algorithm based on the pymarl2 framework trajectories of surrounding vehicles are not available. 

\[27\]. QMIX and QMIXwD both use the basic ϵ-greedy 4\) Action: The steering wheel angle of the vehicle is exploration technique. We tried to set a high initial ϵ which calculated by the built-in control algorithm following the decays over time but we found that the performance of this predetermined route, so we set the discrete action space as method is very sensitive to the decay time parameter and the

\{decelerate, constant speed, accelerate\}. 

final performance varies greatly among different experiment 5\) Reward: The reward consists of three parts including trials. Thus we set ϵ to 0.1 in the whole training process a success reward, a collision penalty, and a time penalty to to reduce the interference of this factor on our proposed encourage all vehicles to cross the intersection safely and method. The expert data in demonstration data is generated quickly:

by a trained QMIX model. For the sake of fair comparison, the number of steps contained in the demonstration data is

100, if all vehicles cross the intersection safely



also counted into the final result curve. The demonstration



r =

− 100, if there is a collision

\(9\)

data contains 100 episodes of expert data and 900 episodes of



0, otherwise

interaction data. The size of replay buffer for online training is 5000 episodes. 

Positive rewards are only earned when the task is successfully completed, so it is a sparse reward problem that D. Main experiment

requires a greater exploration ability. Adopting dense rewards We tested our proposed method and other baselines at at each step can guide the agent to learn basic skills faster, unsignalized intersections, as shown in Figure 1. The success but engineering such a dense reward is not straightforward. 

rate, episodic return, and the average travel time needed in Meanwhile, a sparse reward setting makes it more likely that successful episodes are used as performance indicators. The the algorithm will learn a variety of high-level skills \[25\]. 

results are shown in Figure 2. 

Authorized licensed use limited to: Universitaetsbibliothek Regensburg. Downloaded on January 21,2025 at 15:15:14 UTC from IEEE Xplore. Restrictions apply. 





Downloaded from https://iranpaper.ir https://www.tarjomano.com

https://www.tarjomano.com

Fig. 3. 

Test success rate for ablation methods of the algorithm converging to a suboptimal policy and also saves time for subsequent corrections. The training curves of QMIX and QMIXwD show that QMIXwD requires nearly 30% fewer interaction samples than QMIX, which means the pre-training process with demonstration data can help the agent learn the appropriate policy faster and significantly improve sample efficiency. However, QMIXwD requires more travel time than TTC because QMIXwD tends to adopt a more cautious strategy overall to deal with complex interactions. The question of how to learn more efficient policies while ensuring safety remains to be addressed. 

E. Ablation study

We also designed ablation experiments to test the effec-Fig. 2. 

Test success rate, test episodic return, and test travel time for each tiveness of each part of our proposed method. 

algorithm. The performance of TTC is averaged over 5M steps and shown 1\) no-BC: The supervised margin loss term is canceled. 

as a straight line. 

2\) no-TD: The T D\(λ\) loss term for pre-training is canceled. 

3\) ratio-0.5, ratio-1: The proportions of expert data in the The travel time for TTC to cross the intersection is demonstration data are 0.5 and 1 respectively. 

short but the success rate is very low. The reason is that 4\) fixed-demo: The demonstration data is fixed and the when only a small number of vehicles’ trajectories intersect, interaction data is generated by a randomly initialized model. 

the TTC algorithm needs to slow down appropriately to The test success rate results are shown in Figure 3. 

avoid collision. But when it is necessary to deal with the Experimental results show that each part of our proposed interactions between multiple vehicles or all vehicles enter method is meaningful. Canceling the supervised margin loss the intersection at the same time, TTC cannot determine item or T D\(λ\) hurts the performance of the algorithm and which vehicle has the priority to cross the intersection first T D\(λ\) is particularly important because after pre-training and is unable to handle this situation. 

the individual action value functions by supervised margin PPO fails to learn an effective policy in this complex loss, there is a severe mismatch between current joint action situation. Because from the perspective of a single agent, value estimates and the return in the episodes collected by other agents are part of the environment and such an envi-individual agents, which seriously hinders training. From ronment is non-stationary \[18\], which will seriously hinder the training curves of experiments with different proportions the training of the algorithm. Especially in this kind of of expert data or fixed demonstration data, it can be seen intersection scenario with complex interactions, the actions that both ratio-1 and fixed-demo suffer from huge variance of ego-vehicle are often punished due to collisions between during training and the final performance is far worse than other vehicles, which gives a wrong signal to its own policy QMIXwD, indicating that the smooth transition achieved learning and greatly increases the variance and instability in by self-generated interaction data between pre-training and the training process. 

online training is of great benefit to improving the stability The success rate of QMIX once dropped to around 0 at of the algorithm. 

about 1M steps and the learned policy avoids interacting with other vehicles to reduce collisions at that time. After a period V. CONCLUSIONS

of exploration by ϵ-greedy it jumps out of the local optimal In this paper, we propose QMIXwD, combining the value solution. 

decomposition method QMIX and learning from demon-QMIXwD can obtain high-reward samples more easily in stration, to improve exploration and sample efficiency at the early stage of training process, which reduces the risk complex unsignalized intersections. Dynamic demonstration Authorized licensed use limited to: Universitaetsbibliothek Regensburg. Downloaded on January 21,2025 at 15:15:14 UTC from IEEE Xplore. Restrictions apply. 





Downloaded from https://iranpaper.ir https://www.tarjomano.com

https://www.tarjomano.com

data and self-generated interaction data are used to im-

\[15\] S. Fujimoto, D. Meger, and D. Precup, “Off-Policy Deep Rein-prove the stability of the algorithm. Supervised margin loss, forcement Learning without Exploration,” in Proceedings of the 36th International Conference on Machine Learning. 

PMLR, May 2019, 

T D\(λ\), and regularization loss also help to achieve a smooth pp. 2052–2062. 

transition between pre-training and online training while

\[16\] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Intro-imitating the demonstrator. Experimental results demonstrate duction, ser. Adaptive Computation and Machine Learning Series. 

Cambridge, Massachusetts: The MIT Press, 2018. 

that our proposed method can significantly speed up training. 

\[17\] A. Van der Horst and J. Hogema, “Time-to-collision and collision In the future, we will research on improving the scalability avoidance systems,” Verkeersgedrag in Onderzoek, 1994. 

of the algorithm to handle scenarios with more vehicles and

\[18\] R. Lowe, Y. WU, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch, 

“Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Envi-try to learn a policy that can adaptively pursue high effi-ronments,” in Advances in Neural Information Processing Systems, ciency in simple scenarios while ensuring safety in complex vol. 30. 

Curran Associates, Inc., 2017. 

scenarios. 

\[19\] L. Zheng, J. Chen, J. Wang, J. He, Y. Hu, Y. Chen, C. Fan, Y. Gao, and C. Zhang, “Episodic Multi-agent Reinforcement Learning with Curiosity-driven Exploration,” in Advances in Neural Information R

Processing Systems, vol. 34. Curran Associates, Inc., 2021, pp. 3757–

EFERENCES

3769. 

\[20\] S. Schaal, “Learning from Demonstration,” in Advances in Neural

\[1\] A. Cosgun, L. Ma, J. Chiu, J. Huang, M. Demir, A. M. A˜non, T. Lian, Information Processing Systems, vol. 9. 

MIT Press, 1996. 

H. Tafish, and S. Al-Stouhi, “Towards full automated drive in urban

\[21\] H. Liu, Z. Huang, J. Wu, and C. Lv, “Improved Deep Reinforcement environments: A demonstration in GoMentum Station, California,” in Learning with Expert Demonstrations for Urban Autonomous Driv-2017 IEEE Intelligent Vehicles Symposium \(IV\), June 2017, pp. 1811–

ing,” in 2022 IEEE Intelligent Vehicles Symposium \(IV\), June 2022, 1818. 

pp. 921–928. 

\[2\] C. Xia, M. Xing, and S. He, “Interactive Planning for Autonomous

\[22\] F. A. Oliehoek and C. Amato, A Concise Introduction to Decentralized Driving in Intersection Scenarios Without Traffic Signs,” IEEE Trans-POMDPs. 

Springer, June 2016. 

actions on Intelligent Transportation Systems, pp. 1–11, 2022. 

\[23\] H. T. Tse and H.-f. Leung, “Exploiting semantic epsilon greedy

\[3\] H. Seong, C. Jung, S. Lee, and D. H. Shim, “Learning to Drive at exploration strategy in multi-agent reinforcement learning,” arXiv Unsignalized Intersections using Attention-based Deep Reinforcement preprint arXiv:2201.10803, 2022. 

Learning,” in 2021 IEEE International Intelligent Transportation Sys-

\[24\] E. Leurent, “An environment for autonomous driving decision-tems Conference \(ITSC\). 

Indianapolis, IN, USA: IEEE, Sept. 2021, making,” GitHub, 2018. 

pp. 559–566. 

\[25\] T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, and I. Mordatch, 

\[4\] Z. Peng, Q. Li, K. M. Hui, C. Liu, and B. Zhou, “Learning to Simulate

“Emergent complexity via multi-agent competition,” arXiv preprint Self-driven Particles System with Coordinated Policy Optimization,” in arXiv:1710.03748, 2017. 

Advances in Neural Information Processing Systems, vol. 34. Curran

\[26\] C. Yu, A. Velu, E. Vinitsky, Y. Wang, A. Bayen, and Y. Wu, “The Associates, Inc., 2021, pp. 10 784–10 797. 

surprising effectiveness of ppo in cooperative, multi-agent games,” 

\[5\] T. Wu, M. Jiang, and L. Zhang, “Cooperative Multiagent Deep arXiv preprint arXiv:2103.01955, 2021. 

Deterministic Policy Gradient \(CoMADDPG\) for Intelligent Con-

\[27\] J. Hu, S. Jiang, S. A. Harding, H. Wu, and S.-w. Liao, “Rethinking nected Transportation with Unsignalized Intersection,” Mathematical the implementation tricks and monotonicity constraint in cooperative Problems in Engineering, vol. 2020, pp. 1–12, July 2020. 

multi-agent reinforcement learning,” arXiv preprint arXiv:2102.03479, 

\[6\] Z. Guo, Y. Wu, L. Wang, and J. Zhang, “Coordination for Connected 2021. 

and Automated Vehicles At Non-Signalized Intersections: A Value Decomposition-Based Multiagent Deep Reinforcement Learning Approach,” IEEE Transactions on Vehicular Technology, pp. 1–11, 2022. 

\[7\] T. Yang, H. Tang, C. Bai, J. Liu, J. Hao, Z. Meng, P. Liu, and Z. Wang, 

“Exploration in deep reinforcement learning: a comprehensive survey,” 

arXiv preprint arXiv:2109.06668, 2021. 

\[8\] A. Mahajan, T. Rashid, M. Samvelyan, and S. Whiteson, “MAVEN: Multi-Agent Variational Exploration,” in Advances in Neural Information Processing Systems, vol. 32. 

Curran Associates, Inc., 2019. 

\[9\] T. Zhang, Z. Liu, Z. Pu, and J. Yi, “Peer Incentive Reinforcement Learning for Cooperative Multi-Agent Games,” IEEE Transactions on Games, pp. 1–14, 2022. 

\[10\] N. Jaques, A. Lazaridou, E. Hughes, C. Gulcehre, P. Ortega, D. Strouse, J. Z. Leibo, and N. D. Freitas, “Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,” 

in Proceedings of the 36th International Conference on Machine Learning. 

PMLR, May 2019, pp. 3040–3049. 

\[11\] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and S. Whiteson, “Monotonic value function factorisation for deep multi-agent reinforcement learning,” The Journal of Machine Learning Research, vol. 21, no. 1, pp. 178:7234–178:7284, June 2022. 

\[12\] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A. Sendonaris, I. Osband, G. Dulac-Arnold, J. Agapiou, J. Leibo, and A. Gruslys, “Deep Q-learning From Demonstrations,” Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, Apr. 2018. 

\[13\] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess, T. Rothörl, T. Lampe, and M. Riedmiller, “Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards,” arXiv preprint arXiv:1707.08817, 2017. 

\[14\] Y. Qiu, Y. Zhan, Y. Jin, J. Wang, and X. Zhang, “Sample-Efficient Multi-Agent Reinforcement Learning with Demonstrations for Flocking Control,” in 2022 IEEE 96th Vehicular Technology Conference \(VTC2022-Fall\), Sept. 2022, pp. 1–7. 

Authorized licensed use limited to: Universitaetsbibliothek Regensburg. Downloaded on January 21,2025 at 15:15:14 UTC from IEEE Xplore. Restrictions apply.



