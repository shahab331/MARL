<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Multi-Agent Reinforcement Learning for Connected and Automated Vehicles Control: Recent Advancements and Future Prospects</title><meta name="author" content="Min Hua"/><meta name="description" content="IEEE Transactions on Automation Science and Engineering;2025;22; ;10.1109/TASE.2025.3574280"/><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 24pt; }
 .s2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s4 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .h3, h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s5 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s6 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s7 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s8 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 29.5pt; }
 .a { color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s9 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s10 { color: black; font-family:"Lucida Sans", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s11 { color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s12 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s13 { color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s14 { color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s15 { color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s16 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s17 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s18 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s19 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s20 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -8pt; }
 .s21 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s22 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -6pt; }
 .s23 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s24 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s25 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s26 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 9pt; }
 .s27 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 8pt; }
 .s28 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s29 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s30 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s31 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s32 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -3pt; }
 .s33 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s34 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s35 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s36 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 11pt; }
 .s37 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -1pt; }
 .s38 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -3pt; }
 .s39 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -5pt; }
 .s40 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -7pt; }
 .s41 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s42 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -3pt; }
 .s43 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 6pt; }
 .s44 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s45 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -2pt; }
 .s46 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 6pt; }
 .s48 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s49 { color: #004392; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s50 { color: #004392; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s51 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s52 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 1pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 0; }
 #l1> li:before {counter-increment: c1; content: counter(c1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l2 {padding-left: 0pt;counter-reset: d1 2; }
 #l2> li:before {counter-increment: d1; content: counter(d1, upper-roman)". "; color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l3 {padding-left: 0pt;counter-reset: d2 1; }
 #l3> li:before {counter-increment: d2; content: counter(d2, upper-roman)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l4 {padding-left: 0pt;counter-reset: e1 0; }
 #l4> li:before {counter-increment: e1; content: counter(e1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l5 {padding-left: 0pt;counter-reset: e2 0; }
 #l5> li:before {counter-increment: e2; content: counter(e2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l6 {padding-left: 0pt;counter-reset: e2 0; }
 #l6> li:before {counter-increment: e2; content: counter(e2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l7 {padding-left: 0pt; }
 #l7> li:before {content: "• "; color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l8 {padding-left: 0pt;counter-reset: g1 0; }
 #l8> li:before {counter-increment: g1; content: counter(g1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l9 {padding-left: 0pt;counter-reset: g2 0; }
 #l9> li:before {counter-increment: g2; content: counter(g2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l10 {padding-left: 0pt;counter-reset: g2 0; }
 #l10> li:before {counter-increment: g2; content: counter(g2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l11 {padding-left: 0pt;counter-reset: g2 0; }
 #l11> li:before {counter-increment: g2; content: counter(g2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l12 {padding-left: 0pt;counter-reset: h1 0; }
 #l12> li:before {counter-increment: h1; content: counter(h1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l13 {padding-left: 0pt;counter-reset: h2 0; }
 #l13> li:before {counter-increment: h2; content: counter(h2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l14 {padding-left: 0pt;counter-reset: h2 0; }
 #l14> li:before {counter-increment: h2; content: counter(h2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l15 {padding-left: 0pt;counter-reset: h2 0; }
 #l15> li:before {counter-increment: h2; content: counter(h2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l16 {padding-left: 0pt;counter-reset: h2 0; }
 #l16> li:before {counter-increment: h2; content: counter(h2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l17 {padding-left: 0pt;counter-reset: i1 0; }
 #l17> li:before {counter-increment: i1; content: counter(i1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
</style></head><body><p class="s1" style="padding-left: 7pt;text-indent: 0pt;text-align: center;"><a name="bookmark0">Multi-Agent Reinforcement Learning for Connected and Automated Vehicles Control: Recent Advancements and Future Prospects</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_001.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_002.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_003.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_004.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_005.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_006.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_007.png"/></span></p><p class="s2" style="padding-top: 14pt;padding-left: 41pt;text-indent: -15pt;text-align: left;">Min Hua  , <i>Graduate Student Member, IEEE</i>, Xinda Qi  , <i>Member, IEEE</i>, Dong Chen  , <i>Member, IEEE</i>, Kun Jiang  , Zemin Eitan Liu  , Hongyu Sun, Quan Zhou  , <i>Member, IEEE</i>, and Hongming Xu</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: justify;"><i>Abstract</i>—Connected and automated vehicles (CAVs) have emerged as a potential solution to the future challenges of developing safe, e<span class="s5">ffi</span>cient, and eco-friendly transportation systems. However, CAV control presents significant challenges signifi- cant challenges due to the complexity of interconnectivity and coordination required among vehicles. Multi-agent reinforcement learning (MARL), which has shown notable  advancements in addressing complex problems in autonomous driving, robotics, and human-vehicle interaction, emerges as a promising tool to enhance CAV capabilities. Despite its potential, there is a notable absence of current reviews on mainstream MARL algorithms for CAVs.  To fill this gap, this paper o<span class="s5">ff</span>ers a comprehensive review of MARL’s application in CAV control. The paper begins with an introduction to MARL, explaining its unique advantages in handling complex and multi-agent scenarios. It then presents a detailed survey of MARL applications across various control dimensions for CAVs, including critical scenarios such as platooning control, lane-changing, and unsignalized inter- sections. Additionally, the paper reviews prominent simulation platforms essential for developing and testing MARL algorithms for CAVs. Lastly, it examines the current challenges in deploying MARL for CAV control, including safety, communication, mixed tra<span class="s5">ffi</span>c, and sim-to-real challenges. Potential solutions discussed include hierarchical MARL, decentralized MARL, adaptive inter- actions, and o<span class="s5">ffl</span>ine MARL. The work has been summarized in MARL  in  CAV  Control  Repository.</h3><h3 style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: right;"><i>Note to Practitioners</i>—This paper explores the application of MARL for controlling CAVs in complex tra<span class="s5">ffi</span>c scenarios such as platooning, lane-changing, and intersections. MARL provides an adaptive and decentralized control approach, o<span class="s5">ff</span>ering advantages over traditional rule-based and optimization-based methods in</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 7pt;padding-left: 13pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Received 30 August 2024; revised 12 February 2025 and 16 May 2025;</p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">accepted 25 May 2025. Date of publication 28 May 2025; date of current version 12 June 2025. This article was recommended for publication by Associate Editor X. Zhang and Editor J. Yi upon evaluation of the reviewers’ comments. <i>(Corresponding author: Hongming Xu.)</i></p><p class="s6" style="padding-left: 5pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">Min Hua, Hongyu Sun, and Hongming Xu are with the School of Engi- neering, University of Birmingham, B15 2TT Birmingham, U.K. (e-mail: h.m.xu@bham.ac.uk).</p><p class="s6" style="padding-left: 5pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">Xinda Qi is with the Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI 48824 USA.</p><p class="s6" style="padding-left: 5pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">Dong Chen is with the Agricultural and Biological Engineering, Mississippi State University, Starkville, MS 39762 USA.</p><p class="s6" style="padding-left: 5pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">Kun Jiang is with the School of Automation, Southeast University, Nanjing 210018, China.</p><p class="s6" style="padding-left: 5pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">Zemin Eitan Liu is with the Chemical and Petroleum Engineering Department, University of Pittsburgh, Pittsburgh, PA 15260 USA.</p><p class="s6" style="padding-left: 5pt;text-indent: 7pt;line-height: 9pt;text-align: justify;">Quan Zhou is with the School of Engineering, University of Birmingham, B15 2TT Birmingham, U.K., and also with the School of Automotive Studies, Tongji University, Shanghai 201804, China.</p><p class="s6" style="padding-left: 13pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Digital Object Identifier 10.1109<span class="s8">/</span>TASE.2025.3574280</p><h3 style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">handling dynamic interactions, improving tra<span class="s5">ffi</span>c flow, enhancing safety, and optimizing fuel e<span class="s5">ffi</span>ciency. The paper reviews state- of-the-art MARL algorithms and simulation platforms, serving as a resource for practitioners looking to implement these advanced control strategies. However, real-world deployment remains challenging due to communication reliability, real-time decision-making constraints, and the complexities of mixed tra<span class="s5">ffi</span>c environments involving both automated and human-driven vehi- cles. Future research should focus on ensuring robust inter-agent communication, developing safety-aware MARL frameworks, and addressing the sim-to-real transfer gap. This paper provides insights to help practitioners bridge the gap between research and deployment, facilitating the development of more scalable, adaptive, and reliable CAV control.</h3><p class="s4" style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: justify;">Index Terms<span class="h3">—Connected and automated vehicles, multi-agent reinforcement learning, intelligent transportation systems, vehicle control.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 92pt;text-indent: 0pt;text-align: left;">I. I<span class="s6">NTRODUCTION</span></p><h1 style="text-indent: 0pt;line-height: 28pt;text-align: left;">A</h1><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 22pt;text-align: right;"><a href="#bookmark161" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark1">DVANCEMENTS in automation, artificial intelligence (AI),  and  vehicular  communication  are  transforming transportation,  leading  to  the  emergence  of  connected  and automated  vehicles  (CAVs)  </a><span style=" color: #00F;">[1]</span>.  By  integrating  vehicle-to- vehicle (V2V) and vehicle-to-infrastructure (V2I) communica- tion, CAVs enable cooperative perception, real-time decision- making, and tra<span class="s9">ffi</span><a href="#bookmark162" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c optimization </a><span style=" color: #00F;">[2]</span>. However, achieving safe, e<span class="s9">ffi</span>cient, and scalable CAV control in complex and dynamic<a name="bookmark2">&zwnj;</a></p><p class="s11" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark163" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark3">environments remains a critical challenge </a>[3]<a href="#bookmark164" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[4]<span style=" color: #000;">.</span><a name="bookmark4">&zwnj;</a></p><p class="s11" style="padding-left: 5pt;text-indent: 9pt;text-align: right;"><a name="bookmark5"><span style=" color: #000;">Various control optimization strategies have been explored to enhance the coordination and e</span></a><span class="s9">ffi</span><a href="#bookmark165" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ciency of CAVs </a>[5]<a href="#bookmark166" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[6]<span style=" color: #000;">. Among these, conventional methods such as model predictive control  (MPC)  have  been  employed  to  coordinate  platoon behavior, minimize control delay, and reduce tra</span><span class="s9">ffi</span><a href="#bookmark167" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c oscilla- tions </a>[7]<a href="#bookmark168" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Additionally, non-convex optimal control problems, such as CAV coordination at intersections, have been tackled with semidefinite relaxation techniques </a>[8]<a href="#bookmark169" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. However, these approaches often depend on precise system modeling, which is not always available </a>[9]<a href="#bookmark170" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, and require substantial computational resources, making them impractical for real-time control </a>[10]<a href="#bookmark171" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Besides, reinforcement learning (RL) has gained increasing attention  within  the  research  field  due  to  its  outstanding abilities in addressing sequential decision-making tasks, such as gaming </a>[11]<a href="#bookmark172" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, robotics </a>[12]<a href="#bookmark173" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, behavioral planning </a>[13]<a href="#bookmark174" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, intelli- gent energy management </a>[14]<a href="#bookmark175" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[15]<span style=" color: #000;">. Similarly, AV control has begun exploring the potential of employing RL for various</span><a name="bookmark6">&zwnj;</a><a name="bookmark7">&zwnj;</a><a name="bookmark8">&zwnj;</a><a name="bookmark9">&zwnj;</a><a name="bookmark10">&zwnj;</a><a name="bookmark11">&zwnj;</a><a name="bookmark12">&zwnj;</a><a name="bookmark13">&zwnj;</a><a name="bookmark14">&zwnj;</a><a name="bookmark15">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 3pt;padding-left: 66pt;text-indent: 0pt;line-height: 9pt;text-align: center;">© <span class="s6">2025 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License.</span></p><p class="s6" style="padding-left: 66pt;text-indent: 0pt;line-height: 9pt;text-align: center;">For more information, see https:<span class="s8">//</span>creativecommons.org<span class="s8">/</span>licenses<span class="s8">/</span>by<span class="s8">/</span>4.0<span class="s8">/</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark16">tra</a><span class="s9">ffi</span><a href="#bookmark176" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c scenarios </a><span style=" color: #00F;">[16]</span><a href="#bookmark177" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. For instance, in </a><span style=" color: #00F;">[17]</span>, a deep deter- ministic policy gradient (DDPG)-based model for platooning control is introduced to enhance fuel economy, driving e<span class="s9">ffi</span>- ciency, and safety at signalized intersections through real-time optimization. This approach featuring an e<span class="s9">ff</span>ective reward func- tion, demonstrates strong performance under varying tra<span class="s9">ffi</span>c demands and tra<span class="s9">ffi</span>c light cycles with di<span class="s9">ff</span>erent durations. Tak- ing into account the role of HDVs in the control of AVs, Shi et al. employ a distributed proximal policy optimization (DPPO) control strategy that allows them to learn from and respond to the behavior of the HDVs, optimizing performance at both the local subsystem level and the broader mixed tra<span class="s9">ffi</span><a href="#bookmark178" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c context  </a><span style=" color: #00F;">[18]</span>. Qu et al. introduce a control approach utilizing DDPG algorithm aimed at reducing tra<span class="s9">ffi</span>c fluctuations and enhancing fuel economy. However, these RL-based approaches only consider a single agent (i.e., AV), making them poorly suited for multi-agent coordination and generalization in dense tra<span class="s9">ffi</span><a href="#bookmark179" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c environments </a><span style=" color: #00F;">[19]</span>. As autonomous driving moves toward multi-vehicle interaction and cooperative control, single-agent RL struggles with scalability, decentralized decision-making, and shared state information, limiting its e<span class="s9">ff</span>ectiveness in complex, dynamic tra<span class="s9">ffi</span>c scenarios.<a name="bookmark17">&zwnj;</a><a name="bookmark18">&zwnj;</a><a name="bookmark19">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark180" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark20">Multi-agent reinforcement learning (MARL) extends single- agent RL to settings where multiple agents interact and learn collectively. This innovative approach has found applications across a broad spectrum of fields. Notably, the gaming and simulation industry has utilized MARL to create more com- plex and interactive environments </a><span style=" color: #00F;">[20]</span><a href="#bookmark181" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Furthermore, within the realm of finance, MARL models are employed to simulate the dynamics of markets and the behaviors of agents </a><span style=" color: #00F;">[21]</span>. In tra<span class="s9">ffi</span>c control and management, MARL algorithms have been instrumental in optimizing tra<span class="s9">ffi</span><a href="#bookmark182" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c flows and reducing congestion. The realm of AVs and robotics has also  ben- efited greatly, with MARL enabling cooperative navigation and coordination among vehicles and facilitating collaborative tasks in AVs and robotics </a><span style=" color: #00F;">[22]</span>. Extended to CAV control, each vehicle acts as an independent agent that learns from its interactions with the surrounding environment, which includes other vehicles, tra<span class="s9">ffi</span>c signals, and road conditions. Each CAV learns not only to maneuver safely and e<span class="s9">ffi</span><a href="#bookmark183" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ciently on its own but also to cooperate and communicate with other CAVs </a><span style=" color: #00F;">[23]</span>. Therefore, MARL o<span class="s9">ff</span>ers a valuable strategy for improving the performance and e<span class="s9">ffi</span>ciency of CAVs.<a name="bookmark21">&zwnj;</a><a name="bookmark22">&zwnj;</a><a name="bookmark23">&zwnj;</a></p><p class="s11" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark24"><span style=" color: #000;">Several studies have empirically demonstrated that MARL outperforms single-agent RL in scenarios requiring multi- agent interaction, such as mixed tra</span></a><span class="s9">ffi</span><a href="#bookmark184" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c and cooperative vehicle control </a>[24]<span style=" color: #000;">. Unlike RL, which follows an inde- pendent decision-making framework, MARL enables policy coordination and shared decision-making, resulting in supe- rior scalability, e</span><span class="s9">ffi</span><a href="#bookmark185" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ciency, and safety in dense and dynamic environments </a>[25]<a href="#bookmark186" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. While </a>[26] <a href="#bookmark187" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">provides an overview of single- agent RL in CAV control, it highlights the limitations of independent decision-making in multi-agent environments, where interactions between vehicles significantly impact per- formance. These limitations underscore the necessity of MARL for cooperative lane-changing, intersection manage- ment, and on-ramps merging optimization. In </a>[27]<span style=" color: #000;">, the authors emphasize that existing safe RL methods designed</span><a name="bookmark25">&zwnj;</a><a name="bookmark26">&zwnj;</a><a name="bookmark27">&zwnj;</a></p><p class="s11" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark188" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark28">for single-agent RL cannot be directly applied to MARL scenarios, reinforcing that RL struggles to handle multi-agent coordination challenges in safety-critical CAV applications. Similarly, in </a>[28]<span style=" color: #000;">, MARL-based motion controllers are eval- uated against RL approaches for lane-changing,  showing that MARL achieves smoother lane changes, lower collision rates, and better adaptability to dynamic environments. These findings confirm that MARL outperforms RL when multiple autonomous agents need to be coordinated in an interactive tra</span><span class="s9">ffi</span><span style=" color: #000;">c environment. While MARL o</span><span class="s9">ff</span><a href="#bookmark178" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ers clear advantages, it also introduces additional computational and communication complexity, particularly in terms of information sharing during training and execution. A common concern is that MARL may require excessive connectivity and centralized training, making it impractical for real-world deployment. To mitigate these challenges, recent works have explored hierarchical MARL </a>[18]<a href="#bookmark180" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, graph-based coordination </a>[20]<a href="#bookmark189" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, and decentralized training techniques </a>[29]<span style=" color: #000;">, ensuring scalability without excessive information-sharing demands.</span><a name="bookmark29">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark30">In the context of CAV control, MARL has been increasingly adopted to optimize collective behaviors, o</a><span class="s9">ff</span><a href="#bookmark190" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ering a scalable and cooperative solution to real-world deployment challenges  </a><span style=" color: #00F;">[30]</span>. Connectivity within this cooperative deployment operates on two levels: V2V communication, which  enables  direct data sharing between vehicles, and V2I communication, which expands connectivity through technologies like vehicular ad hoc networks and Ethernet-based systems. Together, these communication channels ensure robust inter-agent communi- cation and situational awareness, both of which are critical for the scalability and e<span class="s9">ff</span><a href="#bookmark191" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ectiveness of CAVs </a><span style=" color: #00F;">[31]</span><a href="#bookmark34" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Fig. </a><span style=" color: #00F;">1 </span>illustrates a comprehensive multi-agent system, i.e., CAVs, including environmental data from sensors such as GPS<span class="s9">/</span>INS, cameras, radar, and lidar, along with V2I communication channels providing information from tra<span class="s9">ffi</span><a href="#bookmark192" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c signals, roadside units, and cloud systems. This data is fused to create a unified environmental representation. Each AV maintains a local data repository while accessing shared V2V and V2I information, enhancing decision-making and situational awareness </a><span style=" color: #00F;">[32]</span>. The decision-making and motion-planning modules process these inputs to generate optimal strategies for safe and e<span class="s9">ffi</span>cient navigation. These strategies are then executed through steering and powertrain control modules. By leveraging  both  V2V and V2I communication, the multi-agent system demonstrates how integrated connectivity enables robust coordination and scalability within the CAV ecosystem, ultimately laying the foundation for smarter and more e<span class="s9">ffi</span>cient transportation net- works.<a name="bookmark31">&zwnj;</a><a name="bookmark32">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark193" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark33">Despite advancements in MARL for CAV systems, sig- nificant challenges remain in adapting these techniques to real-world CAV control. Issues such as scalability, robustness in dynamic environments, and the integration of decentralized learning frameworks require further investigation. While a few articles describe the potential of MARL in CAVs </a><span style=" color: #00F;">[33]</span>, none of them, to the best of our knowledge, are specifically devoted to the application of MARL for CAV control. This paper is thus intended to deliver a comprehensive and systematic review of MARL within the realm of CAVs, such as lane changing, platooning  control,  tra<span class="s9">ffi</span>c  signal  cooperation,  and  on-ramp</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 70pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="515" height="241" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_008.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark34">Fig. 1.  The schematic of the multi-agent system for CAV control.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">merging. Through this review, we not only provide a clear, up- to-date understanding of the current classic MARL algorithms applied in CAV control, but we also outline the potential direction for future work in this area. The main contributions of the work presented in this paper are summarized as follows:</p><ol id="l1"><li style="padding-top: 1pt;padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">We survey the recent developments in MARL algo- rithms, discussing their diverse applications in aspects of CAV control based on the extent of control dimensions. Furthermore, we provide an extensive examination of the leading simulation platforms employed in MARL research for CAV control.</p></li><li style="padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">We highlight and explore the technical challenges that MARL faces in these applications and discuss potential research directions to address these challenges.</p></li><li style="padding-left: 30pt;text-indent: -14pt;text-align: left;"><p style="display: inline;">The work, including a well-classified list of relevant papers, has been presented at the following site: https:<span class="s9">// </span>github.com<span class="s9">/</span>huahuaedi<span class="s9">/</span>MARL  in  CAV  control  review.</p></li></ol><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark35" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">This paper is organized as follows: Section </a><span style=" color: #00F;">II </span>introduces the basics of RL and MARL; A detailed review of MARL applied in CAV control from  di<span class="s9">ff</span>erent  control  dimensions and mainstream simulation platforms are described in section</p><ol id="l2"><li style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><p class="s11" style="display: inline;"><a href="#bookmark148" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Section </a>IV <a href="#bookmark160" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">summarizes the remaining challenges and opportunities. Finally, Section </a>V <span style=" color: #000;">concludes the review.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l3"><li style="padding-left: 110pt;text-indent: -24pt;text-align: left;"><p style="display: inline;"><a name="bookmark35">M</a><span class="s6">ETHODOLOGIES</span></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">In this section, we begin with a comprehensive overview of the fundamental principles of RL. We then delve into various prominent MARL algorithms, establishing the context to enhance the understanding of our review.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l4"><li style="padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s12" style="display: inline;">Preliminaries of RL and MARL</p><p class="s9" style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark194" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark36">RL, typically formalized as a Markov Decision Process (MDP), provides a framework for sequential decision-making under uncertainty </a><span class="s11">[34]</span><span class="p">. An agent interacts with the environ- ment by taking actions and receiving rewards, aiming to learn an optimal policy that maximizes cumulative rewards. The MDP is defined as </span>M = <span class="p">(</span>S<span class="s13">, </span>A<span class="s13">, </span>P<span class="s13">, </span>R<span class="p">), where </span>S <span class="p">is the state space, </span>A <span class="p">the action space, </span>P <span class="p">the transition probability, and </span>R <span class="p">the reward function.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 1pt;text-indent: 9pt;text-align: justify;"><a name="bookmark37">Deep RL methods leverage neural networks to approximate value functions or policies, enabling applications in high- dimensional control problems such as autonomous driving. These methods can be broadly categorized into: </a><b>Value-based methods </b>(e.g., DQN, DDQN): learn the Q-function <i>Q</i>(<i>s</i><span class="s13">, </span><i>a</i>) to select actions via maximization; <b>Policy-based methods </b>(e.g., DDPG): optimize a parameterized policy <span class="s13">π</span><span class="s14">θ</span><span class="s15"> </span>directly using gradient ascent; <b>Actor-critic methods </b><a href="#bookmark195" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">(e.g., Advantage Actor- Critic (A2C) </a><span style=" color: #00F;">[35]</span>): combine policy and value function learning to improve stability and reduce variance.</p><p style="padding-left: 1pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark196" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark38">In multi-agent settings such as CAV control, MARL extends the RL paradigm to systems where multiple agents interact and jointly influence outcomes </a><span style=" color: #00F;">[36]</span>. Agents are modeled as nodes in a communication graph <span class="s9">G = </span>(<span class="s13">ν, ε</span>) and operate under decentralized and partially observable conditions, typically formalized as a partially observable MDP (POMDP).</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 14pt;text-indent: -13pt;text-align: left;"><p class="s12" style="display: inline;">Training and Execution Strategies in MARL</p><p style="padding-top: 3pt;padding-left: 1pt;text-indent: 9pt;text-align: justify;"><a name="bookmark39">In MARL settings, di</a><span class="s9">ff</span>erent approaches and frameworks are employed to tackle the complex challenges of coor- dinating and training agents in decentralized environments. This subsection explores two fundamental paradigms: Cen- tralized Training with Decentralized Execution (CTDE) and Decentralized Training with Decentralized Execution (DTDE), each o<span class="s9">ff</span>ering distinct insights and trade-o<span class="s9">ff</span>s in the pursuit of e<span class="s9">ff</span><a href="#bookmark197" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ective multi-agent learning </a><span style=" color: #00F;">[37]</span><a href="#bookmark198" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a><span style=" color: #00F;">[38]</span><a href="#bookmark199" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a><span style=" color: #00F;">[39]</span>.<a name="bookmark40">&zwnj;</a><a name="bookmark41">&zwnj;</a></p><ol id="l5"><li style="padding-left: 1pt;text-indent: 9pt;text-align: justify;"><p class="s12" style="display: inline;">Centralized Training With Decentralized Execution (CTDE): <a href="#bookmark42" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">In this framework, the decentralized problem is initially transformed into a centralized one, solvable by a central controller, which gathers essential training information, e.g., observation, reward, and global state information, for the training process. Following this data collection, the centralized value functions are learned based on the information of all agents, and then the gradient from the centralized value function is used to train the policy of each agent. But in the process of execution, each agent outputs action according to its individual observation (see Fig. </a><span class="s11">2</span><span class="p">).</span></p><p style="padding-left: 1pt;text-indent: 9pt;text-align: justify;">It is important to acknowledge the inherent trade-o<span class="s9">ff</span>s of the CTDE. One significant strength is the enhanced learning</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="265" height="196" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_009.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark42">Fig. 2. Illustration of centralized training with decentralized execution (CTDE) in MARL.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="264" height="193" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_010.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark43">Fig. 3. Illustration of decentralized training with decentralized execution (DTDE) in MARL.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark200" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark44">stability enabled by  centralized  training,  which  can  result in more robust policies </a>[40]<span style=" color: #000;">. However, it is crucial to rec- ognize that maintaining a centralized controller comes with its own set of challenges. Firstly, it can be prohibitively expensive and infeasible in certain scenarios, particularly in large-scale or resource-constrained environments. For exam- ple, in large-scale CAV networks such as urban tra</span><span class="s9">ffi</span><a href="#bookmark201" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c control  </a>[41] <a href="#bookmark202" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">or highway platooning </a>[42]<a href="#bookmark203" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, maintaining a centralized critic requires high bandwidth and computational resources, making real-time learning impractical. Similarly, resource constraints in edge-computing environments limit the fea- sibility of processing global information centrally, favoring decentralized approaches </a>[43]<span style=" color: #000;">. Additionally, the centralized controller introduces privacy concerns, as it necessitates the sharing of sensitive information among agents. Moreover, the central controller becomes a single point of failure, rendering the whole system vulnerable to disruptions. These trade-o</span><span class="s9">ff</span><span style=" color: #000;">s highlight the need for careful consideration of the CTDE framework’s suitability in specific MARL applications, taking into account the advantages and drawbacks it presents.</span><a name="bookmark45">&zwnj;</a><a name="bookmark46">&zwnj;</a><a name="bookmark47">&zwnj;</a></p></li><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s12" style="display: inline;">Decentralized Training With Decentralized Execution (DTDE): <span class="p">In DTDE, each agent learns and acts independently, relying solely on local observations and policy updates without a central controller. Unlike centralized or partially centralized frameworks, DTDE agents do not require global state informa- tion but instead learn through trial-and-error interactions with</span></p><p class="s11" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark204" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark48">the environment. A key challenge in DTDE is maintaining learning stability due to non-stationarity, as  agents’  poli- cies evolve independently over time. Traditional independent Q-learning (IQL) </a>[44] <a href="#bookmark205" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">and independent advantage actor-critic (IA2C) </a>[45] <a href="#bookmark206" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">are widely used but often struggle with conver- gence due to the lack of inter-agent coordination </a>[46]<span style=" color: #000;">. To mitigate non-stationarity, Zhang et al. proposed a fully decen- tralized MARL framework, where agents exchange messages with neighboring agents, improving coordination and o</span><span class="s9">ff</span><a href="#bookmark207" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ering theoretical convergence guarantees </a>[47]<a href="#bookmark43" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Fig. </a>3 <span style=" color: #000;">illustrates the DTDE framework based on the actor-critic architecture, where each agent independently updates its policy and critic function based on local interactions with the environment.</span><a name="bookmark49">&zwnj;</a><a name="bookmark50">&zwnj;</a><a name="bookmark51">&zwnj;</a></p></li><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p style="display: inline;"><a name="bookmark52"><i>O</i></a>ffl<i>ine Reinforcement Learning: </i>Beyond CTDE and DTDE, O<span class="s9">ffl</span>ine RL provides an alternative learning paradigm that relies exclusively on pre-collected datasets, eliminating the need for continuous environment interaction during training. Unlike traditional online RL, which explores and updates poli- cies dynamically, O<span class="s9">ffl</span><a href="#bookmark208" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ine RL learns solely from fixed datasets, making it particularly useful in scenarios where real-time exploration is expensive, unsafe, or impractical </a><span style=" color: #00F;">[48]</span>. However, a major challenge in O<span class="s9">ffl</span>ine RL is the issue of distributional shift, when the learned policy encounters states or actions that are insu<span class="s9">ffi</span><a href="#bookmark209" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ciently represented in the dataset, leading to suboptimal or unsafe decision-making. To address this, various techniques, such as policy regularization </a><span style=" color: #00F;">[49]</span><a href="#bookmark210" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, conservative Q-learning </a><span style=" color: #00F;">[50]</span><a href="#bookmark211" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, and behavior cloning </a><span style=" color: #00F;">[51]</span>, have been pro- posed to improve policy robustness and generalization. Given its potential for leveraging large-scale o<span class="s9">ffl</span>ine data, O<span class="s9">ffl</span>ine RL continues to be an active research area with broad applications across di<span class="s9">ff</span>erent domains.<a name="bookmark53">&zwnj;</a><a name="bookmark54">&zwnj;</a><a name="bookmark55">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s12" style="display: inline;">MARL Algorithm Variants</p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark60" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">In this subsection, we have exclusively reviewed the com- mon MARL algorithms that find utility in CAV applications. Further, the MARL algorithms are divided into four categories: value function decomposition, learning to communicate, hier- archical structure, and causal inference, which are rooted in the inherent complexity and diverse requirements of multi- agent environments, particularly in CAV control. Table </a><span style=" color: #00F;">I </span>o<span class="s9">ff</span>ers an extensive overview of key works from four distinct perspectives within various settings.</p><ol id="l6"><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p style="display: inline;"><a name="bookmark56"><i>Value Function Decomposition: </i></a>The challenge of credit assignment in cooperative settings has emerged as a significant area of research interest. The shared rewards in a fully cooperative environment make it di<span class="s9">ffi</span><a href="#bookmark229" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cult to distinguish the contribution of each agent, some agents tend to be lazy </a><span style=" color: #00F;">[72]</span>. To solve the above problem, some studies learn di<span class="s9">ff</span><a href="#bookmark230" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">erent value functions to distinguish the contribution of each agent. Foerster et al. </a><span style=" color: #00F;">[73] </span><a href="#bookmark231" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">and Guo et al. </a><span style=" color: #00F;">[74] </span>introduced the counterfactual baseline principle to learn di<span class="s9">ff</span><a href="#bookmark232" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">erent value functions by central- ized learning method in the context of a shared team reward environment. Additionally, Hou et al. have developed a credit allocation mechanism based on role attention, which facilitates the learning process of role policies in multi-agent systems by managing how credit is allocated among agents </a><span style=" color: #00F;">[75]</span>. Liu et al. construct the causal e<span class="s9">ff</span>ect of the agent’s actions on the<a name="bookmark57">&zwnj;</a><a name="bookmark58">&zwnj;</a><a name="bookmark59">&zwnj;</a></p><p class="s6" style="padding-top: 2pt;padding-left: 66pt;text-indent: 0pt;text-align: center;">TABLE I</p><p class="s6" style="padding-top: 2pt;padding-left: 66pt;text-indent: 0pt;text-align: center;"><a name="bookmark60">A</a><span class="s16">N </span>O<span class="s16">VERVIEW OF THE </span>P<span class="s16">RIMARY </span>W<span class="s16">ORKS FOR </span>MARL A<span class="s16">LGORITHMS</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="683" height="314" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_011.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark227" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark61">external state from the perspective of a causal diagram, solving the problem of lazy agents </a>[70]<span style=" color: #000;">.</span></p><p class="s11" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark233" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark62">Among the above methods to solve credit assignment, value function decomposition has remained a key approach for addressing credit assignment challenges in cooperative MARL by decomposing the joint value function into indi- vidual components. Building upon foundational methods like VDN </a>[76] <a href="#bookmark234" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">and QMIX </a>[77]<a href="#bookmark213" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, several advanced  algorithms have been developed to tackle increasingly complex sce- narios. For  instance,  the Value Decomposition  with  Graph Attention Networks (VGN) method employs graph attention networks to dynamically model agent relationships, improving coordination in cooperative tasks </a>[55]<a href="#bookmark214" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Similarly, the  MA- MIX algorithm integrates a multi-head attention mechanism to enhance information exchange and cooperation among agents </a>[57]<span style=" color: #000;">. These advancements underscore the ongoing evolution of value function decomposition techniques, further improving agent coordination, communication e</span><span class="s9">ffi</span><span style=" color: #000;">ciency, and adaptability in increasingly complex environments.</span><a name="bookmark63">&zwnj;</a></p></li><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s11" style="display: inline;"><a name="bookmark64"><span class="s12">Learning to Communicate: </span></a><a href="#bookmark215" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Research in MARL can be categorized into two distinct groups according to their methods of communication </a>[58]<span style=" color: #000;">. The first group operates without com- munication among agents and centers its e</span><span class="s9">ff</span><a href="#bookmark235" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">orts on stabilizing training by employing advanced value estimation methods. For example, MADDPG </a>[78] <a href="#bookmark230" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">extends DDPG to the MARL setting by employing a centralized critic network to enable the global value estimate. Similarly, COMA </a>[73] <a href="#bookmark236" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">modifies the actor-critic method for use in MARL, calculating the advantage function for each agent by employing a centralized critic in conjunction with a counterfactual baseline. The second group investigates explicit communication mechanisms, which can be predefined heuristic protocols or learnable communication strategies. Fin- gerPrint </a>[79] <span style=" color: #000;">stabilizes learning under non-stationary policies</span><a name="bookmark65">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark237" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark66">by sharing low-dimensional policy fingerprints among agents. DIAL </a><span style=" color: #00F;">[80] </span>enables di<span class="s9">ff</span>erentiable communication by having DQN-based agents simultaneously generate and process mes- sages via backpropagation. Further advancements have sought to optimize communication e<span class="s9">ffi</span><a href="#bookmark215" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ciency and reduce redundant message exchange. NeurComm </a><span style=" color: #00F;">[58] </span>proposes that encoding and concatenating communication signals, rather than simply aggregating them, e<span class="s9">ff</span><a href="#bookmark216" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ectively mitigates information loss and enhances coordination among agents. This approach allows for structured message processing, improving decision-making in dynamic environments. Similarly, IMAC </a><span style=" color: #00F;">[59] </span>formulates an e<span class="s9">ffi</span>cient communication protocol based on the information bottleneck principle, ensuring that agents only transmit the most relevant information to their peers. The framework devel- ops a di<span class="s9">ff</span>erentiable communication strategy, incorporating state, policy, and encoded neighboring information, which is then concatenated into the state space of nearby agents.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark217" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">More recent works have further expanded communication capabilities to improve interpretability, adaptivity, and robust- ness in MARL. Verco </a><span style=" color: #00F;">[60] </span><a href="#bookmark218" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">integrates large language models (LLMs) into MARL agents, enabling human-readable verbal communication that enhances coordination and transparency. Meanwhile, AsynCoMARL </a><span style=" color: #00F;">[61] </span>introduces graph transformer- based learning to dynamically adjust communication protocols in real time. By leveraging dynamic graph structures, agents can learn when and how to communicate, ensuring scalable and adaptive multi-agent cooperation. These developments mark a shift towards structured, e<span class="s9">ffi</span>cient, and learnable com- munication mechanisms. Instead of relying on predefined message-passing rules, modern approaches prioritize adaptive protocols, minimal yet e<span class="s9">ff</span>ective communication, and scalable coordination strategies, improving MARL performance in complex, multi-agent environments.</p></li><li style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s11" style="display: inline;"><a name="bookmark67"><span class="s12">Hierarchical Structure: </span></a><span style=" color: #000;">In MARL, hierarchical struc- tures play a crucial role in addressing challenges such as sparse rewards, complex decision-making, and limited transferability. Hierarchical MARL enables agents to learn decision-making strategies across di</span><span class="s9">ff</span><a href="#bookmark238" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">erent levels of temporal abstraction, mak- ing it particularly beneficial for large-scale coordination tasks. Existing hierarchical MARL methods can be broadly catego- rized into option-based hierarchies </a>[81]<a href="#bookmark239" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[82] <a href="#bookmark240" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">and goal-based hierarchies </a>[83]<a href="#bookmark241" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a>[84]<span style=" color: #000;">, both of which have been successfully applied to multi-agent cooperation.</span><a name="bookmark68">&zwnj;</a><a name="bookmark69">&zwnj;</a><a name="bookmark70">&zwnj;</a></p><p class="s11" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><span style=" color: #000;">Several hierarchical MARL frameworks have been intro- duced to enhance scalability, coordination, and e</span><span class="s9">ffi</span><a href="#bookmark219" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ciency. Hierarchical Skill Discovery (HSD) trains decentralized policies for high-level skill selection while simultaneously learning low-level execution policies, improving adaptability in dynamic environments </a>[62]<a href="#bookmark220" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Role-based Decomposition (RODE) further refines action space partitioning through action clustering,  ensuring  that  each  action  corresponds  to a distinct subspace, thereby reducing the need for extensive parameter tuning </a>[63]<a href="#bookmark221" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Hierarchical Multi-Agent Reinforce- ment Learning with PER-QMIX (H-MARL-PQ) combines multi-agent and single-agent RL through a hierarchical frame- work, integrating macro-operations and prioritized experience replay to accelerate learning in complex wargames with sparse rewards </a>[64]<a href="#bookmark222" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. To further improve hierarchical coordination, HierArchical Value dEcompositioN (HAVEN) introduces a dual-coordination mechanism that operates both within hierar- chical layers and across agents, optimizing strategy execution at multiple levels and enhancing policy stability </a>[65]<a href="#bookmark223" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Mean- while, Hierarchical Multi-Agent Skill Discovery (HMASD) employs a transformer-based structure for sequential skill assignment, allowing low-level policies to autonomously dis- cover and refine agent-specific skills, enhancing flexibility in cooperative decision-making </a>[66]<span style=" color: #000;">.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark242" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark71">Despite these advancements, hierarchical MARL  faces key challenges in designing dynamic hierarchical structures. Unlike static hierarchies with predefined roles, dynamic struc- tures evolve based on task complexity, agent interactions, and environmental changes, enabling adaptive role assignment, flexible coordination, and scalable policy learning </a><span style=" color: #00F;">[85]</span>. The primary challenges lie in optimizing inter-level coordination, ensuring e<span class="s9">ffi</span>cient policy transfer, and balancing local and global decision-making. Addressing these issues  is  crucial for improving MARL adaptability in large-scale, complex environments.</p></li><li style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s11" style="display: inline;"><a name="bookmark72"><span class="s12">Causal Inference: </span></a><span style=" color: #000;">There are multiple complex variables in MARL, and it is usually di</span><span class="s9">ffi</span><a href="#bookmark243" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cult to directly discover their internal relationships. Therefore, some recent research has begun to combine MARL with causal inference to discover the causal relationships between agents or variables, and further understand the operating mechanism of agents through intervention and inference, thereby motivating the agent to conduct more targeted learning </a>[86]<a href="#bookmark244" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Pearl’s three-level causal model is considered a powerful tool for constructing causal relationships between variables </a>[87]<span style=" color: #000;">. Causal influence reward methods promote the collaborative performance of agents by rewarding those agents that have a causal influence on the actions of other agents, where this causal influence is evaluated</span><a name="bookmark73">&zwnj;</a></p></li></ol></li></ol><p class="s11" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark245" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark74">by counterfactual reasoning </a>[88]<span style=" color: #000;">. Pina et al proposed the independent causal learning (ICL) algorithm to evaluate the causal e</span><span class="s9">ff</span><a href="#bookmark226" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ect of each agent’s observations on team collabora- tion performance and solve the credit allocation problem in the independent learning framework </a>[69]<span style=" color: #000;">.</span></p><p class="s11" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark75"><span style=" color: #000;">There are also other works that combine causal inference from di</span></a><span class="s9">ff</span><span style=" color: #000;">erent perspectives under the framework of MARL. LAIES mathematically define the concept of fully lazy agents and teams by calculating the causal e</span><span class="s9">ff</span><a href="#bookmark227" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ect of their actions on external states using the do-calculus process, which solved the sparse reward problem in MARL </a>[70]<span style=" color: #000;">. FD-MARL allows agents to modify communication messages by choosing the counterfactual that bears the most significant influence on others, the continuous communication based on causal analysis enables e</span><span class="s9">ffi</span><a href="#bookmark224" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient information transformation in a fully decen- tralized manner </a>[67]<span style=" color: #000;">. The deconfounded value decomposition (DVD) method investigates value function decomposition from the perspective of causal inference, which cuts o</span><span class="s9">ff </span><a href="#bookmark225" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">the back- door confounding path from the global state to  the  joint value function </a>[68]<a href="#bookmark228" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Additionally, deep motor system </a>[71] <span style=" color: #000;">introduces individual intention inference and causal reasoning to enable fully decentralized coordination, allowing agents to disentangle other agents from their environment and improve cooperative behavior without relying on CTDE. Causal infer- ence has also applied to some practical application scenarios, such as tra</span><span class="s9">ffi</span><a href="#bookmark246" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c signal control </a>[89] <a href="#bookmark247" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">and human-computer inter- action decision-making </a>[90]<span style=" color: #000;">.</span><a name="bookmark76">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 6pt;padding-left: 110pt;text-indent: -84pt;text-align: left;"><p style="display: inline;"><a name="bookmark77">T</a><span class="s6">OWARDS </span>A<span class="s6">PPLICATIONS OF </span>MARL <span class="s6">IN </span>CAV C<span class="s6">ONTROL</span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark78">CAV control has obtained substantial interest for its abil- ity to reshape modern transportation, enhancing e</a><span class="s9">ffi</span>ciency, safety, and sustainability. CAVs interact with multiple agents, enabling cooperative driving strategies such as platooning, merging, and intersection navigation, which collectively opti- mize tra<span class="s9">ffi</span><a href="#bookmark259" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c flow and reduce congestion </a><span style=" color: #00F;">[102]</span>. As CAV technology advances, the integration of MARL has become increasingly important, allowing vehicles to develop adaptive, cooperative policies for dynamic tra<span class="s9">ffi</span>c scenarios.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">To systematically explore how MARL contributes to CAV control, we categorize cooperative driving tasks based on the extent of required coordination, forming a taxonomy aligned with fundamental principles  of  CAV  control.  Coordination in CAV environments typically involves three key factors: <b>longitudinal control</b>, <b>lateral control</b>, and <b>timing constraints</b>. These aspects define the complexity of interaction, shaping how vehicles learn and adapt to shared environments. There- fore, we define three levels of cooperation complexity in MARL-driven CAV control.</p><ul id="l7"><li style="padding-top: 1pt;padding-left: 25pt;text-indent: -10pt;text-align: justify;"><h2 style="display: inline;">One-dimensional cooperation <span class="p">focuses on longitudinal control, where vehicles primarily regulate speed and fol- lowing distances, as seen in car-following and platooning</span></h2><p style="padding-left: 25pt;text-indent: 0pt;line-height: 11pt;text-align: left;">tasks.</p></li><li style="padding-left: 25pt;text-indent: -10pt;text-align: justify;"><h2 style="display: inline;">Two-dimensional cooperation <span class="p">extends this by introduc- ing lateral control, requiring inter-agent negotiation for lane changes, merging, and overtaking, which increases</span></h2><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">the complexity of coordination.</p><p class="s6" style="padding-top: 2pt;padding-left: 66pt;text-indent: 0pt;text-align: center;">TABLE II</p><p class="s6" style="padding-top: 2pt;padding-left: 66pt;text-indent: 0pt;text-align: center;">A<span class="s16">N </span>O<span class="s16">VERVIEW OF THE </span>P<span class="s16">RIMARY </span>W<span class="s16">ORKS </span>F<span class="s16">ROM </span>1-D <span class="s16">AND </span>2-D C<span class="s16">OOPERATION FOR </span>M<span class="s16">ARL </span>A<span class="s16">PPLIED IN </span>CAV C<span class="s16">ONTROL</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="652" height="492" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_012.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="302" height="158" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_013.jpg"/></span></p></li><li style="padding-left: 25pt;text-indent: -10pt;text-align: justify;"><p style="display: inline;"><a name="bookmark79"><b>Three-dimensional cooperation </b></a>incorporates time con- straints, encompassing tasks such as tra<span class="s9">ffi</span>c signal coordi- nation, intersection management, and on-ramp merging,</p></li></ul><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">where  spatial  and  temporal  synchronization  becomes essential.</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">We categorize MARL advancements by control components and review key simulation platforms for CAV applications, with a focus on cooperative control.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l8"><li style="padding-top: 3pt;padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s12" style="display: inline;">One-Dimensional Cooperation</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">In this subsection, we provide a detailed review of one- dimensional cooperation applications, primarily focusing on longitudinal control. A key example is <b>platooning control</b>, where CAVs operate in tightly coordinated formations to improve e<span class="s9">ffi</span>ciency, safety, and fuel economy. By minimizing aerodynamic drag and optimizing tra<span class="s9">ffi</span>c flow through syn- chronized control and inter-vehicle coordination, platooning exemplifies how MARL enhances the capabilities of CAVs.</p><ol id="l9"><li style="padding-left: 5pt;text-indent: 9pt;text-align: left;"><p class="s12" style="display: inline;">Application Scenarios: <a href="#bookmark80" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">As shown in Fig. </a><span class="s11">4</span><span class="p">, the problem</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 6pt;text-indent: 0pt;text-align: right;"><a name="bookmark80">Fig. 4. One-dimensional cooperation scenarios: the case of platooning control.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: right;">communication channels. This setup aims to ensure safe, fuel- e<span class="s9">ffi</span><a href="#bookmark260" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient, and smooth vehicle following operations, while also maximizing the advantages of driving in close formation </a><span style=" color: #00F;">[103]</span>. Given <i>N </i>vehicles in a platoon, the control objective is to maintain a desired spacing <i>L</i><span class="s17">i </span><span class="s18"> </span>and ensure velocity matching.</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">The state vector for each vehicle <i>i </i>at time <i>t </i>is:</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">of <i>platooning control </i><a href="#bookmark215" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">is commonly addressed through a model- free, multi-agent network approach </a><span style=" color: #00F;">[58]</span>. In this framework, each agent, symbolizing an AV, has the ability to commu-</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><b>x</b>˙<span class="s17">i</span>(<i>t</i>) <span class="s9">=</span></p><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: center;">  <span class="s20">0</span><span class="p"> 1 </span> </p><p style="text-indent: 0pt;text-align: center;">0 0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><b>x</b><span class="s17">i</span>(<i>t</i>) <span class="s21">+</span></p><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: center;">  <span class="s20">0</span><span class="p"> </span>  <span class="s22"> </span></p><p style="text-indent: 0pt;text-align: center;">1</p><p class="s12" style="padding-top: 10pt;text-indent: 0pt;text-align: left;">k<span class="s17">p</span><span class="s18"> </span><span class="p">((</span>x<span class="s17">i</span><span class="s23">−</span><span class="s24">1</span><span class="p">(</span>t<span class="p">) </span><span class="s21">− </span>x<span class="s17">i</span><span class="p">(</span>t<span class="p">)) </span><span class="s21">− </span>L<span class="s17">i</span><span class="p">)</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">nicate with both preceding and following vehicles via V2V</p><p class="s12" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s21">+</span>k<span class="s17">d</span><span class="s18"> </span><span class="p">(</span>v<span class="s17">i</span><span class="s23">−</span><span class="s24">1</span><span class="p">(</span>t<span class="p">) </span><span class="s21">− </span>v<span class="s17">i</span><span class="p">(</span>t<span class="p">)))          (1)</span></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 89%;text-align: justify;">where <i>x</i><span class="s17">i</span>(<i>t</i>) is the position of the <i>ith </i>vehicle at time <i>t</i>, <i>v</i><span class="s17">i</span>(<i>t</i>) is the velocity of the <i>ith </i>vehicle at time <i>t</i>, <i>L</i><span class="s17">i</span><span class="s18"> </span>is the desired inter- vehicle distance (spacing) between vehicle <i>i </i>and <i>i </i><span class="s21">+ </span>1, <b>x</b><span class="s17">i</span>(<i>t</i>) <span class="s9">= </span>[<i>x</i><span class="s17">i</span>(<i>t</i>)<span class="s13">, </span><i>v</i><span class="s17">i</span>(<i>t</i>)]<span class="s25">T</span><span class="s18"> </span>is the state vector, <i>k</i><span class="s17">p</span><span class="s18">  </span>and <i>k</i><span class="s17">d</span><span class="s18"> </span>are the proportional gain for position error and the derivative gain for velocity</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">error respectively. To optimize the platoon control, an objective function <i>J </i>can be defined, incorporating both spacing error and velocity error over a time horizon <i>T</i>:</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark253" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">Additionally, Relative Position Encoding Multi-Actor Atten- tion Critic (RPE-MAAC) </a><span style=" color: #00F;">[96] </span>introduces relative position encoding to enhance spatial awareness and cooperative control in mixed CAV-HDV platoons. Integrated with the Intelligent Driver Model-Informer (IDM-Informer), RPE-MAAC gener- ates realistic HDV trajectories, mitigating error propagation while improving fuel consumption, stability, and overall tra<span class="s9">ffi</span>c throughput. In vehicular environments characterized by high</p><p class="s19" style="text-indent: 0pt;line-height: 5pt;text-align: left;">Z</p><p style="text-indent: 0pt;text-align: left;"/><p class="s18" style="padding-top: 3pt;text-indent: 0pt;line-height: 5pt;text-align: right;">N         <span class="s17">T</span></p><p class="s12" style="padding-left: 65pt;text-indent: 0pt;line-height: 11pt;text-align: left;">J <span class="s9">= </span><span class="s26">X</span></p><p class="s12" style="padding-top: 5pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s27"> </span><span class="s13">α</span>e<span class="s28">2 </span><span class="s24"> </span><span class="p">(</span>t<span class="p">) </span><span class="s21">+ </span><span class="s13">β</span>e<span class="s28">2 </span><span class="s24"> </span><span class="p">(</span>t<span class="p">)</span><span class="s27">)</span><span class="s19"> </span>dt                  <span class="p">(2)</span></p><p style="padding-left: 9pt;text-indent: 0pt;text-align: left;">mobility, the use of traditional centralized optimization meth-</p><p style="padding-left: 9pt;text-indent: 0pt;line-height: 8pt;text-align: left;">ods that rely on global channel information can be impractical.</p><p class="s18" style="padding-top: 5pt;text-indent: 0pt;text-align: right;">i<span class="s29">=</span><span class="s24">1     </span><span class="s30">0</span></p><p class="s18" style="text-indent: 0pt;line-height: 7pt;text-align: right;">x<span class="s15">,</span>i</p><p class="s18" style="text-indent: 0pt;line-height: 7pt;text-align: right;">v<span class="s15">,</span>i</p><p class="s11" style="padding-top: 4pt;padding-left: 84pt;text-indent: 0pt;text-align: left;"><a href="#bookmark265" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark81">In  </a>[108]<span style=" color: #000;">,  the  authors  model  each  CAV  as  an  individual</span></p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">where <span class="s13">α </span>and <span class="s13">β </span>are weighting factors that balance the impor- tance of position and velocity errors. Thus, cooperative strategies would be developed to achieve the collective goal, adapting their behaviors in response to the actions of other vehicles within the network.</p><p class="s11" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><span class="s12">Mixed platooning control </span><a href="#bookmark261" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">refers to scenarios where mul- tiple vehicle platoons coexist within a network, requiring sophisticated coordination and communication both within (intra-platoon) and between (inter-platoon) these formations </a>[104]<span style=" color: #000;">. To address these challenges, Parvini et al. introduced two advanced MARL-based algorithms specifically designed for mixed platooning control. One of these, Modified MAD- DPG, along with its task decomposition variant, improves resource allocation in platoon-based cellular V2X (C-V2X) networks. By employing task-wise value function decom- position and adaptive agent-specific policies, this method e</span><span class="s9">ff</span><span style=" color: #000;">ectively reduces communication latency and optimizes network throughput, leading to more e</span><span class="s9">ffi</span><a href="#bookmark251" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient vehicular coor- dination </a>[94]<a href="#bookmark251" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. In these frameworks, platoon leaders function as independent agents that interact with the environment to determine optimal policies for platoon management. These algorithms leverage multiple critics to estimate both global and local rewards, fostering cooperative behavior among agents. Additionally, the task decomposition variant further refines individual reward structures, breaking them down into task- specific sub-reward functions to enhance coordination and policy learning, as demonstrated in </a>[94]<span style=" color: #000;">.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark82">Even though CAVs have made significant advancements, there will be an extended period during which CAVs will coexist with human-driven vehicles (HDVs), a situation com- monly referred to as </a><i>mixed-tra</i>ffi<i>c </i><a href="#bookmark205" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">scenarios </a><span style=" color: #00F;">[45]</span>. Di<span class="s9">ff</span>erent penetration rates can be assessed to understand the impact of mixed tra<span class="s9">ffi</span><a href="#bookmark262" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c. In the study presented in </a><span style=" color: #00F;">[105]</span>, the impact of the penetration rate of CAVs on the energy e<span class="s9">ffi</span>ciency of the tra<span class="s9">ffi</span><a href="#bookmark263" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c network is  examined,  with  a  specific  emphasis on a cooperative eco-driving system that utilizes longitudinal control. Lu et al. enhance the evaluation by integrating several key elements, including altruism control, a quantitative car- following strategy, a refined platoon reward function, and a collision avoidance method, as outlined in their work </a><span style=" color: #00F;">[106]</span><a href="#bookmark250" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. In </a><span style=" color: #00F;">[93]</span><a href="#bookmark264" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, the authors introduce  the  concept  of  characteriz- ing consecutive HDVs  as  a  collective  entity,  referred  to as AHDV, to minimize stochastic variability and leverage macroscopic characteristics for the control of following CAVs. They employ a control strategy built on distributed proximal policy optimization (DPPO) </a><span style=" color: #00F;">[107] </span>to anticipate disturbances and downstream tra<span class="s9">ffi</span>c conditions in mixed tra<span class="s9">ffi</span>c scenarios.<a name="bookmark83">&zwnj;</a><a name="bookmark84">&zwnj;</a><a name="bookmark85">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">agent, which makes decisions based on local information and communication with neighboring vehicles, without relying on a centralized controller.</p></li><li style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p style="display: inline;"><a name="bookmark86"><i>Optimization Goals: </i></a>One critical goal of platooning control is to achieve <i>string stability</i><a href="#bookmark266" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, which refers to the ability of a line of CAVs to maintain a stable and orderly formation (e.g., stable distance and speed) as they travel in close proximity to each other </a><span style=" color: #00F;">[109]</span>. Notably,  MARL  has been proposed to address and enhance the achievement of string stability in the platooning control scenario, contributing to the development of more e<span class="s9">ffi</span><a href="#bookmark267" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient and coordinated pla- tooning control strategies. For instance, in </a><span style=" color: #00F;">[110]</span><a href="#bookmark268" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, a MARL approach based on a robust communication protocol with Long Short-Term Memory (LSTM) </a><span style=" color: #00F;">[111] </span>is introduced, resulting in stable platooning control. In addition, Li et al. introduce the Communication Proximal Policy Optimization (CommPPO) algorithm to enhance stability, which adapts to varying agent numbers and supports di<span class="s9">ff</span><a href="#bookmark269" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">erent dynamics of platooning control </a><span style=" color: #00F;">[112]</span>. CommPPO incorporates a predecessor-leader-follower communication protocol to facilitate the transmission of both global and local state information among agents. Notably, CommPPO introduces a novel reward communication channel, e<span class="s9">ff</span><a href="#bookmark252" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ectively mitigating issues related to spurious rewards and mitigating the problem of “lazy agents”, which are com- monly encountered in other MARL approaches. Similarly, Multi-Agent Reinforcement Learning with Proximal Policy Optimization (MARL-PPO) </a><span style=" color: #00F;">[95] </span>enhances highway platoon- ing stability through decentralized learning and an adaptive reward mechanism, dynamically adjusting inter-vehicle dis- tances based on velocity and lane positioning to improve safety, tra<span class="s9">ffi</span>c e<span class="s9">ffi</span>ciency, and merging success rates in dynamic tra<span class="s9">ffi</span><a href="#bookmark215" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c conditions. In </a><span style=" color: #00F;">[58]</span>, the platooning control problem is formulated as a spatiotemporal MDP. They enhance system stability by introducing a spatial discount factor for local agents.<a name="bookmark87">&zwnj;</a><a name="bookmark88">&zwnj;</a><a name="bookmark89">&zwnj;</a></p><p class="s12" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark90">E</a><span class="p">ffi</span>cient communication <a href="#bookmark248" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">among CAVs in platooning con- trol also poses a fundamental task in achieving seamless coordination and maintaining the desired following proper- ties, such as precise inter-vehicle spacing and synchronized maneuvers. For instance, Liu et al. introduce a Multi-Agent Hierarchical Attention RL (MAHARL) framework, which leverages hierarchical Graph Attention Networks to model inter-agent influence and enhance long-term decision-making without immediate rewards </a><span class="s11">[91]</span><a href="#bookmark249" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Additionally, in scenarios where complete information is lacking, Li et al. </a><span class="s11">[92] </span><span class="p">develop a game-theoretic framework to capture the strategic interactions between the leading vehicle and the following vehicles. To</span><a name="bookmark91">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="301" height="154" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_014.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark92">Fig. 5.  Two-dimensional cooperation scenario.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">overcome the absence of full information, the equilibrium solution in platooning control is identified through backward induction, coupled with information collection from the fol- lowing vehicles.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-top: 8pt;padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s12" style="display: inline;">Two-Dimensional Cooperation</p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Compared to one-dimensional cooperation, two- dimensional cooperation  addresses  problems  related  to both longitudinal and lateral control, with its most typical application being <i>cooperative lane changing</i>.</p><ol id="l10"><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s11" style="display: inline;"><a name="bookmark93"><span class="s12">Application Scenarios: </span></a><a href="#bookmark92" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">As shown in Fig. </a>5<span style=" color: #000;">, CAVs communicate and coordinate through V2V and V2I communi- cation channels, enabling them to make decisions and acquire e</span><span class="s9">ff</span><a href="#bookmark270" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ective lane-changing strategies within shared driving envi- ronments </a>[113]<a href="#bookmark271" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Generally, the lane-changing mathematical problem </a>[114] <span style=" color: #000;">can be expressed as follows:</span><a name="bookmark94">&zwnj;</a></p><p class="s19" style="padding-top: 1pt;padding-left: 47pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Z <span class="s31">t</span><span class="s18"> </span><span class="s32">f</span><span class="s33">   </span> </p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">And the constraints are expressed as:</p><p class="s12" style="padding-top: 7pt;padding-left: 66pt;text-indent: 0pt;line-height: 15pt;text-align: left;">x<span class="s17">i</span><span class="s23">−</span><span class="s24">1</span><span class="p">(</span>t<span class="p">) </span><span class="s21">− </span>x<span class="s17">i</span><span class="p">(</span>t<span class="p">) </span><span class="s21">≥ </span>L<span class="s17">i</span><span class="s18"> </span><span class="s21">+ </span><span class="s9">∆</span>x<span class="s34">safe</span></p><p class="s12" style="padding-left: 66pt;text-indent: 0pt;line-height: 15pt;text-align: left;">x<span class="s17">i</span><span class="p">(</span>t<span class="p">) </span><span class="s21">− </span>x<span class="s17">i</span><span class="s35">+</span><span class="s24">1</span><span class="p">(</span>t<span class="p">) </span><span class="s21">≥ </span>L<span class="s17">i</span><span class="s35">+</span><span class="s24">1 </span><span class="s21">+ </span><span class="s9">∆</span>x<span class="s34">safe</span></p><p style="padding-left: 66pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><i>y</i><span class="s17">i</span>(<i>t</i>) <span class="s21">∈ {</span>current lane, target lane<span class="s21">}</span></p><p style="padding-left: 66pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a name="bookmark95">0 </a><span class="s21">≤ </span><i>y</i><span class="s17">i</span>(<i>t</i>) <span class="s21">≤ </span><i>w</i><span class="s34">lane                                                               </span><span class="s24"> </span>(5)</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark96">Hou et al. devise a decentralized </a><i>cooperative lane-changing </i><a href="#bookmark272" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">controller employing a multi-agent Proximal Policy Optimiza- tion (MAPPO) </a><span style=" color: #00F;">[115] </span>approach, which empowers each vehicle to independently learn and assess its policy and action rewards based on local information, while still having access to global state data. The trained policies can be e<span class="s9">ff</span>ectively transferred and applied across various tra<span class="s9">ffi</span>c conditions, achieving  a safe, e<span class="s9">ffi</span><a href="#bookmark273" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient, and comfortable lane-change maneuver </a><span style=" color: #00F;">[116]</span><a href="#bookmark189" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Dubey et al. </a><span style=" color: #00F;">[29] </span>introduce FAIRLANE, a MARL-based framework designed to manage priority lanes in mixed- autonomy tra<span class="s9">ffi</span>c. By utilizing multi-policy bootstrapping and decentralized decision-making, the framework dynamically optimizes lane allocation to enhance tra<span class="s9">ffi</span>c e<span class="s9">ffi</span>ciency, safety, and fairness. This method ensures equitable access to priority lanes while adapting  to  real-time  tra<span class="s9">ffi</span><a href="#bookmark258" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c  conditions.  Zhang et al. </a><span style=" color: #00F;">[101] </span>propose a Multi-Agent Reinforcement Learning (MARL) framework that incorporates a Prioritized Action Extrapolation (PAE) mechanism. This approach dynamically updates lane-changing decision priorities, enabling smoother coordination, improved safety, and faster policy convergence in high-density tra<span class="s9">ffi</span>c scenarios.<a name="bookmark97">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark254" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">In </a><span style=" color: #00F;">[97]</span>, the authors put forth  a  <i>multi-vehicle  coopera- tive driving </i>approach, leveraging the QMIX algorithm. Their method applies RL to dynamically adapt to changing con-</p><p style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;line-height: 6pt;text-align: left;">min <i>J </i><span class="s9">=</span></p><p class="s12" style="padding-left: 12pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s13">α</span><span class="s34">1</span><span class="s36">/</span><span class="s19"> </span><span class="p">(</span>x<span class="s17">i</span><span class="s18">  </span><span class="s24">1</span><span class="p">(</span>t<span class="p">) </span><span class="s21">− </span>x <span class="p">(</span>t<span class="p">) </span><span class="s21">− </span>L <span class="s21">− </span><span class="s9">∆</span>x   <span class="p">)</span><span class="s28">2</span></p><p style="padding-left: 12pt;text-indent: 0pt;line-height: 10pt;text-align: left;">ditions  on  highways,  aiming  to  find  an  optimal  balance</p><p class="s24" style="padding-top: 3pt;padding-left: 14pt;text-indent: 0pt;text-align: left;"><b>u</b><span class="s37">i</span><span class="s33"> </span>(<i>t</i>)              <i>t</i><span class="s38">0</span></p><p class="s18" style="padding-left: 14pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="s23">–            </span>i             i</p><p class="s24" style="padding-left: 14pt;text-indent: 1pt;line-height: 7pt;text-align: left;">safe</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s39" style="padding-left: 14pt;text-indent: 0pt;line-height: 2pt;text-align: left;">2<span class="s24"> </span><span class="s19"> </span></p><p style="padding-left: 14pt;text-indent: 0pt;line-height: 12pt;text-align: left;">between autonomous decision-making and cooperative interac- tion among the vehicles. This algorithm allows each vehicle in</p><p class="s12" style="padding-left: 47pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="s21">+ </span>x<span class="s17">i</span><span class="p">(</span>t<span class="p">) </span><span class="s21">− </span>x<span class="s17">i</span><span class="s35">+</span><span class="s24">1</span><span class="p">(</span>t<span class="p">) </span><span class="s21">− </span>L<span class="s17">i</span><span class="s35">+</span><span class="s24">1 </span><span class="s21">− </span><span class="s9">∆</span>x<span class="s34">safe</span><span class="s27">)</span></p><p class="s19" style="padding-top: 5pt;text-indent: 0pt;line-height: 3pt;text-align: right;">  <span class="s40">d</span></p><p class="s21" style="padding-left: 11pt;text-indent: 0pt;line-height: 9pt;text-align: left;">+ <span class="s13">α</span><span class="s34">2  </span><span class="s24"> </span><span class="s12">t </span><span class="s17">f </span><span class="s18"> </span>− <span class="s12">t</span><span class="s34">0</span><span class="s27">)</span></p><p class="s19" style="padding-top: 2pt;padding-left: 4pt;text-indent: 0pt;line-height: 5pt;text-align: left;"> <span class="s41">2</span></p><p style="padding-left: 16pt;text-indent: 0pt;line-height: 12pt;text-align: left;">the pair to independently perform lane changes and overtaking maneuvers, even in dense tra<span class="s9">ffi</span>c scenarios, while maintaining</p><p class="s12" style="padding-left: 47pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s21">+ </span><span class="s13">α</span><span class="s34">3</span><span class="s36">/</span>u<span class="s17">x</span><span class="s15">,</span><span class="s18">i</span><span class="p">(</span>t<span class="p">)</span><span class="s28">2</span><span class="s24"> </span><span class="s21">+ </span>u<span class="s17">y</span><span class="s15">,</span><span class="s18">i</span><span class="p">(</span>t<span class="p">)</span><span class="s28">2</span><span class="s24"> </span><span class="s21">+</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="1" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_015.png"/></span></p><p class="s12" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s40">dt</span> u<span class="s17">x</span><span class="s15">,</span><span class="s18">i</span><span class="p">(</span>t<span class="p">)</span></p><p style="padding-left: 47pt;text-indent: 0pt;text-align: left;">a predetermined formation between them. An enhanced QMIX</p><p class="s19" style="padding-top: 2pt;padding-left: 57pt;text-indent: 0pt;line-height: 10pt;text-align: left;">  <span class="s40">d           </span><span class="s12"> </span><span class="s41">2</span><span class="s24"> </span><span class="s42"> </span> </p><p class="s11" style="padding-left: 57pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark274" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark98">algorithm </a>[117] <span style=" color: #000;">is further proposed to enhance the flexibility</span><a name="bookmark99">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="1" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_016.png"/></span></p><p class="s12" style="padding-top: 2pt;padding-left: 65pt;text-indent: -17pt;line-height: 55%;text-align: left;"><span class="s21">+      </span>u<span class="s17">y</span><span class="s15">,</span><span class="s18">i</span><span class="p">(</span>t<span class="p">) </span>dt</p><p class="s12" style="padding-left: 21pt;text-indent: 0pt;line-height: 11pt;text-align: left;">dt                                              <span class="p">(3)</span></p><p style="padding-left: 9pt;text-indent: 0pt;line-height: 10pt;text-align: left;">and e<span class="s9">ff</span>ectiveness of the collaborative lane-changing system.</p><p style="padding-left: 9pt;text-indent: 0pt;text-align: left;">They implement a stable estimation method to mitigate the</p><p style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Subject to the vehicle dynamics:</p><p style="padding-top: 8pt;padding-left: 5pt;text-indent: 80pt;text-align: left;"><b>x</b>˙<span class="s17">i</span>(<i>t</i>) <span class="s9">= </span><b>Ax</b><span class="s17">i</span>(<i>t</i>) <span class="s21">+ </span><b>Bu</b><span class="s17">i</span>(<i>t</i>)                           (4)</p><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">where <span class="s13">α</span><span class="s34">1</span>, <span class="s13">α</span><span class="s34">2</span>, and <span class="s13">α</span><span class="s34">3</span><span class="s24"> </span>are the corresponding weights for safety, e<span class="s9">ffi</span>ciency,  and  comfort,  respectively.  Other  objectives  may</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;"><a name="bookmark100">problem of overestimated joint Q-values among agents. This approach strikes a fine balance between maintaining formation and allowing for smooth overtaking, thus facilitating intelligent adaptation to a variety of scenarios, including heavy tra</a><span class="s9">ffi</span>c, light tra<span class="s9">ffi</span>c, and emergencies. In an e<span class="s9">ff</span>ort to enhance mobility within  complex  tra<span class="s9">ffi</span><a href="#bookmark275" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c  environments,  in  </a><span style=" color: #00F;">[118]</span>,  instead  of</p><p class="s27" style="text-indent: 0pt;line-height: 9pt;text-align: right;">2<span class="s19"> </span><span class="p">0 0 1 0 </span>3</p><p class="s19" style="padding-left: 33pt;text-indent: 0pt;line-height: 9pt;text-align: left;">2 <span class="s20">0</span><span class="p"> 0 </span>3</p><p style="padding-left: 42pt;text-indent: 0pt;line-height: 10pt;text-align: left;">relying on relative distance and semantic maps, the authors</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 3pt;text-align: left;">also be considered; <b>A </b><span class="s9">= </span><span class="s27">6</span><span class="s19"> </span><span class="s43">0</span> 0 0 1 <span class="s27">7</span><span class="s19"> </span><span class="s13">,    </span><b>B </b><span class="s9">= </span><span class="s27">6</span><span class="s19"> </span><span class="s43">0</span> 0 <span class="s27">7</span>, <b>x</b><span class="s17">i</span>(<i>t</i>) <span class="s9">=</span></p><p class="s19" style="padding-left: 109pt;text-indent: 0pt;line-height: 9pt;text-align: left;">6         7         6    7</p><p style="padding-left: 42pt;text-indent: 0pt;line-height: 9pt;text-align: left;">suggest the utilization of tra<span class="s9">ffi</span>c states that encapsulate the</p><p class="s44" style="text-indent: 0pt;line-height: 10pt;text-align: right;">4<span class="s19"> </span><span class="p">0 0 0 0 </span>5</p><p style="text-indent: 0pt;line-height: 11pt;text-align: right;">0 0 0 0</p><p class="s18" style="padding-left: 102pt;text-indent: 0pt;line-height: 1pt;text-align: left;">T</p><p class="s19" style="padding-left: 33pt;text-indent: 0pt;line-height: 10pt;text-align: center;">4 <span class="s45">1</span><span class="p"> 0 </span>5</p><p style="padding-left: 33pt;text-indent: 0pt;text-align: center;">0 1</p><p style="padding-top: 1pt;padding-left: 42pt;text-indent: 0pt;line-height: 12pt;text-align: left;">spatio-temporal  interactions  between  neighboring  vehicles. Within  the  MADRL  framework,  three  prediction  models,</p><p class="s12" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s27">1</span>x<span class="s17">i</span><span class="p">(</span>t<span class="p">)</span><span class="s13">, </span>y<span class="s17">i</span><span class="p">(</span>t<span class="p">)</span><span class="s13">, </span>v<span class="s17">x</span><span class="s15">,</span><span class="s18">i</span><span class="p">(</span>t<span class="p">)</span><span class="s13">, </span>v<span class="s17">y</span><span class="s15">,</span><span class="s18">i</span><span class="p">(</span>t<span class="p">)</span><span class="s27">l</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">is the state vector for vehicle <i>i</i>; <i>x</i><span class="s17">i</span>(<i>t</i>)</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">namely  the  transformer-based  (TS-Transformer),  generative</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">and <i>y</i><span class="s17">i</span>(<i>t</i>) are the longitudinal and lateral position of the <i>ith</i></p><p class="s18" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i                   x<span class="s15">,</span>i          y<span class="s15">,</span>i</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 41%;text-align: justify;">vehicle at time <i>t</i>; <i>v</i><span class="s17">x</span><span class="s15">,</span><span class="s18">i</span>(<i>t</i>) and <i>v</i><span class="s17">y</span><span class="s15">,</span><span class="s18">i</span>(<i>t</i>) are longitudinal and lateral velocity of the <i>ith </i>vehicle at time <i>t</i>. <b>u </b>(<i>t</i>) <span class="s9">= </span><span class="s27">1</span><i>u  </i>(<i>t</i>)<span class="s13">, </span><i>u  </i>(<i>t</i>)<span class="s27">l</span><span class="s46">T</span></p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">is the control input  vector  for  vehicle  <i>i</i>,  <i>u</i><span class="s17">x</span><span class="s15">,</span><span class="s18">i</span>(<i>t</i>)  and  <i>u</i><span class="s17">y</span><span class="s15">,</span><span class="s18">i</span>(<i>t</i>) are the control input for longitudinal and lateral acceleration respectively; <i>L</i><span class="s17">i</span><span class="s18"> </span>is the length of the <i>ith </i>vehicle; <span class="s9">∆</span><i>x</i><span class="s34">safe</span><span class="s24"> </span>is the safety distance to avoid collision; <i>w</i><span class="s34">lane</span><span class="s24"> </span>is the width of the lane.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">adversarial network-based (TS-GAN), and conditional varia- tional autoencoder-based (TS-CVAE) models, are developed and compared for tra<span class="s9">ffi</span>c state prediction.</p></li><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s12" style="display: inline;"><a name="bookmark101">Optimization Goals: Safety </a><a href="#bookmark276" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">is of paramount importance in the context of CAV scenarios. In  </a><span class="s11">[119]</span><span class="p">,  the  authors have introduced a safety-enhancing actor-critic algorithm that incorporates two innovative techniques. The first technique is</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="653" height="210" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_017.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark102">Fig. 6.  Three-dimensional cooperation scenarios: (a) Tra</a><span class="s8">ffi</span>c Signal Coordination; (b) Merging on-ramps;(c) Unsignalized intersections.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark103">the ‘truncated Q-function’, which e</a><span class="s9">ff</span><a href="#bookmark255" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ectively leverages shared information from neighboring CAVs, ensuring scalability for large-scale CAV systems. The second technique, “safe action mapping”, provides safety guarantees throughout both the training and execution phases by utilizing control barrier functions. Additionally, a bi-level strategy is proposed in </a><span style=" color: #00F;">[98] </span>to further enhance safety and e<span class="s9">ffi</span>ciency. At the upper level, the MADQN model is utilized for making decisions about lane changes. This approach acknowledges the cooperative aspect of driving by factoring in the intentions of surrounding vehicles, enabling implicit negotiation for right-of-way. At the lower level, a right-of-way assignment model is utilized to ensure safety. A novel reward function is further proposed to encourage coordination and account for tra<span class="s9">ffi</span>c impact. Moreover, Li et al. have introduced MetaDrive, a platform that focuses on safe driving by addressing generalizability, safety awareness, and multi-agent decision-making. It o<span class="s9">ff</span><a href="#bookmark277" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ers diverse driving scenarios for benchmarking single and multi-agent RL tasks </a><span style=" color: #00F;">[120]</span>.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark278" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark104">Graph neural networks (GNNs) </a><span style=" color: #00F;">[121] </span>have gained sig- nificant attention in MARL settings, particularly for <i>tra</i>ffi<i>c flow optimization</i><a href="#bookmark279" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Chen et al. introduce a novel algorithm based on MADQN, which combines a GCN with a deep Q- network </a><span style=" color: #00F;">[122]</span>. This approach facilitates e<span class="s9">ff</span>ective information fusion and decision-making within the MARL framework, thereby enhancing both safety and tra<span class="s9">ffi</span>c flow in various tra<span class="s9">ffi</span><a href="#bookmark280" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c scenarios. Moreover, in </a><span style=" color: #00F;">[123]</span>, the authors leverage GNN in conjunction with MADDPG in their work for multi-agent training, addressing the complexity of real-world inputs and aiding in  congestion  mitigation  e<span class="s9">ff</span>orts.  Guo  et al. introduce deep Q-Network with Request-Respond Mech- anism (DQNRR), which enhances lane-changing coordination through a request-respond mechanism. This method hierar- chically structures lane-change decisions, allowing vehicles to negotiate merging and overtaking maneuvers more e<span class="s9">ffi</span>ciently, thus reducing computational complexity while improving lane- changing e<span class="s9">ffi</span>ciency in congested urban tra<span class="s9">ffi</span><a href="#bookmark257" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c </a><span style=" color: #00F;">[100]</span>.<a name="bookmark105">&zwnj;</a><a name="bookmark106">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Thoughtful reward function design is paramount for the <i>adaptivity and e</i>ffi<i>ciency </i><a href="#bookmark256" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">of lane-changing strategies. In </a><span style=" color: #00F;">[99]</span>, the authors introduce the multi-agent advantage actor-critic (MA2C) framework, which integrates a novel local reward</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark107">design and parameter-sharing scheme for cooperative lane changing in mixed tra</a><span class="s9">ffi</span>c scenarios. They also present a multi- objective reward function that considers factors such as fuel e<span class="s9">ffi</span>ciency, driving comfort, and safety to facilitate successful and e<span class="s9">ffi</span><a href="#bookmark281" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient lane changing. In </a><span style=" color: #00F;">[124]</span>, a MARL approach is explored in which agents collaborate to reach a zero-sum game state. In contrast to cooperative driving, this approach, known as harmonious driving, places emphasis on achieving a bal- ance between overall and individual e<span class="s9">ffi</span>ciency while utilizing limited sensing data from individual vehicles. They design a reward function that promotes harmony by considering both individual and overall e<span class="s9">ffi</span>ciency, as opposed to competitive strategies that solely prioritize individual interests.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s12" style="display: inline;">Three-Dimensional Cooperation</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Three-dimensional cooperation involves the intricate coor- dination of longitudinal and lateral movements along with timing, across a variety of tra<span class="s9">ffi</span>c scenarios such as  tra<span class="s9">ffi</span><a href="#bookmark102" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c signal coordination, on-ramp merging, and navigating through unsignalized intersections, as depicted in Fig. </a><span style=" color: #00F;">6</span>. To ensure safe, e<span class="s9">ffi</span>cient, and e<span class="s9">ff</span>ective cooperation, CAVs need to rapidly learn longitudinal and lateral maneuvers within tight time constraints. The three-dimensional cooperation problem can</p><p class="s12" style="padding-left: 5pt;text-indent: 0pt;line-height: 89%;text-align: justify;"><a href="#bookmark98" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">also be described by Eq. </a><span class="s11">3</span><a href="#bookmark95" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">–Eq. </a><span class="s11">5</span><span class="p">, with the added time constraints </span>t<span class="s17">i</span><span class="s15">, </span><span class="s24">start   </span><span class="s21">≤ </span>t <span class="s21">≤ </span>t<span class="s17">i</span><span class="s15">, </span><span class="s24">end </span><span class="p">.</span></p><ol id="l11"><li style="padding-left: 5pt;text-indent: 9pt;line-height: 10pt;text-align: justify;"><p class="s12" style="display: inline;">Tra<span class="p">ffi</span>c  Signal  Control  (TSC):   <span class="p">TSC  plays  a  critical</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark102" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">role in shaping vehicle interactions by introducing time- based constraints at intersections (see Fig. </a><a href="#bookmark102" class="a">6 </a><span style=" color: #00F;">(a)</span>). Traditional optimization-based TSC methods focus on tra<span class="s9">ffi</span><a href="#bookmark205" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c management, adjusting signal timings to regulate flow, but they treat vehicles as passive entities responding to fixed control policies </a><span style=" color: #00F;">[45]</span>. In contrast, when considering MARL in CAV control, TSC is not the primary research target but rather an external constraint that impacts how CAVs cooperate to navigate intersections e<span class="s9">ffi</span>ciently. CAVs, unlike human-driven vehicles, can actively communicate and coordinate their movements under these constraints, optimizing their joint decision-making to minimize congestion, delays, and energy consumption. MARL provides a natural framework for CAVs to learn cooperative policies in response to signal timings,  ensuring  e<span class="s9">ffi</span>cient  merging, gap acceptance, and right-of-way negotiation. The challenge</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">lies not in optimizing TSC itself but in understanding how CAVs interact under these time-dependent tra<span class="s9">ffi</span>c conditions to improve overall system e<span class="s9">ffi</span>ciency. TSC can be viewed as a multi-objective optimization problem and Markov<span class="s9">/</span>Stochastic Games with cooperative settings, which require agents to coor- dinate with each other. Therefore, the learning-based method can show better performance without prior knowledge about the given environment. In multi-agent-based algorithms, all tra<span class="s9">ffi</span>c light controllers within a tra<span class="s9">ffi</span>c grid need to coordinate to address tra<span class="s9">ffi</span>c congestion.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark108"><i>Value-based MARL </i></a><a href="#bookmark282" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">algorithms have been extensively inves- tigated for TSC challenges. For instance, to alleviate  the curse of dimensionality and environmental nonstationarity problems, a decentralized  coordination  MADQN  approach is proposed in </a><span style=" color: #00F;">[125] </span><a href="#bookmark283" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">to explicitly identify and dynamically adapt agent coordination needs during the learning process. Similarly, in </a><span style=" color: #00F;">[126]</span>, the complex TSC problem is decomposed into simpler subproblems and tackled using multiple regional agents and a centralized global agent. Each regional agent learns its RL policy for smaller regions with reduced action spaces, while the centralized global agent combines the RL contributions from various regional agents to form a final Q-function for the entire large-scale tra<span class="s9">ffi</span><a href="#bookmark284" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c grid. Notably, QMIX, an extension of  VDN,  was  investigated  in  </a><span style=" color: #00F;">[127] </span><a href="#bookmark285" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">for TSC, achieving promising  results.  Additionally,  Liu  et al. introduce a multi-agent dueling-double-deep Q network (MAD3QN) for TSC </a><span style=" color: #00F;">[128]</span>. This framework features an innovative <span class="s13">γ</span>-reward design, which combines the traditional <span class="s13">γ</span>-reward with a novel <span class="s13">γ</span>-attention-reward. By employing a spatial di<span class="s9">ff</span><a href="#bookmark286" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">erentiation method for agent coordination, their approach enables decentralized control and decoupling of road networks, enhancing scalability and convergence. In </a><span style=" color: #00F;">[129]</span>, a cooperative group-based multi-agent Q-learning for ATSC (CGB-MATSC) is proposed. This method enhances the learn- ing process by incorporating a k-nearest-neighbor approach for state representation, a pheromone-based strategy for creating regional green-wave tra<span class="s9">ffi</span>c flows, and a spatially discounted reward system.<a name="bookmark109">&zwnj;</a><a name="bookmark110">&zwnj;</a><a name="bookmark111">&zwnj;</a><a name="bookmark112">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark113">On the other hand, </a><i>policy-based </i><a href="#bookmark196" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">MARL algorithms have also been widely studied for TSC. In </a><span style=" color: #00F;">[36]</span>, a fully scalable and decentralized Multi-Agent Actor-Critic (MA2C) algorithm is introduced, which incorporates an advanced discount factor to mitigate the e<span class="s9">ff</span><a href="#bookmark287" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ects of remote agents. This approach demon- strates significant advantages over random and independent controllers. Wu et al. introduced the multi-agent recurrent deep deterministic policy gradient (MARDDPG) algorithm, specifically designed for TSC in vehicular networks </a><span style=" color: #00F;">[130]</span>. They utilize a strategy  of CTDE,  enhancing the  e<span class="s9">ffi</span>ciency of the training process through  parameter  sharing  among the actor networks. Furthermore, the integration of LSTM networks enables the algorithm to utilize historical data for more e<span class="s9">ff</span><a href="#bookmark288" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ective control decisions. In </a><span style=" color: #00F;">[131]</span>, a MA2C approach with a feudal hierarchy concept for TSC is presented. This approach segments the tra<span class="s9">ffi</span>c network into several regions, each monitored by a manager agent, with tra<span class="s9">ffi</span>c signal agents acting as workers. The managers are responsible for high-level coordination and setting objectives for the workers, who then adjust tra<span class="s9">ffi</span>c signals to meet these objectives. This hierarchical<a name="bookmark114">&zwnj;</a></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">structure fosters global coordination while ensuring the system scalability.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark115">E</a><span class="s9">ffi</span>cient <i>communication </i>is  crucial  in  applying  MARL to TSC as it enables coordinated decision-making, optimal resource allocation, adaptation to dynamic environments, scal- ability, conflict resolution, and e<span class="s9">ffi</span>cient learning, all of which contribute to e<span class="s9">ff</span>ective tra<span class="s9">ffi</span><a href="#bookmark289" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c management </a><span style=" color: #00F;">[132]</span>. Liu et al. present a novel algorithm that enhances communication e<span class="s9">ffi</span>- ciency through a new message exchange method and improves congestion measurement by introducing a more comprehensive reward calculation method. Then a clear and simple represen- tation of tra<span class="s9">ffi</span>c conditions is provided, and the synchronization between agents at di<span class="s9">ff</span><a href="#bookmark290" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">erent intersections with varying cycle lengths needs to be addressed </a><span style=" color: #00F;">[133]</span><a href="#bookmark291" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. To incorporate spatial and temporal dependencies, Wang et al. introduce STMARL  </a><span style=" color: #00F;">[134]</span>, which models tra<span class="s9">ffi</span>c lights using a tra<span class="s9">ffi</span>c light adjacency graph and recurrent neural networks. This approach optimizes coordination across intersections by merging historical and real-time tra<span class="s9">ffi</span><a href="#bookmark292" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c data. Recognizing the importance of shared information, Jiang et al. propose UniComm </a><span style=" color: #00F;">[135]</span>, which predicts the impact of local observations on neighboring intersections, improving communication e<span class="s9">ffi</span>ciency.<a name="bookmark116">&zwnj;</a><a name="bookmark117">&zwnj;</a><a name="bookmark118">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark119"><i>Graph-based MARL </i></a>provides advantages such as scalability, e<span class="s9">ffi</span>cient message passing, global coordination, and adapt- ability, making it particularly suitable for multi-agent tra<span class="s9">ffi</span><a href="#bookmark293" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c signal control (TSC) problems </a><span style=" color: #00F;">[136]</span><a href="#bookmark294" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. CoLight </a><span style=" color: #00F;">[137] </span>intro- duces a graph-attentional MARL model that enables tra<span class="s9">ffi</span>c signals to communicate using temporal and spatial data from neighboring intersections. By achieving index-free modeling of intersections, it enhances adaptability and e<span class="s9">ffi</span><a href="#bookmark295" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ciency. Sim- ilarly, Yang et al. propose Inductive Heterogeneous Graph Multi-agent Actor-Critic (IHG-MA) </a><span style=" color: #00F;">[138]</span>, which leverages an inductive heterogeneous graph neural network to encode tra<span class="s9">ffi</span><a href="#bookmark296" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c network structures and a multi-agent actor-critic framework for policy learning, allowing real-time adaptation to previ- ously unseen intersections. Further enhancing MARL-based intersection control, Antonio et al. </a><span style=" color: #00F;">[139] </span>integrate multi-agent TD3 with LSTM to manage varying observation sizes due to fluctuating vehicle numbers. Their method employs curriculum learning through self-play to optimize collaborative CAV con- trol at intersections, improving tra<span class="s9">ffi</span>c e<span class="s9">ffi</span>ciency and safety. In contrast, Ren et al. highlight potential security vulnerabilities in MARL-based tra<span class="s9">ffi</span><a href="#bookmark297" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c signal control by proposing a stealthy opaque-model attack strategy. Their method selectively per- turbs critical states to increase congestion and delays while remaining undetectable, underscoring the need for robust secu- rity mechanisms in intelligent transportation systems </a><span style=" color: #00F;">[140]</span>.<a name="bookmark120">&zwnj;</a><a name="bookmark121">&zwnj;</a><a name="bookmark122">&zwnj;</a><a name="bookmark123">&zwnj;</a></p></li><li style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p style="display: inline;"><i>On-Ramps Merging: </i>The on-ramp merging task requires CAVs to integrate into the main tra<span class="s9">ffi</span>c flow while maintain- ing safety and e<span class="s9">ffi</span><a href="#bookmark102" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ciency (see Fig. </a><a href="#bookmark102" class="a">6 </a><span style=" color: #00F;">(b)</span>). This maneuver involves both spatial and temporal coordination, as vehicles must dynamically adjust their speeds and trajectories to avoid conflicts. CAVs in the through lane proactively modify their velocities—either accelerating or decelerating—to create suf- ficient gaps for merging vehicles. Meanwhile, CAVs on the ramp must regulate their speeds and make timely lane-change decisions to merge smoothly without causing  congestion. The success of this process depends on precise multi-agent</p><p class="s6" style="padding-top: 2pt;padding-left: 66pt;text-indent: 0pt;text-align: center;">TABLE III</p><p class="s6" style="padding-top: 2pt;padding-left: 66pt;text-indent: 0pt;text-align: center;">A<span class="s16">N </span>O<span class="s16">VERVIEW OF THE </span>P<span class="s16">RIMARY </span>W<span class="s16">ORKS </span>F<span class="s16">ROM </span>F<span class="s16">OUR </span>P<span class="s16">ERSPECTIVES IN </span>D<span class="s16">IFFERENT </span>S<span class="s16">ETTINGS</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="686" height="680" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_018.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark124">coordination, where vehicles anticipate and adapt to each other’s actions, ensuring safe and e</a><span class="s9">ffi</span>cient merging under real- time tra<span class="s9">ffi</span><a href="#bookmark298" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c constraints </a><span style=" color: #00F;">[141]</span>.<a name="bookmark125">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark299" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark126">In </a><span style=" color: #00F;">[142]</span>, a simplified mathematical formulation for an on-ramp merging scenario is presented to model the funda- mental interactions and solved with a  MADQN  algorithm that accounts for the interaction between an on-ramp merging vehicle and a tra<span class="s9">ffi</span>c vehicle in the target lane. The results indicate that a multi-agent approach can result in reduced collision rates compared to a single-agent approach, but this</p><p class="s11" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark127"><span style=" color: #000;">improvement is contingent on the optimality of the tra</span></a><span class="s9">ffi</span><a href="#bookmark300" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c vehicle’s behavior in the target lane. Zhou et al. introduce a cooperative merging control strategy for CAVs using a distributed MADDPG approach, which accounts for safe merging distances, acceleration limits, and factors like rear- end safety, lateral safety, and energy consumption </a>[143]<a href="#bookmark301" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. To address the challenge of a dynamic environment resulting from the decentralized learning of CAVs, a decentralized framework employing MADDPG is presented to coordinate CAVs during highway merging </a>[144]<span style=" color: #000;">. This framework enables</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">policies learned from a small group of trained CAVs to be transferred and applied to any number of CAVs, with a reward function that promotes high-speed travel, resulting in smoother tra<span class="s9">ffi</span>c flow while maintaining safety in terms of rear-end and lateral collisions.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark128">Some studies also examine  the  influence  of  </a><i>human drivers </i><a href="#bookmark302" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">in on-ramp merging scenarios emphasizing adaptive decision-making and cooperative control between connected and automated vehicles (CAVs) and human-driven vehicles (HDVs). Hu et al. propose Interaction-aware Decision-making with Adaptive Strategies (IDAS), which leverages a modified Multi-Agent Advantage Actor-Critic (MA2C) method and curriculum learning to train AVs to adapt to varying driver behaviors by assessing their level of cooperativeness </a><span style=" color: #00F;">[145]</span>. Similarly, Chen et al. introduce a multi-agent cooperative merging framework using MA2C, enabling  CAVs  on  both the merge and through lanes to jointly develop policies that optimize tra<span class="s9">ffi</span><a href="#bookmark190" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c flow and adapt to HDV dynamics </a><span style=" color: #00F;">[30]</span>. Their framework ensures scalability in dynamic tra<span class="s9">ffi</span>c scenarios through parameter sharing, local rewards, and action mask- ing, promoting inter-agent cooperation while incorporating a priority-based safety supervisor to reduce collision risks and accelerate learning.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark129">Beyond RL approaches, game-theoretic strategies have also been explored to enhance </a><i>safety</i><a href="#bookmark303" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Chandra et al. propose GAMEPLAN, a game-theoretic multi-agent planning approach that integrates both CAVs and human drivers in merging scenarios </a><span style=" color: #00F;">[146]</span>. This method utilizes game theory and an auction-based mechanism to compute optimal driving actions while inferring driving styles from sensor data. By assigning priority based on driver aggressiveness or patience, GAME- PLAN optimally balances safety and e<span class="s9">ffi</span>ciency, preventing both collisions and deadlocks.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">To tackle the complexities of multilane on-ramp merging, Liu et al. introduce Graph Convolutional Proximal Policy Optimization (GCAV-CPO), a <i>graph-based MARL </i>framework that integrates vehicle graph structures with an eco-friendly Markov decision process to enhance coordination,  safety, and energy e<span class="s9">ffi</span>ciency in mixed-autonomy tra<span class="s9">ffi</span><a href="#bookmark180" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c </a><span style=" color: #00F;">[20]</span>. Their approach generalizes well across varying tra<span class="s9">ffi</span>c densities and CAV penetration rates, demonstrating superior performance over existing baselines.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s11" style="display: inline;"><a name="bookmark130"><span class="s12">Unsignalized Intersections: </span></a><span style=" color: #000;">Intersections serve as pivotal nodes in urban road networks but can also become major bottlenecks if not managed e</span><span class="s9">ffi</span><span style=" color: #000;">ciently. However, managing vehicular interactions in the absence of tra</span><span class="s9">ffi</span><a href="#bookmark102" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c signals, as illustrated in Fig. </a><a href="#bookmark102" class="a">6 </a>(b)<a href="#bookmark179" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, introduces significant challenges due to the need for decentralized decision-making, right-of-way negotiation, and collision avoidance </a>[19]<a href="#bookmark305" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Unlike TSC, where vehicles follow fixed timing rules, unsignalized intersections require real-time adaptive cooperation among multiple CAVs. Many conventional approaches, such as rule-based </a>[148]<a href="#bookmark306" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, planning-based </a>[149]<a href="#bookmark304" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, and single-agent RL methods </a>[147]<span style=" color: #000;">, often treat intersection management as a single-agent problem, which can not capture the complex interactions among CAVs. Recently, MARL has emerged as a promising and e</span><span class="s9">ff</span><span style=" color: #000;">ective tool for managing tra</span><span class="s9">ffi</span><span style=" color: #000;">c at unsignalized intersections.</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark131">Spatharis et al. propose a multi-agent double DQN frame- work for unsignalized intersections, integrating an e</a><span class="s9">ffi</span>cient reward function to balance safety and tra<span class="s9">ffi</span>c e<span class="s9">ffi</span><a href="#bookmark179" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ciency while enabling transfer learning for adaptability </a><span style=" color: #00F;">[19]</span><a href="#bookmark307" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Capasso et al. develop a MAD-A3C model using model-free DRL to predict vehicle actions, allowing CAVs to autonomously learn intersection navigation while ensuring safety </a><span style=" color: #00F;">[150]</span>. Yan et al. introduce a DQN-based  policy  decomposition  strategy for decentralized CAV coordination, achieving near-optimal throughput without reward shaping and demonstrating strong generalizability across tra<span class="s9">ffi</span><a href="#bookmark308" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c conditions </a><span style=" color: #00F;">[151]</span>.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark132">E</a><span class="s9">ff</span>ective <i>coordination </i>among CAVs is essential to ensure safe and e<span class="s9">ffi</span><a href="#bookmark309" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient maneuvers at unsignalized intersections. In </a><span style=" color: #00F;">[152]</span>, a decentralized MADQN algorithm is adopted and learned to make decisions for CAVs. To enable e<span class="s9">ff</span><a href="#bookmark310" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ective coordination among agents, the intent trajectories of other neighboring agents are incorporated into each agent’s state space. Furthermore, a decentralized and conflict-free coordina- tion scheme designed for CAVs at unsignalized intersections is proposed in </a><span style=" color: #00F;">[153]</span><a href="#bookmark311" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, with the objective of enhancing intersection management precision. They frame the challenge of safely guiding multiple vehicles through unsignalized intersections as a partially observable stochastic game (POSG). To facilitate collaborative decision-making in a distributed fashion, they introduce a cooperative multi-agent proximal optimization algorithm (CMAPPO). In </a><span style=" color: #00F;">[154]</span>, a multi-layer coordination strategy is proposed for the management of unsignalized intersections. This architecture comprises two layers: a low- level layer that employs a dynamics-based algorithm to control individual CAVs, and a high-level layer that  integrates  a TD3 algorithm, trained in a centralized manner but executed in a decentralized fashion by multiple agents for decision- making. Their findings reveal a successful training process, with the MATD3 algorithm achieving a remarkable 100% success rate in preventing intersection collisions. Addressing adjacent intersection coordination, Xu et al. propose a multi- agent deep reinforcement learning scheduling (MA-DRLS) algorithm, allowing each intersection agent to independently optimize tra<span class="s9">ffi</span><a href="#bookmark312" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c scheduling via information exchange, using DQN networks for stable training </a><span style=" color: #00F;">[155]</span>. Additionally, Tal- lapragada et al. introduce a multi-agent joint-action DDPG (MAJA-DDPG), a framework integrating RL with sequential optimization. It first learns a  shared  policy  for  determin- ing vehicle crossing orders based on tra<span class="s9">ffi</span><a href="#bookmark313" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c states, followed by trajectory optimization, ensuring scalable and distributed coordination </a><span style=" color: #00F;">[156]</span>.<a name="bookmark133">&zwnj;</a><a name="bookmark134">&zwnj;</a><a name="bookmark135">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark136">To enhance coordination in bandwidth-constrained multi- agent settings, Li et al. introduce the E</a><span class="s9">ffi</span><a href="#bookmark314" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient Communication Method (ECM)-MA2C algorithm </a><span style=" color: #00F;">[157]</span>. By integrating a variational auto-encoder with a multi-head attention mech- anism, ECM-MA2C selectively extracts and retains critical information from neighboring agents, thereby optimizing com- munication e<span class="s9">ffi</span>ciency. Experimental results demonstrate that this approach significantly outperforms baseline methods, ensuring e<span class="s9">ff</span>ective coordination with minimal communication overhead. While ECM-MA2C focuses on e<span class="s9">ffi</span><a href="#bookmark315" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient information sharing, Guo et al. </a><span style=" color: #00F;">[158] </span>tackle decision-making challenges at unsignalized intersections using a value decomposition-based<a name="bookmark137">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="620" height="328" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_019.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark138">Fig. 7.  Representative scenarios in di</a><span class="s8">ff</span><a href="#bookmark316" class="s48">erent simulators: (a) SUMO </a><span style=" color: #00F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;">[159]</span><a href="#bookmark317" class="s48">; (b) CityFlow </a><span style=" color: #00F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;">[160]</span><a href="#bookmark318" class="s48">; (c) SMARTS </a><span style=" color: #00F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;">[161]</span><a href="#bookmark277" class="s48">; (d) MetaDrive </a><span style=" color: #00F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;">[120]</span><a href="#bookmark319" class="s48">; (e) CARLA </a><span style=" color: #00F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;">[162]</span>;<a name="bookmark139">&zwnj;</a><a name="bookmark140">&zwnj;</a><a name="bookmark141">&zwnj;</a><a name="bookmark142">&zwnj;</a></p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a href="#bookmark320" class="s48" name="bookmark143">(f) MACAD </a><span style=" color: #00F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;">[163]</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">MARL framework, QMIX. Their enhanced QMIX implemen- tation introduces network-level optimizations, TD updates with discount factor <span class="s13">γ</span>, and reward clipping techniques, all of which contribute to improved learning stability and more reliable cooperative control. To  further  improve  learning  e<span class="s9">ffi</span><a href="#bookmark179" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ciency in complex intersection scenarios, Spatharis et al. </a><span style=" color: #00F;">[19] </span>pro- posed a Route-Agent Cooperative Multi-Agent Reinforcement Learning (RA-CoMARL) framework for autonomous driving at unsignalized intersections, which introduces route agents as a scalable decision-making structure and a collision prediction mechanism to enhance tra<span class="s9">ffi</span><a href="#bookmark178" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c coordination, reduce congestion, and improve safety. Building upon these advancements, Liu et al. </a><span style=" color: #00F;">[18] </span>introduce the Multi-Agent Game-prior Attention DDPG (MA-GA-DDPG) algorithm. This framework integrates an attention mechanism to capture inter-vehicle dependencies while incorporating a hierarchical game-theoretic structure to enhance safety, e<span class="s9">ffi</span>ciency, and learning stability. By leveraging these mechanisms, MA-GA-DDPG ensures robust and scal- able cooperative decision-making for CAVs at unsignalized intersections, paving the way for more adaptive and resilient intersection management strategies.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s12" style="display: inline;"><a name="bookmark144">Simulation Platforms</a></p></li></ol><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark145">The simulation platforms, which act as the training</a><span class="s9">/</span>learning environments, play an important role in the performance and adaptability of the autonomous agents. The learning platforms provide di<span class="s9">ff</span>erent challenges for CAV control by considering di<span class="s9">ff</span>erent tra<span class="s9">ffi</span>c scenarios and di<span class="s9">ff</span><a href="#bookmark138" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">erent characters of agents (Fig. </a><span style=" color: #00F;">7</span>). The tra<span class="s9">ffi</span>c ecosystem<span class="s9">/</span>simulation platform incorpo- rates realistic models, physics-based simulations, and e<span class="s9">ffi</span><a href="#bookmark321" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient numerical solvers, providing a controllable environment to accelerate the development of CAVs </a><span style=" color: #00F;">[164]</span>. Comparisons of di<span class="s9">ff</span><a href="#bookmark146" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">erent simulation platforms for CAVs are shown in Table </a><span style=" color: #00F;">IV</span>.</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><b>SUMO </b>(Simulation of Urban MObility) is an open- source, large-scale tra<span class="s9">ffi</span>c simulator widely used for studying platooning (1D), lane coordination (2D), and tra<span class="s9">ffi</span><a href="#bookmark316" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c flow opti- mization (3D) </a><span style=" color: #00F;">[159]</span>. It supports customizable road networks, real-time tra<span class="s9">ffi</span>c data integration, and reinforcement learning experiments, making it a versatile tool for evaluating CAV interactions.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><b>CityFlow </b>is a high-performance tra<span class="s9">ffi</span><a href="#bookmark317" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c simulator designed for large-scale, city-wide coordination </a><span style=" color: #00F;">[160]</span>. With its e<span class="s9">ffi</span>cient data structures and multi-agent RL compatibility, it excels in optimizing TSC (3D) and large-scale intersection coordination while maintaining computational e<span class="s9">ffi</span>ciency.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><b>SMARTS </b><a href="#bookmark318" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">(Scalable Multi-Agent RL Training School) is a multi-agent RL platform tailored for cooperative CAV learn- ing </a><span style=" color: #00F;">[161]</span>. It provides realistic tra<span class="s9">ffi</span>c behaviors, customizable scenarios, and benchmark tasks, making it ideal for studying CAV interactions at intersections (3D) and lane coordination (2D) under mixed-tra<span class="s9">ffi</span>c conditions.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><b>MetaDrive </b>o<span class="s9">ff</span>ers infinite procedurally generated driving environments, balancing realism with e<span class="s9">ffi</span><a href="#bookmark277" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ciency </a><span style=" color: #00F;">[120]</span>. It is particularly well-suited for studying generalizable RL in high- way merging (2D) and intersection scenarios (3D) while maintaining lightweight computational requirements.</p><h2 style="padding-left: 5pt;text-indent: 9pt;text-align: right;">CARLA<a href="#bookmark319" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, is a high-fidelity 3D driving simulator with realis- tic physics and sensor modeling, enabling MARL research in complex urban settings </a><span class="s11">[162]</span><span class="p">. It supports CAV coordination under dynamic environments, making it ideal for advanced intersection management (3D) and adaptive lane merging (2D). </span>MACAD <a href="#bookmark320" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">(Multi-Agent Connected and Autonomous Driv- ing) provides a multi-agent RL testbed for CAVs based on partially observable Markov games </a><span class="s11">[163]</span><span class="p">. It supports cooper- ative intersection navigation (3D) and deep RL-based vehicle interaction studies, making it a valuable tool for high-level</span></h2><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">CAV coordination research.</p><p class="s6" style="padding-top: 2pt;padding-left: 66pt;text-indent: 0pt;text-align: center;">TABLE IV</p><p class="s6" style="padding-top: 2pt;padding-left: 66pt;text-indent: 0pt;text-align: center;"><a name="bookmark146">C</a><span class="s16">OMPARISONS </span>B<span class="s16">ETWEEN </span>S<span class="s16">OME </span>T<span class="s16">YPICAL </span>CAV C<span class="s16">ONTROL </span>S<span class="s16">IMULATION </span>P<span class="s16">LATFORMS</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="686" height="188" alt="image" src="2025-Multi-Agent_Reinforcement_Learning_for_Connected_and_Automated_Vehicles_Control_Recent_Advancements_and_Future_Prospects/Image_020.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><a name="bookmark147">More simulators for CAVs have been designed to consider more detailed characters and versatile scenarios, which pro- vides more tools to mitigate the gaps between the simulation and reality.</a></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">A <b>PCMA </b><a href="#bookmark322" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">(pedestrian crash avoidance mitigation) system is a MARL-based system designed to study CAV-pedestrian interactions in unmarked crosswalks </a><span style=" color: #00F;">[165]</span>. By modeling pedestrian behaviors with both predefined and DRL-based adaptive strategies, it evaluates collision rates, tra<span class="s9">ffi</span>c flow e<span class="s9">ffi</span>ciency, and observation uncertainty, enabling safer and more adaptive CAV decision-making.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><b>Nocturne </b><a href="#bookmark323" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">is a 2D driving simulator specifically designed for studying partial observability in multi-agent driving </a><span style=" color: #00F;">[166]</span>. It incorporates real-world data-driven scenarios and e<span class="s9">ffi</span>ciently simulates the limited visibility conditions that CAVs encounter, such as occlusions at intersections or sensor limitations.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Generally speaking, the simulation-based evaluations are simpler to implement, reproduce, and scale compared with real experiments for learning. However, simulations may not capture all the challenges associated with an actual deploy- ment. For example, factors such as network delay, vehicle model discrepancies, computation  time,  and  the  necessity of implementing clock synchronization and fail-safe routines pose challenges in real-world implementations.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 74pt;text-indent: -18pt;text-align: left;"><p style="display: inline;"><a name="bookmark148">D</a><span class="s6">ISCUSSION </span>&amp; F<span class="s6">UTURE </span>W<span class="s6">ORK</span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">CAVs leveraging MARL algorithms present a promising frontier in the evolution of transportation.  As  urban  cen- ters grapple with tra<span class="s9">ffi</span>c congestion and road safety, the nuanced and adaptive behaviors o<span class="s9">ff</span>ered by MARL algo- rithms can enable CAVs to respond more fluidly to the actions of other road participants. The dynamism of MARL allows these vehicles to cooperatively optimize tra<span class="s9">ffi</span>c flows, potentially alleviating congestion and reducing transit times. Nevertheless, the journey to this future is not without its challenges. Interactions with unpredictable human drivers, managing the delicate balance between exploration and safety in learning, and navigating a complex regulatory land- scape are just a few of the hurdles. Yet, with ongoing collaborations among tech giants, startups, and academic cir- cles, there is a collective push to overcome these barriers,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">signaling an optimistic trajectory for the marriage of MARL and CAVs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l12"><li style="padding-top: 6pt;padding-left: 19pt;text-indent: -13pt;text-align: left;"><p class="s12" style="display: inline;">Safety Challenges</p><ol id="l13"><li style="padding-top: 4pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p style="display: inline;"><a name="bookmark149"><i>Current Progress: </i></a>Safety is a fundamental concern in the deployment of MARL for CAVs, as it directly a<span class="s9">ff</span><a href="#bookmark324" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ects public trust, regulatory approval, and real-world applicabil- ity. Current research has explored various safety-enhancing strategies, including safety-constrained policy training, coop- erative decision-making frameworks, and hierarchical MARL methods </a><span style=" color: #00F;">[168]</span><a href="#bookmark325" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Some approaches enforce hard constraints or incorporate barrier functions into MARL training to restrict unsafe actions, while others modify reward functions to penalize high-risk behaviors </a><span style=" color: #00F;">[169]</span>. Additionally, graph-based learning has been leveraged to enhance inter-vehicle coordi- nation, reducing the risk of conflicts in dense tra<span class="s9">ffi</span>c scenarios. Safe MARL frameworks have also been introduced for specific driving situations, such as lane merging, intersection coordina- tion, and emergency braking scenarios, where explicit safety constraints can be embedded into agent interactions. However, despite these advancements, the majority of existing methods rely on empirical safety validation rather than theoretical guarantees, making their applicability to real-world tra<span class="s9">ffi</span><a href="#bookmark326" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c con- ditions uncertain. Furthermore, the challenge of generalization to diverse and unseen scenarios persists, as most safety mech- anisms are designed and evaluated in constrained simulation settings rather than in open-ended, adversarial, or extreme conditions </a><span style=" color: #00F;">[170]</span>.</p></li><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p style="display: inline;"><i>Research Gaps: </i>Despite the progress in safety-aware MARL for CAVs, several fundamental challenges remain unre- solved. First, formal safety guarantees are largely absent, as most existing safety frameworks rely on heuristic-based reward shaping, risk-sensitive training, or pre-defined constraints that do not provide mathematically rigorous assurances of policy safety. To achieve provable safety, future research must explore <i>control-theoretic safety verification</i>, <i>distributionally robust optimization</i>, and formal methods that can certify MARL policies under <i>worst-case scenarios</i>. Second, uncertainty in real-world driving environments—including stochastic tra<span class="s9">ffi</span>c behaviors, sensor noise, adversarial interactions, and rare edge cases—poses a major obstacle to safe MARL policy</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark327" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark150">learning </a><span style=" color: #00F;">[171]</span>. While some methods incorporate robust train- ing techniques, they often struggle with out-of-distribution generalization when deployed in novel, high-risk tra<span class="s9">ffi</span>c sce- narios. Third, safe exploration remains a significant open challenge, as conventional RL exploration strategies may lead to high-risk behaviors during training, potentially resulting in catastrophic failures. Safe RL techniques, such as conserva- tive policy learning, shielding mechanisms, and risk-aware exploration strategies, require further refinement to balance exploration e<span class="s9">ffi</span>ciency and safety constraints. Future research should focus on developing provably safe MARL frameworks, improving robustness to real-world uncertainties, and advanc- ing risk-sensitive training methods that enable MARL agents to operate reliably in diverse, dynamic, and unpredictable tra<span class="s9">ffi</span>c conditions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 19pt;text-indent: -13pt;text-align: justify;"><p class="s12" style="display: inline;">Communication Challenges</p><ol id="l14"><li style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s11" style="display: inline;"><a name="bookmark151"><span class="s12">Current Progress: </span></a><span style=" color: #000;">E</span><span class="s9">ff</span><a href="#bookmark328" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">ective communication is pivotal in MARL for CAVs, enabling collaboration, information sharing, and coordinated decision-making through reliable protocols </a>[172]<span style=" color: #000;">. First, the development of advanced communication protocols tailored specifically for CAVs is a pressing need. Existing research focuses on protocols that enable e</span><span class="s9">ffi</span><a href="#bookmark329" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient data sharing </a>[173]<a href="#bookmark279" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. For instance, research endeavors have delved into the utilization of graph-based MARL algorithms </a>[122]<a href="#bookmark293" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, such as those explored in </a>[136] <a href="#bookmark295" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">and </a>[138]<span style=" color: #000;">, to enable e</span><span class="s9">ffi</span><span style=" color: #000;">- cient and adaptive message exchange among agents. Second, the challenge of ensuring robust communication in dynam- ically evolving tra</span><span class="s9">ffi</span><a href="#bookmark330" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c scenarios represents another critical aspect that deserves meticulous attention </a>[174]<a href="#bookmark314" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. For exam- ple, in </a>[157]<span style=" color: #000;">, a variational auto-encoder algorithm combined with advanced multi-head attention mechanisms is proposed to extract and retain valuable information from neighboring agents while making e</span><span class="s9">ffi</span><a href="#bookmark265" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient use of constrained communi- cation resources. Last but not least, it is worth noting that managing unstable communication channels, particularly in terms of latency, emerges as a particularly intriguing consid- eration in the context of MARL for CAVs </a>[108]<a href="#bookmark265" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. In </a>[108]<span style=" color: #000;">, the double deep Q-learning algorithm is proposed to jointly train the agents to maximize the sum rate of V2N links while ensuring the desired packet delivery probability for each V2V link within specified latency constraints.</span></p></li><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p class="s11" style="display: inline;"><a name="bookmark152"><span class="s12">Research Gaps: </span></a><a href="#bookmark289" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">While communication in MARL is a well-explored domain, its adaptation and application within CAVs present distinctive research gaps, which necessitate further exploration </a>[132]<a href="#bookmark216" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. For instance, existing works often overlook pressing real-world concerns in the realm of CAVs, such as communication costs and the challenges posed by noisy environments. These critical factors, when ignored, can introduce substantial obstacles  to  the  practical  deployment of CAV technology. MARL algorithms applied in computer science domains are expected to be explored for  use  in CAV applications within challenging environments marked by limited bandwidth </a>[59] <a href="#bookmark331" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">and noisy communication channels </a>[175]<a href="#bookmark35" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. Moreover, the majority of current MARL algorithms in CAVs rely on CTDE (Sec. </a>II<span style=" color: #000;">), potentially posing a significant privacy threat within the realm of communication.  In  the field  of  single-agent  RL,  security  and  privacy  issues  have</span></p><p class="s11" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark332" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" name="bookmark153">garnered extensive attention </a>[176]<span style=" color: #000;">. However, in the context of MARL, and similarly in the domain of CAVs, these areas have remained relatively unexplored. This suggests a promising area for future research that merits exploration.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s12" style="display: inline;">Mixed-Tra<span class="p">ffi</span>c  Challenges</p><ol id="l15"><li style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p style="display: inline;"><a name="bookmark154"><i>Current Progress: </i></a>One of the formidable challenges posed by CAVs stems from navigating the intricate dynamics of mixed tra<span class="s9">ffi</span><a href="#bookmark333" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c, wherein CAVs share the road with various other road users, including human-driven vehicles, bicycles, e- scooters, and an array of other modes of transportation </a><span style=" color: #00F;">[177]</span><a href="#bookmark334" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, </a><span style=" color: #00F;">[178]</span><a href="#bookmark335" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. This multifaceted coexistence necessitates advanced MARL algorithms to ensure seamless and safe interactions among these diverse road users. Current research into MARL algorithms for CAV applications primarily concentrates on integrating the prediction of human behavior into the decision- making process </a><span style=" color: #00F;">[179]</span><a href="#bookmark190" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">. For instance, in </a><span style=" color: #00F;">[30]</span>, the authors introduce a novel MARL framework with a  priority-based safety supervisor designed to preempt potential collisions. This is achieved by predicting  future trajectories, encompassing both CAVs and HDVs and then the unsafe actions generated by MARL agents will be replaced with safe actions, thereby contributing to the enhancement of safety in mixed tra<span class="s9">ffi</span>c scenarios. However, it is important to note that human-driven vehicles can introduce an additional layer of unpredictability into the tra<span class="s9">ffi</span>c environment, since mixed tra<span class="s9">ffi</span>c involves a diverse range of road  users,  presenting  unique  challenges for the seamless integration of CAVs into the existing road ecosystem.<a name="bookmark155">&zwnj;</a><a name="bookmark156">&zwnj;</a></p></li><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p style="display: inline;"><a name="bookmark157"><i>Research Gaps: </i></a>Given the diverse spectrum of road users, future research endeavors will encompass the develop- ment of real-time, adaptive safety assessment and intervention strategies capable of  responding  to  the  dynamic  nature of tra<span class="s9">ffi</span>c conditions. Another significant aspect entails the establishment of novel coordination and communication mech- anisms between CAVs  and  other  road  users.  For  instance, as the presence of e-scooters on the roads continues  to surge, there is a growing imperative to develop e<span class="s9">ffi</span><a href="#bookmark336" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cient methods for coordinating the interactions between CAVs and e-scooters </a><span style=" color: #00F;">[180]</span>. Furthermore, the existing MARL algorithms are typically trained within specific scenarios, underscoring the necessity for research aimed at enhancing their capacity to generalize across a wide range of mixed tra<span class="s9">ffi</span>c conditions and environmental contexts.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p class="s12" style="display: inline;">Sim-to-Real Challenges</p><ol id="l16"><li style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p style="display: inline;"><i>Current Progress: </i><a href="#bookmark144" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">The transition from simulated training environments to real-world deployment remains a fundamental challenge in MARL-based CAV control. While deep MARL has achieved significant breakthroughs in simulation environ- ments (see Sec. </a><span style=" color: #00F;">III-D</span>), models often su<span class="s9">ff</span>er from performance degradation in real-world tra<span class="s9">ffi</span>c due to discrepancies in traf- fic dynamics, sensor uncertainties, and unforeseen driving behaviors. To mitigate this sim-to-real gap, researchers have explored domain adaptation techniques, risk-sensitive training frameworks, and hybrid o<span class="s9">ffl</span>ine-online RL approaches. O<span class="s9">ffl</span>ine RL,  which  trains  policies  using  pre-collected  datasets,  has</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">also been considered to improve safety and data e<span class="s9">ffi</span>ciency. However, its applicability to MARL-based CAVs remains an open challenge. Another emerging approach for enhancing generalization and policy transfer is Transformer RL (TRL). Unlike traditional RL architectures that rely on recurrent net- works like LSTMs, TRL leverages self-attention mechanisms to capture long-range dependencies in sequential decision- making. This enables MARL agents to better process complex tra<span class="s9">ffi</span>c patterns, adapt to diverse driving scenarios, and improve multi-agent coordination. Moreover, TRL scales e<span class="s9">ffi</span>ciently with large interaction datasets, making it a promising tool for handling the high-dimensional decision-making required for real-world CAV deployment.</p></li><li style="padding-left: 5pt;text-indent: 9pt;text-align: justify;"><p style="display: inline;"><a name="bookmark158"><i>Research Gaps: </i></a>The primary challenge in sim-to-real transfer lies in the inherent unpredictability of real-world environments, where tra<span class="s9">ffi</span><a href="#bookmark337" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">c behaviors, adversarial interac- tions, and rare safety-critical events deviate from synthetic training data </a><span style=" color: #00F;">[181]</span>. Most MARL policies struggle to gen- eralize beyond controlled simulation settings, as existing domain adaptation techniques fail to account for com- plex, evolving dynamics. Real-world uncertainties, including sensor noise, connectivity disruptions, and multi-agent coor- dination failures, further degrade performance. Additionally, the lack of real-world benchmarks  and  standardized  test- ing frameworks hinders objective evaluation, making it di<span class="s9">ffi</span><a href="#bookmark338" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">cult to measure and compare progress in sim-to-real adaptation </a><span style=" color: #00F;">[182]</span>. Ethical, legal, and regulatory constraints further complicate direct deployment, as autonomous decision- making in safety-critical  scenarios  demands  a  high  level of explainability, accountability, and formal safety guaran- tees. Future research must focus on developing adaptive, risk-aware policies, leveraging real-time learning frame- works, improving uncertainty quantification, and establishing regulatory-compliant evaluation methodologies to ensure the scalable and responsible deployment of MARL-based CAV control.<a name="bookmark159">&zwnj;</a></p></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 110pt;text-indent: -14pt;text-align: left;"><p style="display: inline;"><a name="bookmark160">C</a><span class="s6">ONCLUSION</span></p></li></ol></li></ol><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Recently, MARL has emerged as a focal point of interest within the realm of CAVs. The capability of MARL to tackle intricate control and coordination challenges in CAVs has unlocked new horizons for the development of intelligent and e<span class="s9">ffi</span>cient next-generation transportation systems. This review has undertaken a comprehensive exploration of the applica- tions of MARL in the control of CAVs.</p><ol id="l17"><li style="padding-top: 1pt;padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">The review began with an  overview  of  single-agent RL techniques and an extensive survey of the diverse landscape of MARL architectural variants, facilitating a deeper understanding of applications to both individual and collective agent behaviors.</p></li><li style="padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">Our analysis categorized these  contributions  based on di<span class="s9">ff</span>erent degrees of control, ranging from one- dimensional to two-dimensional and three-dimensional collaboration. Within this multifaceted arena,  MARL has exhibited remarkable prowess, demonstrating its promising performance in various CAV control tasks. These include cooperative endeavors such as platooning control,  lane-changing  maneuvers,  on-ramp  merging,</p><p style="padding-top: 2pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">tra<span class="s9">ffi</span>c  signal  coordination,  and  unsignalized  intersec- tions, among others.</p></li><li style="padding-left: 30pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">We highlighted significant challenges in designing and evaluating MARL  methods  within  the  CAV.  Striking a balance between performance, scalability, and safety remains a complex challenge.</p></li></ol><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">This review serves as a crucial resource, aimed at encour- aging further research and the practical application of MARL within the field of CAVs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: center;">R<span class="s6">EFERENCES</span></p><p class="s6" style="padding-top: 6pt;padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark161">[1] G. Wang, X. Wang, and S. Li, “A guidance module based forma- tion control scheme for multi-mobile robot systems with collision avoidance,” </a><i>IEEE Trans. Autom. Sci. Eng.</i>, vol. 21, no. 1, pp. 382–393, Jan. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark162">[2] K. Wu, J. Hu, Z. Ding, and F. Arvin, “Finite-time  fault-tolerant formation control for distributed multi-vehicle networks with bear- ing measurements,” </a><i>IEEE Trans. Autom.  Sci.  Eng.</i>,  vol. 21,  no. 2, pp. 1346–1357, Feb. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark163">[3] B. Li et al., “Tra</a><span class="s8">ffi</span>c-aware ecological cruising control for connected electric vehicle,” <i>IEEE Trans. Transport. Electrific.</i>, vol. 10, no. 3, pp. 5225–5240, Sep. 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark164">[4] M. Hua, G. Chen, C. Zong, and L. He, “Research on synchronous control strategy of steer-by-wire system with  dual  steering  actu- ator motors,” </a><i>Int. J. Veh. Auton. Syst.</i>, vol. 15,  no. 1,  pp. 50–76, 2020.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark165">[5] T. Garg and G. Kaur, “A systematic review on intelligent transport systems,” </a><i>J. Comput. Cognit. Eng.</i>, vol. 2, no. 3, pp. 175–188, Jun. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark166">[6] M. Hua, G. Chen, B. Zhang, and Y. Huang, “A hierarchical energy e</a><span class="s8">ffi</span>ciency optimization control strategy for distributed drive electric vehicles,” <i>Proc. Inst. Mech. Eng. D, J. Automobile Eng.</i>, vol. 233, no. 3, pp. 605–621, 2019.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark167">[7] Y. Lin, J. McPhee, and N. L. Azad, “Comparison of deep  rein- forcement learning and model predictive control for adaptive cruise control,” </a><i>IEEE Trans. Intell. Vehicles</i>, vol. 6,  no. 2,  pp. 221–231, Jun. 2021.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark168">[8] A. Katriniok, B. Rosarius, and P. Ma¨ho¨nen, “Fully distributed model predictive control of connected automated vehicles in intersections: Theory and vehicle experiments,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 23, no. 10, pp. 18288–18300, Oct. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark169">[9] D. Chen, K. Zhang, Y. Wang, X. Yin, Z. Li, and D. Filev, “Communication-e</a><span class="s8">ffi</span>cient decentralized multi-agent reinforcement learning for cooperative adaptive cruise control,” <i>IEEE Trans. Intell. Vehicles</i><a href="http://dx.doi.org/10.1109/TIV.2024.3368025" class="s48" target="_blank">, early access, Feb. 21, 2024, doi: </a><a href="http://dx.doi.org/10.1109/TIV.2024.3368025" class="s49" target="_blank">10.1109</a><a href="http://dx.doi.org/10.1109/TIV.2024.3368025" class="s50" target="_blank">/</a><span style=" color: #004392; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;">TIV.2024.3368025</span>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark170">[10] W. Liu et al., “A systematic survey of control techniques and applica- tions in connected and automated vehicles,” </a><i>IEEE Internet Things J.</i>, vol. 10, no. 24, pp. 21892–21916, Dec. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark171">[11] D.  Guo,  H.  Ding,  L.  Tang,  X.  Zhang,  L.  Yang,  and  Y.-C.  Liang, “A proactive eavesdropping game in MIMO systems based on mul- tiagent deep reinforcement learning,” </a><i>IEEE Trans. Wireless Commun.</i>, vol. 21, no. 11, pp. 8889–8904, Nov. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark172">[12] X. Bai, A. Fielbaum, M. Kronmuller, L. Knoedler, and J. Alonso- Mora, “Group-based distributed auction algorithms for  multi-robot task assignment,” </a><i>IEEE Trans. Autom.  Sci.  Eng.</i>,  vol. 20,  no. 2, pp. 1292–1303, Apr. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark173">[13] X. Wang, D. Ye, L. Zhang, and X. Zhao, “Prescribed performance tracking control for nonlinear multiagent systems with distributed observation errors compensation,” </a><i>IEEE Trans. Autom.  Sci.  Eng.</i>, vol. 22, pp. 5182–5192, 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark174">[14] Z. E. Liu, Q. Zhou, Y. Li, S. Shuai, and H. Xu, “Safe deep rein- forcement learning-based constrained optimal control scheme for HEV energy management,” </a><i>IEEE Trans. Transport. Electrific.</i>, vol. 9, no. 3, pp. 4278–4293, Sep. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark175">[15] Z. E. Liu et al., “Deep reinforcement learning-based energy man- agement for heavy duty HEV considering discrete-continuous hybrid action space,” </a><i>IEEE Trans.  Transport.  Electrific.</i>,  vol. 10,  no. 4, pp. 9864–9876, Dec. 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark176">[16] H. Shu, T. Liu, X. Mu, and D. Cao, “Driving tasks transfer using deep reinforcement learning for decision-making of autonomous vehicles in unsignalized intersection,” </a><i>IEEE Trans. Veh. Technol.</i>, vol. 71, no. 1, pp. 41–52, Jan. 2022.</p><p class="s6" style="padding-top: 2pt;padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark177">[17] M. Zhou, Y. Yu, and X. Qu, “Development of an e</a><span class="s8">ffi</span>cient driving strat- egy for connected and automated vehicles at signalized intersections: A reinforcement learning approach,” <i>IEEE Trans. Intell. Transport. Syst.</i>, vol. 21, no. 1, pp. 433–443, Jan. 2020.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark178">[18] J. Liu, P. Hang, X. Na, C. Huang, and J. Sun, “Cooperative decision-making  for   CAVs   at   unsignalized   intersections:   A Marl  approach  with  attention  and  hierarchical  game  priors,” </a><i>IEEE Trans. Intell. Transport. Syst.</i>, vol. 26,  no. 1,  pp. 443–456, Jan. 2025.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark179">[19] C. Spatharis and K. Blekas, “Multiagent reinforcement learning for autonomous driving in tra</a><span class="s8">ffi</span>c zones with unsignalized intersections,”</p><p class="s7" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">J. Intell. Transp. Syst.<span class="s6">, vol. 28, no. 1, pp. 103–119, Jan. 2024.</span></p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark180">[20] L. Liu, X. Li, Y. Li, J. Li, and Z. Liu, “Reinforcement-Learning-Based multilane cooperative control for on-ramp merging in mixed-autonomy tra</a><span class="s8">ffi</span>c,” <i>IEEE Internet Things J.</i>, vol. 11, no. 24, pp. 39809–39819, Dec. 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark181">[21] Q. Hou and J. Dong, “Distributed dynamic event-triggered consensus control for multiagent systems with guaranteed performance and pos- itive inter-event times,” </a><i>IEEE Trans. Autom. Sci. Eng.</i>, vol. 21, no. 1, pp. 746–757, Jan. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark182">[22] J. Yang, J. Ni, M. Xi, J. Wen, and Y. Li,  “Intelligent  path planning of underwater robot based on reinforcement learning,” </a><i>IEEE  Trans.  Autom.  Sci.  Eng.</i>,  vol. 20,  no. 3,   pp. 1983–1996, Mar. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark183">[23] E. Palacios-Morocho, S. Inca, and J. F. Monserrat, “Enhancing cooperative multi-agent systems with self-advice and near-neighbor priority collision control,” </a><i>IEEE Trans. Intell. Vehicles</i>, vol. 9, no. 1, pp. 2864–2877, Jan. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark184">[24]  K. Zhang, Z. Yang, and T. Bas¸ar, “Multi-agent reinforcement learning: A selective overview of theories and algorithms,” in </a><i>Handbook of Reinforcement Learning and Control</i>. Cham, Switzerland: Springer, Jun. 2021, pp. 321–384.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark185">[25]   M. Bettini, R. Kortvelesy, J. Blumenkamp, and A. Prorok, “VMAS: A vectorized multi-agent simulator for collective robot learning,” in </a><i>Proc. Int. Symp. Distrib. Auto. Robotic Syst. </i>Cham, Switzerland: Springer, Jan. 2024, pp. 42–56.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark186">[26] A. Irshayyid, J. Chen, and G. Xiong, “A review on reinforcement learning-based highway autonomous vehicle control,” </a><i>Green Energy Intell. Transp.</i>, vol. 3, no. 4, Aug. 2024, Art. no. 100156.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark187">[27] S. Han et al., “A multi-agent reinforcement learning approach for safe and e</a><span class="s8">ffi</span>cient behavior planning of connected autonomous vehicles,” <i>IEEE Trans. Intell. Transport. Syst.</i>, vol. 25, no. 5, pp. 3654–3670, May 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark188">[28]  B.  Hegde  and  M.  Bouroche,   “Multi-agent   reinforcement learning for safe lane changes by  connected  and  autonomous vehicles: A survey,” </a><i>AI Commun.</i>, vol. 37,  no. 2,  pp. 203–222, Apr. 2024.</p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark189">[29]    R.   K.   Dubey,   D.   Dailisan,   J.   Argota   Sa´nchez–Vaquerizo,   and</a></p><p class="s6" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">D. Helbing, “FAIRLANE:  A multi-agent approach  to priority lane management in diverse tra<span class="s8">ffi</span>c composition,” <i>Transp. Res. C, Emerg. Technol.</i>, vol. 171, Feb. 2025, Art. no. 104919.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark190">[30] D. Chen et al., “Deep multi-agent reinforcement learning for highway on-ramp merging in mixed tra</a><span class="s8">ffi</span>c,” <i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 24, no. 11, pp. 11623–11638, Nov. 2023.</p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark191">[31]    P.-M.  Liu,  X.-G.  Guo,  J.-L.  Wang,  X.-P.  Xie,  and  F.-W.  Yang,</a></p><p class="s6" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">“Fully distributed hierarchical et intrusion-and fault-tolerant group control for  mass  with  application  to  robotic  manipulators,” <i>IEEE  Trans.  Autom.  Sci.  Eng.</i>,   vol. 21,   no. 3,   pp. 2868–2881, Jul. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark192">[32] Z. Chen et al., “Data-driven methods applied to soft robot model- ing and control: A review,” </a><i>IEEE Trans. Autom. Sci. Eng.</i>, vol. 22, pp. 2241–2256, 2025.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark193">[33] P. Yadav, A. Mishra, and S. Kim, “A comprehensive survey on multi- agent reinforcement learning for connected and automated vehicles,” </a><i>Sensors</i>, vol. 23, no. 10, p. 4710, May 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark194">[34] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement learning: A survey,” </a><i>J. Artif. Intell. Res.</i>, vol. 4, no. 1, pp. 237–285, Jan. 1996.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark195">[35] V. Mnih et al., “Asynchronous methods for deep reinforcement learning,” in </a><i>Proc. Int. Conf. Mach. Learn.</i>, 2016, pp. 1928–1937.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark196">[36] T. Chu, J. Wang, L. Codeca`, and Z. Li, “Multi-agent deep reinforcement learning for large-scale tra</a><span class="s8">ffi</span>c signal control,” <i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 21, no. 3, pp. 1086–1095, May 2019.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark197">[37] J. Wang, M. Yuan, Y. Li, and Z. Zhao, “Hierarchical attention Master–Slave for heterogeneous multi-agent reinforcement learning,” </a><i>Neural Netw.</i>, vol. 162, pp. 359–368, May 2023.</p><p class="s6" style="padding-top: 2pt;padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark198">[38] A. Mahajan, T. Rashid, M. Samvelyan, and S. Whiteson, “MAVEN: Multi-agent variational exploration,” in </a><i>Proc. Adv. Neural Inf. Process. Syst.</i>, Jan. 2019, pp. 7611–7622.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark199">[39] W. Li et al., “Multi-agent evolution reinforcement learning method for machining parameters optimization based on bootstrap aggregating graph attention network simulated environment,”  </a><i>J.  Manuf.  Syst.</i>, vol. 67, pp. 424–438, Apr. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark200">[40] D. Qiu, J. Wang, Z. Dong, Y. Wang, and G. Strbac, “Mean-field multi- agent reinforcement learning for peer-to-peer multi-energy trading,” </a><i>IEEE Trans. Power Syst.</i>, vol. 38, no. 5, pp. 4853–4866, Sep. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark201">[41] Z. Wang, Y. Xue, L. Liu, H. Zhang, C. Qu, and C. Fang, “Multi-agent DRL-controlled connected and automated vehicles in mixed  tra</a><span class="s8">ffi</span>c with time delays,” <i>IEEE Trans. Intell. Transport. Syst.</i>, vol. 25, no. 11, pp. 17676–17688, Nov. 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark202">[42] Z. Yan and C. Wu, “Scalability of platoon-based coordination for mixed autonomy intersections,” in </a><i>Proc. IEEE</i>/<i>RSJ Int. Conf. Intell. Robots Syst. (IROS)</i>, Oct. 2024, pp. 10304–10311.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark203">[43] Y. Xiao, Y. Song, and J. Liu, “Collaborative multi-agent deep reinforce- ment learning for energy-e</a><span class="s8">ffi</span>cient resource allocation in heterogeneous mobile edge computing networks,” <i>IEEE Trans. Wireless Commun.</i>, vol. 23, no. 6, pp. 6653–6668, Jun. 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark204">[44] M. Tan, “Multi-agent reinforcement learning: Independent vs. coopera- tive agents,” in </a><i>Proc. 10th Int. Conf. Mach. Learn.</i>, 1993, pp. 330–337.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark205">[45] T. Chu and U. Kalabic´, “Model-based deep reinforcement learning for CACC in mixed-autonomy vehicle platoon,” in </a><i>Proc. IEEE 58th Conf. Decis. Control (CDC)</i>, Dec. 2019, pp. 4079–4084.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark206">[46] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications,” </a><i>IEEE Trans. Cybern.</i>, vol. 50, no. 9, pp. 3826–3839, Sep. 2020.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark207">[47] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, “Fully decentralized multi-agent reinforcement learning with networked agents,” in </a><i>Proc. Int. Conf. Mach. Learn.</i>, 2018, pp. 5872–5881.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark208">[48] R. F. Prudencio, M. R. O. A. Maximo, and E. L. Colombini, “A sur- vey on o</a><span class="s8">ffl</span>ine reinforcement learning: Taxonomy, review, and open problems,” <i>IEEE Trans. Neural Netw.  Learn.  Syst.</i>,  vol. 35,  no. 8, pp. 10237–10257, Aug. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark209">[49] Y. Ran, Y.-C. Li, F. Zhang, Z. Zhang, and Y. Yu, “Policy regularization with dataset constraint for o</a><span class="s8">ffl</span>ine reinforcement learning,” in <i>Proc. Int. Conf. Mach. Learn.</i>, Jan. 2023, pp. 28701–28717.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark210">[50] A. Kumar, A. Zhou, G. Tucker, and S. Levine, “Conservative Q-learning for o</a><span class="s8">ffl</span>ine reinforcement learning,” in <i>Proc. Int. Conf. Adv. Neural Inf. Process. Syst.</i>, vol. 33, 2020, pp. 1179–1191.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark211">[51] Q. Wang et al., “Identifying expert behavior in o</a><span class="s8">ffl</span>ine training datasets improves behavioral cloning of robotic manipulation policies,” <i>IEEE Robot. Autom. Lett.</i>, vol. 9, no. 2, pp. 1294–1301, Feb. 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[52] T. Rashid, G. Farquhar, B. Peng, and S. Whiteson, “Weighted QMIX: Expanding monotonic value function factorisation for deep multi- agent reinforcement learning,” in <i>Proc. Adv. Neural Inf. Process. Syst.</i>, Jan. 2020, pp. 10199–10210.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[53] Y. Zhang, H. Ma, and Y. Wang, “AVD-net: Attention value decompo- sition network for deep multi-agent reinforcement learning,” in <i>Proc. 25th Int. Conf. Pattern Recognit. (ICPR)</i>, Jan. 2021, pp. 7810–7816.</p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark212">[54]    T.   Phan,   L.   Belzner,   T.   Gabor,   A.   Sedlmeier,   F.   Ritz,   and</a></p><p class="s6" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">C. Linnho<span class="s8">ff</span>–Popien, “Resilient multi-agent reinforcement learning with adversarial value decomposition,” in <i>Proc. AAAI Conf. Artif. Intell.</i>, vol. 35, May 2021, pp. 11308–11316.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark213">[55] Q. Wei, Y. Li, J. Zhang, and F.-Y. Wang, “VGN: Value decomposition with graph attention networks for multiagent reinforcement learning,” </a><i>IEEE Trans. Neural Netw. Learn. Syst.</i>, vol. 35, no. 1, pp. 182–195, Jan. 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[56]  S. Liu et al., “Learning multi-agent cooperation via considering actions of teammates,” <i>IEEE Trans. Neural Netw. Learn. Syst.</i>, vol. 35, no. 8, pp. 11553–11564, Aug. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark214">[57] Y. Niu, H. Zhao, and L. Yu, “MA-MIX: Value function decomposition for cooperative multiagent reinforcement learning based on multi-head attention mechanism,” in </a><i>Proc. 23rd Int. Conf. Auto. Agents Multiagent Syst.</i>, 2024, pp. 2402–2404.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark215">[58] T. Chu, S. Chinchali, and S. Katti, “Multi-agent reinforcement learning for networked system control,” in </a><i>Proc. Int. Conf.  Learn.  Repre- sent.</i><a href="https://openreview.net/forum?id=Syx7A3NFvH" class="s48" target="_blank">, Jan. 2020, pp. 1–16. [Online]. Available: https:</a><a href="https://openreview.net/forum?id=Syx7A3NFvH" class="s51" target="_blank">//</a><a href="https://openreview.net/forum?id=Syx7A3NFvH" class="s48" target="_blank">openreview.net</a><a href="https://openreview.net/forum?id=Syx7A3NFvH" class="s51" target="_blank">/ </a><a href="https://openreview.net/forum?id=Syx7A3NFvH" class="s48" target="_blank">forum?id</a><a href="https://openreview.net/forum?id=Syx7A3NFvH" class="s51" target="_blank">=</a><a href="https://openreview.net/forum?id=Syx7A3NFvH" class="s48" target="_blank">Syx7A3NFvH</a></p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark216">[59] R. Wang, X. He, R. Yu, W. Qiu, B. An, and Z. Rabinovich, “Learning e</a><span class="s8">ffi</span>cient multi-agent communication: An information bottleneck approach,” in <i>Proc. Int. Conf. Mach. Learn.</i>, 2020, pp. 9908–9918.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark217">[60] D. Li et al., “Verco: Learning coordinated verbal communication for multi-agent reinforcement learning,” 2024, </a><i>arXiv:2404.17780</i>.</p><p class="s6" style="padding-top: 2pt;padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark218">[61] S. Dolan, S. Nayak, J. J. Aloor, and H. Balakrishnan, “Asynchronous cooperative multi-agent reinforcement learning with limited communication,” 2025, </a><i>arXiv:2502.00558</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark219">[62] J. Yang, I. Borovikov, and H. Zha,  “Hierarchical  cooperative multi-agent reinforcement learning with skill discovery,” 2019, </a><i>arXiv:1912.03558</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark220">[63] T. Wang, T. Gupta, A. Mahajan, B. Peng, S. Whiteson, and C. Zhang, “RODE: Learning roles to decompose multi-agent tasks,” 2020, </a><i>arXiv:2010.01523</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark221">[64]  B. Li, “Hierarchical architecture for multi-agent reinforcement learning in intelligent game,” in </a><i>Proc. Int. Joint Conf. Neural Netw. (IJCNN)</i>, Jul. 2022, pp. 1–8.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark222">[65] Z. Xu, Y. Bai, B. Zhang, D. Li, and G. Fan, “HAVEN: Hierarchical cooperative multi-agent reinforcement learning with dual coordination mechanism,” in </a><i>Proc. AAAI Conf. Artif. Intell.</i>, Jun. 2023, vol. 37, no. 10, pp. 11735–11743.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark223">[66] M. Yang, Y. Yang, Z. Lu, W. Zhou, and H. Li, “Hierarchical multi- agent skill discovery,” in </a><i>Proc. Adv. Neural Inf. Process. Syst.</i>, vol. 36, 2024, pp. 1–15.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark224">[67] H. Wang, Y. Yu, and Y. Jiang, “Fully decentralized multiagent commu- nication via causal inference,” </a><i>IEEE Trans. Neural Netw. Learn. Syst.</i>, vol. 34, no. 12, pp. 10193–10202, Dec. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark225">[68] J. Li et al., “Deconfounded value decomposition for multi-agent reinforcement learning,” in </a><i>Proc. Int.  Conf.  Mach.  Learn.</i>,  2022, pp. 12843–12856.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark226">[69] R. Pina, V. De Silva, and C. Artaud, “Discovering causality for e</a><span class="s8">ffi</span>cient cooperation in multi-agent environments,” 2023, <i>arXiv:2306.11846</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark227">[70]  B. Liu, Z. Pu, Y. Pan, J. Yi, Y. Liang, and D. Zhang, “Lazy agents: A new perspective on solving sparse reward problem in multi-agent reinforcement learning,” in </a><i>Proc. Int.  Conf.  Mach.  Learn.</i>,  2023, pp. 21937–21950.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark228">[71] Z. Li et al., “Coordination as inference in multi-agent reinforcement learning,” </a><i>Neural Netw.</i>, vol. 172, Apr. 2024, Art. no. 106101.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark229">[72] K. Jiang, W. Liu, Y. Wang, L. Dong, and C. Sun, “Credit assignment in heterogeneous multi-agent reinforcement learning for fully cooperative tasks,” </a><i>Int. J. Speech Technol.</i>, vol. 53, no. 23,  pp. 29205–29222, Dec. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark230">[73] J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, “Counterfactual multi-agent policy gradients,” in </a><i>Proc. AAAI</i>, 2018, pp. 2974–2982.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark231">[74] D. Guo, L. Tang, X. Zhang, and Y. Liang, “Joint optimization  of handover control and power allocation based on multi-agent deep reinforcement learning,” </a><i>IEEE Trans. Veh. Technol.</i>, vol. 69, no. 11, pp. 13124–13138, Nov. 2020.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark232">[75] Y. Hou et al., “A multi-agent cooperative learning system with evo- lution of  social roles,” </a><i>IEEE Trans.  Evol. Comput.</i>, vol. 28,  no. 2, pp. 531–543, Apr. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark233">[76] P. Sunehag et al., “Value-decomposition networks for cooperative multi-agent learning based on team reward,” in </a><i>Proc. AAMAS</i>, 2018, pp. 2085–2087.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark234">[77] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi, “QTRAN: Learning to factorize with transformation for cooperative multi-agent reinforcement learning,” in </a><i>Proc. ICML</i>, 2019, pp. 5887–5896.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark235">[78] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multi- agent actor-critic for mixed cooperative-competitive environments,” in </a><i>Proc. Adv. Neural Inf. Process. Syst.</i>, Jan. 2017, pp. 1–19.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark236">[79] J. Foerster et al., “Stabilising experience replay for deep multi-agent reinforcement learning,” in </a><i>Proc. ICML</i>, 2017, pp. 1146–1155.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark237">[80] J. Foerster, Y. Assael, N. D. Freitas, and S. Whiteson, “Learning to communicate with deep multi-agent reinforcement learning,” in </a><i>Proc. Adv. Neural Inf. Process. Syst.</i>, Jan. 2016, pp. 1–16.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark238">[81]  P.-L. Bacon, J. Harb, and D. Precup, “The option-critic architecture,” in </a><i>Proc. AAAI Conf. Artif. Intell.</i>, Feb. 2017, vol. 31, no. 1, pp. 1–18.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark239">[82]   J. Harb, P.-L. Bacon, M. Klissarov, and D. Precup, “When waiting is not an option: Learning options with a deliberation cost,” in </a><i>Proc. AAAI Conf. Artif. Intell.</i>, Apr. 2018, vol. 32, no. 1, pp. 1–16.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark240">[83] A. S. Vezhnevets et al., “Feudal networks for hierarchical reinforce- ment learning,” in </a><i>Proc. Int.  Conf.  Mach.  Learn.  (ICML)</i>,  2017, pp. 3540–3549.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark241">[84] O. Nachum, S. Gu, H. Lee, and S. Levine, “Data-e</a><span class="s8">ffi</span>cient hierarchical reinforcement learning,” in <i>Proc. Adv. Neural Inf.  Process.  Syst.</i>, May 2018, pp. 1–11.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark242">[85] S. Pateria, B. Subagdja, A.-H. Tan, and C. Quek, “Hierarchical rein- forcement learning: A comprehensive survey,” </a><i>ACM Comput. Surveys</i>, vol. 54, no. 5, pp. 1–35, 2021.</p><p class="s6" style="padding-top: 2pt;padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark243">[86]  S.  John  Grimbly,  J.  Shock,  and   A.   Pretorius,   “Causal   multi- agent reinforcement learning: Review and open problems,” 2021, </a><i>arXiv:2111.06721</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark244">[87] J. Pearl, “Theoretical impediments to machine learning with seven sparks from the causal revolution,” 2018, </a><i>arXiv:1801.04016</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark245">[88] N. Jaques et al.. (2018). </a><i>Intrinsic Social Motivation Via Causal Influ- ence in Multi-agent RL</i><a href="https://openreview.net/forum?id=B1lG42C9Km" class="s48" target="_blank">. [Online]. Available: https:</a><a href="https://openreview.net/forum?id=B1lG42C9Km" class="s51" target="_blank">//</a><a href="https://openreview.net/forum?id=B1lG42C9Km" class="s48" target="_blank">openreview.net</a><a href="https://openreview.net/forum?id=B1lG42C9Km" class="s51" target="_blank">/ </a><a href="https://openreview.net/forum?id=B1lG42C9Km" class="s48" target="_blank">forum?id</a><a href="https://openreview.net/forum?id=B1lG42C9Km" class="s51" target="_blank">=</a><a href="https://openreview.net/forum?id=B1lG42C9Km" class="s48" target="_blank">B1lG42C9Km</a></p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark246">[89] S. Yang, B. Yang, Z. Zeng, and Z. Kang, “Causal inference multi-agent reinforcement learning for tra</a><span class="s8">ffi</span>c signal control,” <i>Inf. Fusion</i>, vol. 94, pp. 243–256, Jun. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark247">[90] J. W. K. Ho and C. Wang, “Human-centered AI using ethical causal- ity and learning representation for multi-agent deep reinforcement learning,” in </a><i>Proc. IEEE 2nd Int. Conf. Hum.-Mach. Syst. (ICHMS)</i>, vol. 37, Sep. 2021, pp. 1–6.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark248">[91] B. Liu et al., “An e</a><span class="s8">ffi</span>cient message dissemination scheme for coop- erative drivings via multi-agent hierarchical attention reinforcement learning,” in <i>Proc. IEEE 41st Int. Conf. Distrib. Comput. Syst. (ICDCS)</i>, Jul. 2021, pp. 326–336.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark249">[92] B. Li, K. Xie, X. Huang, Y. Wu, and S. Xie, “Deep reinforcement learning based incentive mechanism design for platoon autonomous driving with social e</a><span class="s8">ff</span>ect,” <i>IEEE Trans. Veh. Technol.</i>, vol. 71, no. 7, pp. 7719–7729, Jul. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark250">[93] H. Shi, D. Chen, N. Zheng, X. Wang, Y. Zhou, and B. Ran, “A deep reinforcement learning based distributed control strategy for connected automated vehicles in mixed tra</a><span class="s8">ffi</span>c platoon,” <i>Transp. Res. C, Emerg. Technol.</i>, vol. 148, Mar. 2023, Art. no. 104019.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark251">[94] M. Parvini, M. R. Javan, N. Mokari, B. Abbasi, and E. A. Jorswieck, “AoI-aware resource allocation for  platoon-based  C-V2X  networks via multi-agent multi-task reinforcement learning,” </a><i>IEEE Trans. Veh. Technol.</i>, vol. 72, no. 8, pp. 9880–9896, Aug. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark252">[95] M. Kolat and T. Be´csi, “Cooperative MARL-PPO approach for auto- mated highway platoon merging,” </a><i>Electronics</i>, vol. 13, no. 15, p. 3102, Aug. 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark253">[96] Y. Xu, Y. Shi, X. Tong, S. Chen, and Y. Ge, “A multi-agent reinforcement learning based control method for cavs in a mixed platoon,” </a><i>IEEE Trans. Veh. Technol.</i>, vol. 73, no. 11, pp. 16160–16172, Nov. 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark254">[97] S. Chen, M. Wang, W. Song, Y. Yang, and M. Fu, “Multi-agent reinforcement learning-based twin-vehicle fair cooperative driving in dynamic highway scenarios,” in </a><i>Proc. IEEE 25th Int. Conf. Intell. Transp. Syst. (ITSC)</i>, Oct. 2022, pp. 730–736.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark255">[98] J. Zhang, C. Chang, X. Zeng,  and  L.  Li,  “Multi-agent  DRL- based lane  change  with  right-of-way  collaboration  awareness,” </a><i>IEEE Trans. Intell. Transport. Syst.</i>, vol. 24,  no. 1,  pp. 854–869, Jan. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark256">[99] W. Zhou, D. Chen, J. Yan, Z. Li, H. Yin, and W. Ge, “Multi-agent reinforcement learning for cooperative lane changing of connected and autonomous vehicles in mixed tra</a><span class="s8">ffi</span>c,” <i>Auto. Intell. Syst.</i>, vol. 2, no. 1, p. 5, Dec. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark257">[100] J. Guo and I. Harmati, “Lane-changing system based on deep q- learning with a request–respond mechanism,” </a><i>Expert  Syst.  Appl.</i>, vol. 235, Jun. 2024, Art. no. 121242.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark258">[101] X. Zhang, S. Li, B. Wang, M. Xue, Z. Li,  and  H.  Liu,  “Multi- vehicle collaborative lane changing based on multi-agent reinforcement learning,” in </a><i>Proc. IEEE Intell.  Vehicles  Symp.  (IV)</i>,  Jun.  2024, pp. 1214–1221.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark259">[102] H. Wang, W. Hao, J. So, Z. Chen, and  J.  Hu,  “A  faster  coop- erative lane change controller enabled by formulating in spatial domain,” </a><i>IEEE Trans. Intell. Vehicles</i>, vol. 8, no. 12, pp. 4685–4695, Dec. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark260">[103] S. Tsugawa, S. Jeschke, and S. E. Shladover, “A review of truck platooning projects for energy savings,” </a><i>IEEE Trans. Intell. Veh.</i>, vol. 1, no. 1, pp. 68–77, Mar. 2016.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark261">[104] Y. Xu, K.  Zhu,  H.  Xu,  and  J.  Ji,  “Deep  reinforcement  learning for multi-objective resource allocation in multi-platoon cooperative vehicular networks,” </a><i>IEEE Trans. Wireless Commun.</i>, vol. 22, no. 9, pp. 6185–6198, Sep. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark262">[105] Z. Wang, G. Wu, and M. J. Barth, “Cooperative eco-driving at signalized intersections in a partially connected and automated vehi- cle environment,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 21, no. 5, pp. 2029–2038, May 2020.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark263">[106] S. Lu, Y. Cai, L. Chen, H. Wang, X. Sun, and H. Gao, “Altruistic cooperative adaptive cruise control of mixed tra</a><span class="s8">ffi</span>c platoon based on deep reinforcement learning,” <i>IET Intell. Transp. Syst.</i>, vol. 17, no. 10, pp. 1951–1963, Oct. 2023.</p><p class="s6" style="padding-top: 2pt;padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark264">[107] N. Heess et al., “Emergence of locomotion behaviours in rich environments,” 2017, </a><i>arXiv:1707.02286</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark265">[108] H. V. Vu, M. Farzanullah, Z. Liu, D. H. N. Nguyen, R. Morawski, and T. Le-Ngoc, “Multi-agent reinforcement learning for channel assignment and power allocation in platoon-based C-V2X systems,” 2020, </a><i>arXiv:2011.04555</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark266">[109] S. Feng, Y. Zhang, S. E. Li, Z. Cao, H. X. Liu, and L. Li, “String sta- bility for vehicular platoon control: Definitions and analysis methods,” </a><i>Annu. Rev. Control</i>, vol. 47, pp. 81–97, Aug. 2019.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark267">[110] A. Peake, J. McCalmon, B. Raiford, T. Liu, and S. Alqahtani, “Multi- agent reinforcement learning for cooperative adaptive cruise control,” in </a><i>Proc. IEEE 32nd Int. Conf. Tools Artif. Intell. (ICTAI)</i>, Nov. 2020, pp. 15–22.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark268">[111] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” </a><i>Neural Comput.</i>, vol. 9, no. 8, pp. 1735–1780, 1997.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark269">[112] M. Li, Z. Cao, and Z. Li, “A reinforcement learning-based vehicle platoon control strategy for reducing energy consumption in tra</a><span class="s8">ffi</span>c oscillations,” <i>IEEE Trans. Neural Netw. Learn. Syst.</i>, vol. 32, no. 12, pp. 5309–5322, Dec. 2021.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark270">[113] X. He, H. Yang, Z. Hu, and C. Lv, “Robust lane change decision making for autonomous vehicles: An observation adversarial reinforce- ment learning approach,” </a><i>IEEE Trans. Intell. Vehicles</i>, vol. 8, no. 1, pp. 184–193, Jan. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark271">[114]  G.  Chen,  Z.  Gao,  M.  Hua,  B.  Shuai,  and  Z.  Gao,   “Lane change trajectory prediction considering driving style uncertainty for autonomous vehicles,” </a><i>Mech. Syst. Signal Process.</i>, vol. 206, Jan. 2024, Art. no. 110854.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark272">[115] C. Yu et al., “The surprising e</a><span class="s8">ff</span>ectiveness of PPO in cooperative multi-agent games,” in <i>Proc. Adv. Neural Inf. Process. Syst.</i>, vol. 35, Nov. 2022, pp. 24611–24624.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark273">[116] Y. Hou and P. Graf, “Decentralized cooperative lane changing at freeway weaving areas using multi-agent deep reinforcement learning,” 2021, </a><i>arXiv:2110.08124</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark274">[117] S. Chen, M. Wang, W. Song, Y.  Yang,  and  M.  Fu,  “Multi- agent reinforcement learning-based decision making for twin-vehicles cooperative driving in stochastic dynamic highway environments,” </a><i>IEEE  Trans.  Veh.  Technol.</i>,  vol. 72,  no. 10,   pp. 12615–12627, Oct. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark275">[118] C. Vishnu, V. Abhinav, D. Roy, C. K. Mohan, and C. S. Babu, “Improving multi-agent trajectory prediction using tra</a><span class="s8">ffi</span>c states on interactive driving scenarios,” <i>IEEE Robot. Autom. Lett.</i>, vol. 8, no. 5, pp. 2708–2715, May 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark276">[119] S. Han et al., “A multi-agent reinforcement learning approach for safe and e</a><span class="s8">ffi</span>cient behavior planning of connected autonomous vehicles,” 2020, <i>arXiv:2003.04371</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark277">[120] Q. Li, Z. Peng, L. Feng, Q. Zhang, Z. Xue, and B. Zhou, “MetaDrive: Composing diverse driving scenarios for generalizable reinforcement learning,” </a><i>IEEE Trans. Pattern Anal.  Mach.  Intell.</i>,  vol. 45,  no. 3, pp. 3461–3475, Mar. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark278">[121] J. Zhou et al., “Graph neural networks: A review of methods and applications,” </a><i>AI Open</i>, vol. 1, pp. 57–81, Jun. 2020.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark279">[122] S. Chen, J. Dong, P. J. Ha, Y. Li, and S. Labi,  “Graph  neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles,” </a><i>Comput.-Aided Civil Infrastruct. Eng.</i>, vol. 36, no. 7, pp. 838–857, Jul. 2021.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark280">[123] P. Young Joun  Ha, S. Chen, J. Dong, R. Du, Y. Li, and S. Labi, “Leveraging the capabilities of connected and autonomous vehicles and multi-agent reinforcement learning to mitigate highway bottleneck congestion,” 2020, </a><i>arXiv:2010.05436</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark281">[124] G. Wang, J. Hu, Z. Li, and L. Li, “Harmonious lane changing via deep reinforcement learning,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 23, no. 5, pp. 4642–4650, May 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark282">[125] Y. Wu, H. Chen, and F. Zhu, “DCL-AIM: Decentralized coordina- tion learning of autonomous intersection management for connected and automated vehicles,” </a><i>Transp. Res. C, Emerg. Technol.</i>, vol. 103, pp. 246–260, Jun. 2019.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark283">[126] T. Tan, F. Bao, Y. Deng, A. Jin,  Q.  Dai,  and  J.  Wang, “Cooperative deep reinforcement learning for large-scale tra</a><span class="s8">ffi</span>c grid signal control,” <i>IEEE Trans. Cybern.</i>, vol. 50, no. 6, pp. 2687–2700, Jun. 2020.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark284">[127] E. Van der Pol and F. A. Oliehoek, “Coordinated deep reinforcement learners for tra</a><span class="s8">ffi</span>c light control,” in <i>Proc. Learn., Inference Control Multi-Agent Syst. (NIPS)</i>, vol. 8, 2016, pp. 21–38.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark285">[128] J. Liu, H. Zhang, Z. Fu, and Y. Wang, “Learning scalable multi-agent coordination by spatial di</a><span class="s8">ff</span>erentiation for tra<span class="s8">ffi</span>c signal control,” <i>Eng. Appl. Artif. Intell.</i>, vol. 100, Apr. 2021, Art. no. 104165.</p><p class="s6" style="padding-top: 2pt;padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark286">[129] T. Wang, J.  Cao,  and  A.  Hussain,  “Adaptive  tra</a><span class="s8">ffi</span>c  signal  control for large-scale scenario with cooperative group-based multi-agent reinforcement learning,” <i>Transp. Res. C, Emerg. Technol.</i>, vol. 125, Apr. 2021, Art. no. 103046.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark287">[130] T. Wu et al., “Multi-agent deep reinforcement learning for urban tra</a><span class="s8">ffi</span>c light control in vehicular networks,” <i>IEEE Trans. Veh. Technol.</i>, vol. 69, no. 8, pp. 8243–8256, Aug. 2020.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark288">[131] J. Ma and F. Wu, “Feudal multi-agent deep reinforcement learning for tra</a><span class="s8">ffi</span>c signal control,” in <i>Proc. 19th Int. Conf. Auto. Agents Multiagent Syst. (AAMAS)</i>, May 2020, pp. 816–824.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark289">[132] C. Zhu, M. Dastani, and S. Wang, “A survey of multi-agent deep reinforcement learning with communication,” 2022, </a><i>arXiv:2203.08975</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark290">[133] D. Liu and L. Li, “A tra</a><span class="s8">ffi</span>c light control method based on multi- agent deep reinforcement learning algorithm,” <i>Sci. Rep.</i>, vol. 13, no. 1, p. 9396, Jun. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark291">[134] Y. Wang, T. Xu, X. Niu, C. Tan, E. Chen, and H. Xiong, “STMARL: A spatio-temporal multi-agent reinforcement learning approach for cooperative tra</a><span class="s8">ffi</span>c light control,” <i>IEEE Trans. Mobile Comput.</i>, vol. 21, no. 6, pp. 2228–2242, Jun. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark292">[135] Q. Jiang, M. Qin, S. Shi, W. Sun, and B. Zheng, “Multi-agent reinforcement learning for tra</a><span class="s8">ffi</span>c signal control through universal communication method,” 2022, <i>arXiv:2204.12190</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark293">[136] Q. Liu, X. Li, Y. Tang, X. Gao, F. Yang, and Z. Li, “Graph reinforce- ment learning-based decision-making technology for connected and autonomous vehicles: Framework, review, and future trends,” </a><i>Sensors</i>, vol. 23, no. 19, p. 8229, Oct. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark294">[137] H. Wei et al., “CoLight: Learning network-level cooperation for tra</a><span class="s8">ffi</span>c signal control,” in <i>Proc. 28th ACM Int. Conf. Inf. Knowl. Manag.</i>, 2019, pp. 1913–1922.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark295">[138] S. Yang, B. Yang, Z. Kang, and L. Deng, “IHG-MA: Inductive hetero- geneous graph multi-agent reinforcement learning for multi-intersection tra</a><span class="s8">ffi</span>c signal control,” <i>Neural Netw.</i>, vol. 139, pp. 265–277, Jul. 2021.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark296">[139] G. Antonio and C. Maria-Dolores, “Multi-agent deep  reinforce- ment learning to manage connected autonomous vehicles at tomor- row’s intersections,” </a><i>IEEE Trans. Veh.  Technol.</i>,  vol. 71,  no. 7, pp. 7033–7043, Jul. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark297">[140] Y. Ren, H. Zhang, L. Du, Z. Zhang, J. Zhang, and H. Li, “Stealthy black-box attack with dynamic threshold against MARL-based tra</a><span class="s8">ffi</span>c signal control system,” <i>IEEE Trans. Ind. Informat.</i>, vol. 20, no. 10, pp. 12021–12031, Oct. 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark298">[141] Y. Yan et al., “A multi-vehicle game-theoretic framework for decision making and planning of autonomous vehicles in mixed tra</a><span class="s8">ffi</span>c,” <i>IEEE Trans. Intell. Vehicles</i>, vol. 8, no. 11, pp. 4572–4587, Nov. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark299">[142] L. Schester and L. E. Ortiz, “Longitudinal position control for highway on-ramp merging: A multi-agent approach to automated driving,” in </a><i>Proc. IEEE Intell. Transp. Syst. Conf. (ITSC)</i>, Oct. 2019, pp. 3461–3468.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark300">[143] S. Zhou, W. Zhuang, G. Yin, H. Liu, and C. Qiu, “Cooperative on- ramp merging control of connected and automated vehicles: Distributed multi-agent deep reinforcement learning approach,” in </a><i>Proc. IEEE 25th Int. Conf. Intell. Transp. Syst. (ITSC)</i>, Jul. 2022, pp. 402–408.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark301">[144]  S.  K.  Sumanth  Nakka,   B.   Chalaki,   and   A.   A.   Malikopoulos, “A multi-agent deep reinforcement learning coordination framework for connected and automated vehicles at merging roadways,” in </a><i>Proc. Amer. Control Conf. (ACC)</i>, Jun. 2022, pp. 3297–3302.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark302">[145] Y. Hu, A. Nakhaei, M. Tomizuka, and K. Fujimura, “Interaction-aware decision making with adaptive strategies under merging scenarios,” in </a><i>Proc. IEEE</i>/<i>RSJ Int. Conf. Intell. Robots Syst. (IROS)</i>, Nov. 2019, pp. 151–158.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark303">[146] R. Chandra and D. Manocha, “GamePlan: Game-theoretic  multi- agent planning with human drivers at intersections, roundabouts, and merging,” </a><i>IEEE  Robot.  Autom.  Lett.</i>,  vol. 7,  no. 2,  pp. 2676–2683, Apr. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark304">[147] C. Huang, J. Zhao, H. Zhou, H. Zhang, X. Zhang, and C. Ye, “Multi- agent decision-making at unsignalized intersections with reinforcement learning from demonstrations,” in </a><i>Proc. IEEE Intell. Vehicles Symp.</i>, Jun. 2023, pp. 1–6.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark305">[148] A. Aksjonov and V. Kyrki, “Rule-based decision-making system for autonomous vehicles at intersections with mixed tra</a><span class="s8">ffi</span>c environment,” in <i>Proc. IEEE Int.  Intell.  Transp.  Syst.  Conf.  (ITSC)</i>,  Sep.  2021, pp. 660–666.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark306">[149] Y. Gu, Y. Hashimoto, L.-T. Hsu, and S. Kamijo, “Motion planning based on learning models of pedestrian and driver  behaviors,”  in </a><i>Proc. IEEE 19th Int. Conf. Intell. Transp. Syst. (ITSC)</i>, Nov. 2016, pp. 808–813.</p><p class="s6" style="padding-top: 2pt;padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark307">[150] A. P. Capasso, P. Maramotti, A. Dell’Eva, and A. Broggi, “End-to-end intersection handling using multi-agent deep reinforcement learning,” in </a><i>Proc. IEEE Intell. Vehicles Symp. (IV)</i>, Jul. 2021, pp. 443–450.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark308">[151] Z. Yan and C. Wu, “Reinforcement learning for mixed autonomy intersections,” in </a><i>Proc. IEEE Int. Intell. Transp. Syst. Conf. (ITSC)</i>, Oct. 2021, pp. 2089–2094.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark309">[152] C. Mavrogiannis, J. A. DeCastro, and S. S. Srinivasa, “Implicit multiagent coordination at unsignalized intersections via multimodal inference enabled by topological braids,” 2020, </a><i>arXiv:2004.05205</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark310">[153] J. X. Zheng, K. Zhu, and R. Wang, “Deep reinforcement learning for autonomous vehicles collaboration at unsignalized intersections,” in </a><i>Proc. IEEE Global Commun. Conf.</i>, Dec. 2022, pp. 1115–1120.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark311">[154] A. H. Hamouda, D. M. Mahfouz, C. M. Elias, and O. M. Shehata, “Multi-layer control architecture for unsignalized intersection manage- ment via nonlinear MPC and deep reinforcement learning,” in </a><i>Proc. IEEE Int. Intell. Transp. Syst. Conf. (ITSC)</i>, Sep. 2021, pp. 1990–1996.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark312">[155] Y. Xu, H. Zhou, T. Ma,  J.  Zhao,  B.  Qian,  and  X.  Shen, “Leveraging multiagent learning for automated vehicles scheduling at nonsignalized intersections,” </a><i>IEEE Internet Things J.</i>, vol. 8, no. 14, pp. 11427–11439, Jul. 2021.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark313">[156] N. Hoysal and P. Tallapragada, “Reinforcement learning aided sequen- tial optimization for unsignalized intersection management of robot tra</a><span class="s8">ffi</span>c,” 2023, <i>arXiv:2302.05082</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark314">[157] Z. Li, Q. Yuan, G. Luo, and J. Li, “Learning e</a><span class="s8">ff</span>ective multi-vehicle cooperation at unsignalized intersection via bandwidth-constrained communication,” in <i>Proc. IEEE 94th Veh. Technol. Conf.</i>, Sep. 2021, pp. 1–7.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark315">[158] Z. Guo, Y. Wu, L.  Wang,  and  J.  Zhang,  “Coordination  for connected and automated  vehicles  at  non-signalized  intersections: A value decomposition-based multiagent deep reinforcement learning approach,” </a><i>IEEE Trans. Veh. Technol.</i>, vol. 72, no. 3, pp. 3025–3034, Mar. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark316">[159] P. A</a><span class="s52">´</span> .  Lo´pez  et  al.,  “Microscopic  tra<span class="s8">ffi</span>c  simulation  using  SUMO,” in <i>Proc. 21st Int.  Conf.  Intell.  Transp.  Syst.  (ITSC)</i>,  Nov.  2018, pp. 2575–2582.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark317">[160] H. Zhang et al., “CityFlow: A multi-agent reinforcement learning environment for large scale city tra</a><span class="s8">ffi</span>c scenario,” in <i>Proc. World Wide Web Conf.</i>, 2019, pp. 3620–3624.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark318">[161] M. Zhou et al., “SMARTS: Scalable multi-agent reinforcement learning training school for autonomous driving,” 2020, </a><i>arXiv:2010.09776</i>.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark319">[162] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA: An open urban driving simulator,” in </a><i>Proc. 1st Annu. Conf. Robot Learn.</i>, vol. 78, 2017, pp. 1–16.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark320">[163] P. Palanisamy, “Multi-agent connected autonomous driving  using deep reinforcement learning,” in </a><i>Proc. Int. Joint Conf. Neural Netw. (IJCNN)</i>, Jul. 2020, pp. 1–7.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark321">[164] S. Chen, Y. Chen, S. Zhang, and N. Zheng, “A novel integrated simulation and testing platform for self-driving cars with hardware in the loop,” </a><i>IEEE Trans. Intell. Veh.</i>,  vol. 4,  no. 3,  pp. 425–436, Sep. 2019.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark322">[165] R. Trumpp, H. Bayerlein, and D. Gesbert, “Modeling interactions of autonomous vehicles and pedestrians with deep multi-agent reinforce- ment learning for collision avoidance,” in </a><i>Proc. IEEE Intell. Vehicles Symp. (IV)</i>, Jun. 2022, pp. 331–336.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark323">[166] E. Vinitsky, N. Lichtle´, X. Yang, B. Amos, and J. Foerster, “Nocturne: A scalable driving benchmark for bringing multi-agent learning one step closer to the real world,” in </a><i>Proc. Adv. Neural Inf. Process. Syst.</i>, Jan. 2022, pp. 3962–3974.</p><p class="s6" style="padding-top: 2pt;padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">[167] E. Leurent. (2018). <i>An Environment for Autonomous Driving Decision- making</i><a href="https://github.com/eleurent/highway-env" class="s48" target="_blank">. [Online]. Available: https:</a><a href="https://github.com/eleurent/highway-env" class="s51" target="_blank">//</a><a href="https://github.com/eleurent/highway-env" class="s48" target="_blank">github.com</a><a href="https://github.com/eleurent/highway-env" class="s51" target="_blank">/</a><a href="https://github.com/eleurent/highway-env" class="s48" target="_blank">eleurent</a><a href="https://github.com/eleurent/highway-env" class="s51" target="_blank">/</a><a href="https://github.com/eleurent/highway-env" class="s48" target="_blank">highway-env</a></p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark324">[168] S. Gu et al., “A review of safe reinforcement learning: Methods, theories, and applications,” </a><i>IEEE Trans. Pattern Anal. Mach. Intell.</i>, vol. 46, no. 12, pp. 11216–11235, Dec. 2024.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark325">[169] Q. Yang, T. D. Sima˜o, S. H. Tindemans, and M. T. J. Spaan, “Safety-constrained reinforcement learning with  a  distributional safety   critic,”   </a><i>Mach.   Learn.</i>,   vol. 112,   no. 3,   pp. 859–887, Mar. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark326">[170] S. Gu et al., “Safe multi-agent reinforcement learning for multi-robot control,” </a><i>Artif. Intell.</i>, vol. 319, Jun. 2023, Art. no. 103905.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark327">[171] C. Hou, C. Zhou, C.-G. Wu, R. Cong, and K. Li, “Optimization of cloud-based multi-agent system for trade-o</a><span class="s8">ff </span>between trustworthiness of data and cost of data usage,” <i>IEEE Trans. Autom. Sci. Eng.</i>, vol. 21, no. 1, pp. 106–122, Jan. 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark328">[172] M. H. Rahman, M. Abdel-Aty, and Y. Wu, “A multi-vehicle com- munication system to assess the safety  and  mobility  of  connected and automated vehicles,” </a><i>Transp. Res. C, Emerg. Technol.</i>, vol. 124, Mar. 2021, Art. no. 102887.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark329">[173] Y. Cheng, L. Shi, J. Shao, L. Bai, and K. Chen, “Emergent dynamics of asynchronous multiagent systems under signed networks and limited communication resources,” </a><i>IEEE Trans. Netw. Sci. Eng.</i>, vol. 10, no. 1, pp. 477–488, Jan. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark330">[174] R. Valiente, B. Toghi, R. Pedarsani, and Y. P. Fallah, “Robustness and adaptability of reinforcement learning-based cooperative autonomous driving in mixed-autonomy tra</a><span class="s8">ffi</span>c,” <i>IEEE Open J. Intell. Transp. Syst.</i>, vol. 3, pp. 397–410, 2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark331">[175] B. Freed, G. Sartoretti, J. Hu, and H. Choset, “Communication learn- ing via backpropagation in discrete channels with unknown noise,” in </a><i>Proc. AAAI Conf. Artif. Intell.</i>,  vol. 34,  no. 5,  pp. 7160–7168, Apr. 2020.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark332">[176] Y. Lei, D. Ye, S. Shen, Y. Sui, T. Zhu, and W. Zhou, “New challenges in reinforcement learning: A survey of security and privacy,” </a><i>Artif. Intell. Rev.</i>, vol. 56, no. 7, pp. 7195–7236, 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark333">[177] R. Valiente, B. Toghi, M. Razzaghpour, R. Pedarsani, and Y. P. Fallah, “Learning-based social coordination to improve safety and robustness of cooperative autonomous vehicles in mixed tra</a><span class="s8">ffi</span>c,” in <i>Machine Learning and Optimization Techniques for Automotive Cyber-Physical Systems</i>. Cham, Switzerland: Springer, 2023, pp. 671–707.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark334">[178] S. Zhao, G. Chen, M. Hua, and C. Zong, “An identification algorithm of driver steering characteristics based on backpropagation neural network,” </a><i>Proc. Inst. Mech. Eng. D, J. Automobile Eng.</i>, vol. 233, no. 9, pp. 2333–2342, 2019.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark335">[179] C. Dai, C.  Zong,  D.  Zhang,  M.  Hua,  H.  Zheng,  and  K.  Chuyo, “A bargaining game-based human–machine shared driving control authority allocation strategy,” </a><i>IEEE Trans. Intell. Transp. Syst.</i>, vol. 24, no. 10, pp. 10572–10586, Oct. 2023.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark336">[180] S. Gilroy, D. Mullins, E. Jones, A. Parsi, and M. Glavin, “E-scooter rider detection and classification in dense urban environments,” </a><i>Results Eng.</i>, vol. 16, Dec. 2022, Art. no. 100677.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark337">[181] X. Guo, D. Zhang, J. Wang, J. Park, and L. Guo, “Observer-based event-triggered composite anti-disturbance control for multi-agent systems under multiple disturbances and stochastic FDIAs,” </a><i>IEEE Trans.  Autom.  Sci.  Eng.</i>,  vol. 20,  no. 1,  pp. 528–540,  Mar.  31,</p><p class="s6" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;">2022.</p><p class="s6" style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark338">[182] N. Lichtle´ et al., “From sim to real: A pipeline for training and deploying tra</a><span class="s8">ffi</span>c smoothing cruise controllers,” <i>IEEE Trans. Robot.</i>, vol. 40, pp. 4490–4505, 2024.</p></body></html>
